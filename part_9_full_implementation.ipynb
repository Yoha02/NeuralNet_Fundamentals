{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Network Fundamentals\n",
        "\n",
        "## Part 9: Full Implementation - Mastery\n",
        "\n",
        "### The Brain's Decision Committee - Chapter 9\n",
        "\n",
        "---\n",
        "\n",
        "## The Complete Journey\n",
        "\n",
        "We've come a long way! From understanding matrices to building neurons, from single perceptrons to multi-layer networks, from training basics to handling deep learning challenges - now it's time to bring **everything together**.\n",
        "\n",
        "*\"The complete, trained committee works in harmony. All the lessons learned, all the challenges overcome, unified into one elegant solution.\"*\n",
        "\n",
        "---\n",
        "\n",
        "## What You'll Learn in Part 9\n",
        "\n",
        "By the end of this notebook, you will have:\n",
        "\n",
        "1. **A Complete Neural Network Class** - All concepts unified in clean, documented code\n",
        "2. **A Full Data Pipeline** - Train/validation/test splits with proper handling\n",
        "3. **A Robust Training Pipeline** - With validation monitoring and early stopping\n",
        "4. **Complete Evaluation** - All metrics, confusion matrix, and saliency visualization\n",
        "5. **Interactive Dashboard** - Experiment with hyperparameters in real-time\n",
        "6. **The Final V/H Classifier** - Our mission accomplished!\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "This is the culmination notebook - you should have completed:\n",
        "- **Part 0-1:** Matrices and fundamentals\n",
        "- **Part 2:** Single neurons\n",
        "- **Part 3:** Activation functions\n",
        "- **Part 4:** The Perceptron\n",
        "- **Part 5:** Training\n",
        "- **Part 6:** Evaluation\n",
        "- **Part 7:** Hidden layers\n",
        "- **Part 8:** Deep learning challenges\n",
        "\n",
        "---\n",
        "\n",
        "## Concepts We're Unifying\n",
        "\n",
        "| Part | Concept | How We'll Use It |\n",
        "|------|---------|-----------------|\n",
        "| 1 | Matrices, dot product | Data representation, weight operations |\n",
        "| 2 | Neuron anatomy | Building blocks of our network |\n",
        "| 3 | Activation functions | ReLU for hidden, sigmoid for output |\n",
        "| 4 | Forward pass | Making predictions |\n",
        "| 5 | Loss, gradients, backprop | Learning from mistakes |\n",
        "| 6 | Metrics, saliency | Evaluating and understanding |\n",
        "| 7 | Hidden layers | Multiple specialists |\n",
        "| 8 | Overfitting prevention | Early stopping, proper sizing |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Setup: Import Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PART 9: FULL IMPLEMENTATION - SETUP\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Try to import ipywidgets for interactive features\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    WIDGETS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WIDGETS_AVAILABLE = False\n",
        "    print(\"Note: ipywidgets not installed. Interactive features will be limited.\")\n",
        "\n",
        "# Set up matplotlib style\n",
        "style_options = ['seaborn-v0_8-whitegrid', 'seaborn-whitegrid', 'ggplot', 'default']\n",
        "for style in style_options:\n",
        "    try:\n",
        "        plt.style.use(style)\n",
        "        break\n",
        "    except OSError:\n",
        "        continue\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PART 9: FULL IMPLEMENTATION\")\n",
        "print(\"The Complete V/H Line Classifier\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 9.1 The Complete Neural Network Class\n",
        "\n",
        "This is the unified implementation incorporating everything we've learned:\n",
        "\n",
        "| Feature | Part Learned | Implementation |\n",
        "|---------|--------------|----------------|\n",
        "| Activation functions | Part 3 | ReLU for hidden, Sigmoid for output |\n",
        "| Forward propagation | Parts 4, 7 | Matrix operations through layers |\n",
        "| Loss function | Part 5 | Binary Cross-Entropy |\n",
        "| Backpropagation | Parts 5, 7 | Chain rule through all layers |\n",
        "| Validation monitoring | Part 8 | Track train/val metrics |\n",
        "| Early stopping | Part 8 | Stop when val loss increases |\n",
        "\n",
        "### Why This Architecture?\n",
        "\n",
        "**Input (9) â†’ Hidden (8, ReLU) â†’ Output (1, Sigmoid)**\n",
        "\n",
        "| Layer | Size | Activation | Why? |\n",
        "|-------|------|------------|------|\n",
        "| Input | 9 | None | One neuron per pixel (3Ã—3 = 9) |\n",
        "| Hidden | 8 | ReLU | Enough specialists without overfitting; ReLU prevents vanishing gradients |\n",
        "| Output | 1 | Sigmoid | Binary classification needs probability in (0,1) |\n",
        "\n",
        "### Why Two Different Initializations?\n",
        "\n",
        "We use different initialization strategies for different activations:\n",
        "\n",
        "| Initialization | Formula | Used For | Why? |\n",
        "|----------------|---------|----------|------|\n",
        "| **He** | $w \\sim N(0, \\sqrt{2/n_{in}})$ | ReLU layers | ReLU \"kills\" half the neurons (negative z), so we need 2Ã— variance |\n",
        "| **Xavier** | $w \\sim N(0, \\sqrt{1/n_{in}})$ | Sigmoid/Tanh | These are symmetric around 0, so standard variance works |\n",
        "\n",
        "Using the wrong initialization can cause:\n",
        "- **Too small:** Signals shrink through layers (vanishing)\n",
        "- **Too large:** Signals explode through layers (exploding)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# THE COMPLETE NEURAL NETWORK CLASS\n",
        "# =============================================================================\n",
        "\n",
        "class NeuralNetwork:\n",
        "    \"\"\"\n",
        "    Complete Neural Network implementation for binary classification.\n",
        "    \n",
        "    This class unifies all concepts from Parts 1-8:\n",
        "    - Matrix operations (Part 1)\n",
        "    - Neuron anatomy (Part 2)\n",
        "    - Activation functions (Part 3)\n",
        "    - Forward propagation (Part 4)\n",
        "    - Training with backprop (Part 5)\n",
        "    - Evaluation metrics (Part 6)\n",
        "    - Hidden layers (Part 7)\n",
        "    - Overfitting prevention (Part 8)\n",
        "    \n",
        "    Architecture: Input â†’ Hidden (ReLU) â†’ Output (Sigmoid)\n",
        "    \"\"\"\n",
        "    \n",
        "    # =========================================================================\n",
        "    # ACTIVATION FUNCTIONS (Part 3)\n",
        "    # =========================================================================\n",
        "    \n",
        "    @staticmethod\n",
        "    def sigmoid(z):\n",
        "        \"\"\"Sigmoid: maps to (0, 1) - used for output layer (Part 3.3)\"\"\"\n",
        "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
        "    \n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(z):\n",
        "        \"\"\"Derivative of sigmoid: Ïƒ(z) * (1 - Ïƒ(z)) (Part 3.3.1)\"\"\"\n",
        "        s = NeuralNetwork.sigmoid(z)\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    @staticmethod\n",
        "    def relu(z):\n",
        "        \"\"\"ReLU: max(0, z) - used for hidden layers (Part 3.5)\"\"\"\n",
        "        return np.maximum(0, z)\n",
        "    \n",
        "    @staticmethod\n",
        "    def relu_derivative(z):\n",
        "        \"\"\"Derivative of ReLU: 1 if z > 0, else 0 (Part 3.5)\"\"\"\n",
        "        return (z > 0).astype(float)\n",
        "    \n",
        "    # =========================================================================\n",
        "    # INITIALIZATION (Part 7 - Xavier/He initialization)\n",
        "    # =========================================================================\n",
        "    \n",
        "    def __init__(self, n_inputs, n_hidden, n_outputs=1, seed=None):\n",
        "        \"\"\"\n",
        "        Initialize the neural network.\n",
        "        \n",
        "        Parameters:\n",
        "            n_inputs: Number of input features (9 for 3x3 images)\n",
        "            n_hidden: Number of hidden neurons (the \"specialists\")\n",
        "            n_outputs: Number of outputs (1 for binary classification)\n",
        "            seed: Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        \n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_outputs = n_outputs\n",
        "        \n",
        "        # He initialization for ReLU layers (Part 8 - proper initialization)\n",
        "        self.W1 = np.random.randn(n_hidden, n_inputs) * np.sqrt(2.0 / n_inputs)\n",
        "        self.b1 = np.zeros(n_hidden)\n",
        "        \n",
        "        # Xavier initialization for sigmoid output\n",
        "        self.W2 = np.random.randn(n_outputs, n_hidden) * np.sqrt(1.0 / n_hidden)\n",
        "        self.b2 = np.zeros(n_outputs)\n",
        "        \n",
        "        # Cache for forward pass (needed for backprop)\n",
        "        self.cache = {}\n",
        "        \n",
        "        # Training history\n",
        "        self.train_loss_history = []\n",
        "        self.val_loss_history = []\n",
        "        self.train_acc_history = []\n",
        "        self.val_acc_history = []\n",
        "        \n",
        "        # Best model weights (for early stopping)\n",
        "        self.best_weights = None\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.best_epoch = 0\n",
        "    \n",
        "    # =========================================================================\n",
        "    # FORWARD PROPAGATION (Parts 4, 7)\n",
        "    # =========================================================================\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass: Input â†’ Hidden (ReLU) â†’ Output (Sigmoid)\n",
        "        \n",
        "        The \"Committee Meeting\" - each specialist examines the evidence,\n",
        "        then the final decision maker combines their opinions.\n",
        "        \"\"\"\n",
        "        # Ensure X is 2D\n",
        "        X = np.atleast_2d(X)\n",
        "        \n",
        "        # Layer 1: Input â†’ Hidden (with ReLU - Part 3.5)\n",
        "        self.cache['X'] = X\n",
        "        self.cache['Z1'] = np.dot(X, self.W1.T) + self.b1  # (batch, n_hidden)\n",
        "        self.cache['A1'] = self.relu(self.cache['Z1'])      # ReLU activation\n",
        "        \n",
        "        # Layer 2: Hidden â†’ Output (with Sigmoid - Part 3.3)\n",
        "        self.cache['Z2'] = np.dot(self.cache['A1'], self.W2.T) + self.b2  # (batch, n_outputs)\n",
        "        self.cache['A2'] = self.sigmoid(self.cache['Z2'])                  # Sigmoid for probability\n",
        "        \n",
        "        return self.cache['A2']\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make binary predictions (0 or 1).\"\"\"\n",
        "        probs = self.forward(X)\n",
        "        return (probs >= 0.5).astype(int).flatten()\n",
        "    \n",
        "    # =========================================================================\n",
        "    # LOSS FUNCTION (Part 5.3 - Binary Cross-Entropy)\n",
        "    # =========================================================================\n",
        "    \n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Binary Cross-Entropy loss (Part 5.3)\n",
        "        \n",
        "        Measures \"surprise\" - how unexpected the predictions are.\n",
        "        \"\"\"\n",
        "        epsilon = 1e-15  # Prevent log(0)\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "        y_true = y_true.reshape(-1, 1)\n",
        "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "        return loss\n",
        "    \n",
        "    # =========================================================================\n",
        "    # BACKPROPAGATION (Parts 5.6, 5.7, 7.4)\n",
        "    # =========================================================================\n",
        "    \n",
        "    def backward(self, y_true, learning_rate):\n",
        "        \"\"\"\n",
        "        Backpropagation: Compute gradients and update weights.\n",
        "        \n",
        "        The \"Blame Assignment\" - tracing errors back through the committee.\n",
        "        \"\"\"\n",
        "        m = len(y_true)\n",
        "        y_true = y_true.reshape(-1, 1)\n",
        "        \n",
        "        # Output layer gradients (Part 5.6)\n",
        "        dZ2 = self.cache['A2'] - y_true  # (batch, n_outputs)\n",
        "        dW2 = np.dot(dZ2.T, self.cache['A1']) / m\n",
        "        db2 = np.mean(dZ2, axis=0)\n",
        "        \n",
        "        # Hidden layer gradients (Part 7.4 - chain rule)\n",
        "        dA1 = np.dot(dZ2, self.W2)\n",
        "        dZ1 = dA1 * self.relu_derivative(self.cache['Z1'])\n",
        "        dW1 = np.dot(dZ1.T, self.cache['X']) / m\n",
        "        db1 = np.mean(dZ1, axis=0)\n",
        "        \n",
        "        # Update weights (Gradient Descent - Part 5.4)\n",
        "        self.W2 -= learning_rate * dW2\n",
        "        self.b2 -= learning_rate * db2\n",
        "        self.W1 -= learning_rate * dW1\n",
        "        self.b1 -= learning_rate * db1\n",
        "    \n",
        "    # =========================================================================\n",
        "    # EVALUATION (Part 6)\n",
        "    # =========================================================================\n",
        "    \n",
        "    def evaluate(self, X, y):\n",
        "        \"\"\"Compute loss and accuracy on a dataset.\"\"\"\n",
        "        y_pred = self.forward(X)\n",
        "        loss = self.compute_loss(y, y_pred)\n",
        "        predictions = (y_pred >= 0.5).astype(int).flatten()\n",
        "        accuracy = np.mean(predictions == y)\n",
        "        return loss, accuracy\n",
        "    \n",
        "    def confusion_matrix(self, X, y):\n",
        "        \"\"\"Compute confusion matrix (Part 6.3).\"\"\"\n",
        "        predictions = self.predict(X)\n",
        "        TP = np.sum((predictions == 1) & (y == 1))\n",
        "        TN = np.sum((predictions == 0) & (y == 0))\n",
        "        FP = np.sum((predictions == 1) & (y == 0))\n",
        "        FN = np.sum((predictions == 0) & (y == 1))\n",
        "        return {'TP': TP, 'TN': TN, 'FP': FP, 'FN': FN}\n",
        "    \n",
        "    # =========================================================================\n",
        "    # TRAINING WITH EARLY STOPPING (Parts 5.8, 8.2)\n",
        "    # =========================================================================\n",
        "    \n",
        "    def train(self, X_train, y_train, X_val=None, y_val=None, \n",
        "              learning_rate=0.1, epochs=100, early_stopping_patience=10,\n",
        "              verbose=True):\n",
        "        \"\"\"\n",
        "        Train the neural network with optional early stopping.\n",
        "        \n",
        "        Parameters:\n",
        "            X_train, y_train: Training data\n",
        "            X_val, y_val: Validation data (for early stopping)\n",
        "            learning_rate: Step size for gradient descent (Part 5.5)\n",
        "            epochs: Maximum training iterations\n",
        "            early_stopping_patience: Stop if val loss doesn't improve (Part 8.2)\n",
        "            verbose: Print progress\n",
        "        \"\"\"\n",
        "        self.train_loss_history = []\n",
        "        self.val_loss_history = []\n",
        "        self.train_acc_history = []\n",
        "        self.val_acc_history = []\n",
        "        \n",
        "        patience_counter = 0\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            self.forward(X_train)\n",
        "            \n",
        "            # Backward pass (learning)\n",
        "            self.backward(y_train, learning_rate)\n",
        "            \n",
        "            # Evaluate training\n",
        "            train_loss, train_acc = self.evaluate(X_train, y_train)\n",
        "            self.train_loss_history.append(train_loss)\n",
        "            self.train_acc_history.append(train_acc)\n",
        "            \n",
        "            # Evaluate validation (if provided)\n",
        "            if X_val is not None:\n",
        "                val_loss, val_acc = self.evaluate(X_val, y_val)\n",
        "                self.val_loss_history.append(val_loss)\n",
        "                self.val_acc_history.append(val_acc)\n",
        "                \n",
        "                # Early stopping check (Part 8.2)\n",
        "                if val_loss < self.best_val_loss:\n",
        "                    self.best_val_loss = val_loss\n",
        "                    self.best_epoch = epoch\n",
        "                    self.best_weights = {\n",
        "                        'W1': self.W1.copy(), 'b1': self.b1.copy(),\n",
        "                        'W2': self.W2.copy(), 'b2': self.b2.copy()\n",
        "                    }\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                \n",
        "                if patience_counter >= early_stopping_patience:\n",
        "                    if verbose:\n",
        "                        print(f\"\\n  Early stopping at epoch {epoch+1}!\")\n",
        "                        print(f\"  Best epoch was {self.best_epoch+1} with val_loss={self.best_val_loss:.4f}\")\n",
        "                    self._restore_best_weights()\n",
        "                    break\n",
        "            \n",
        "            # Progress output\n",
        "            if verbose and (epoch + 1) % 20 == 0:\n",
        "                msg = f\"  Epoch {epoch+1:3d}: Train Loss={train_loss:.4f}, Train Acc={train_acc*100:.1f}%\"\n",
        "                if X_val is not None:\n",
        "                    msg += f\", Val Loss={val_loss:.4f}, Val Acc={val_acc*100:.1f}%\"\n",
        "                print(msg)\n",
        "        \n",
        "        if verbose:\n",
        "            final_acc = self.train_acc_history[-1]\n",
        "            print(f\"\\nTraining complete! Final train accuracy: {final_acc*100:.1f}%\")\n",
        "            if X_val is not None:\n",
        "                print(f\"Best validation loss: {self.best_val_loss:.4f} at epoch {self.best_epoch+1}\")\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def _restore_best_weights(self):\n",
        "        \"\"\"Restore weights from best epoch.\"\"\"\n",
        "        if self.best_weights is not None:\n",
        "            self.W1 = self.best_weights['W1']\n",
        "            self.b1 = self.best_weights['b1']\n",
        "            self.W2 = self.best_weights['W2']\n",
        "            self.b2 = self.best_weights['b2']\n",
        "\n",
        "print(\"NeuralNetwork class defined!\")\n",
        "print(\"This combines ALL concepts from Parts 1-8.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Key Implementation Details\n",
        "\n",
        "**Why do we use a `cache` dictionary?**\n",
        "\n",
        "During backpropagation, we need values from the forward pass:\n",
        "- `X` - the input, needed to compute gradients for W1\n",
        "- `Z1` - pre-activation of hidden layer, needed for ReLU derivative\n",
        "- `A1` - hidden activations, needed to compute gradients for W2\n",
        "- `Z2`, `A2` - output layer values for computing output gradients\n",
        "\n",
        "Without caching, we'd have to recompute forward pass during backward pass (wasteful!).\n",
        "\n",
        "**Why save `best_weights` separately?**\n",
        "\n",
        "Early stopping works by:\n",
        "1. Training for many epochs\n",
        "2. Saving weights whenever validation loss improves\n",
        "3. Restoring the best weights at the end\n",
        "\n",
        "If we only kept current weights, we'd lose the best model when we continue training past the optimal point.\n",
        "\n",
        "**Why use `np.atleast_2d(X)`?**\n",
        "\n",
        "This ensures our math works for both:\n",
        "- Single sample: shape (9,) â†’ (1, 9)\n",
        "- Batch of samples: shape (batch, 9) â†’ unchanged\n",
        "\n",
        "Matrix multiplication requires 2D arrays, so this handles both cases gracefully.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 9.2 The Complete Data Pipeline\n",
        "\n",
        "A proper data pipeline includes:\n",
        "\n",
        "| Step | Purpose | Part Referenced |\n",
        "|------|---------|-----------------|\n",
        "| **Data Generation** | Create V/H line images | Part 4 |\n",
        "| **Train/Val/Test Split** | Separate data for different purposes | Part 6, 8 |\n",
        "| **Shuffling** | Prevent order-based patterns | Part 5 |\n",
        "\n",
        "### Why Three Splits?\n",
        "\n",
        "| Split | Purpose | Used For |\n",
        "|-------|---------|----------|\n",
        "| **Training** (60%) | Learn patterns | Backpropagation |\n",
        "| **Validation** (20%) | Tune hyperparameters | Early stopping, model selection |\n",
        "| **Test** (20%) | Final evaluation | Report true performance |\n",
        "\n",
        "**Key Rule:** NEVER use test data during training or tuning!\n",
        "\n",
        "### Why These Specific Percentages?\n",
        "\n",
        "**60/20/20 is a common starting point, but it depends on your data:**\n",
        "\n",
        "| Dataset Size | Recommended Split | Reasoning |\n",
        "|--------------|-------------------|-----------|\n",
        "| Small (<500) | 60/20/20 | Need enough validation/test for reliable estimates |\n",
        "| Medium (500-10K) | 70/15/15 | Can afford more training data |\n",
        "| Large (>10K) | 80/10/10 | Even 10% gives hundreds of test samples |\n",
        "\n",
        "**For our 300 samples:**\n",
        "- 180 training (60%) â†’ Enough to learn V/H patterns\n",
        "- 60 validation (20%) â†’ Enough to detect overfitting\n",
        "- 60 test (20%) â†’ Enough for reliable accuracy estimate\n",
        "\n",
        "### Why Shuffle the Data?\n",
        "\n",
        "**Without shuffling, disaster can strike!**\n",
        "\n",
        "Imagine our data is generated in order:\n",
        "```\n",
        "Samples 1-150:   All VERTICAL\n",
        "Samples 151-300: All HORIZONTAL\n",
        "```\n",
        "\n",
        "If we split 60/20/20 without shuffling:\n",
        "- Training (1-180): 150 vertical, 30 horizontal (imbalanced!)\n",
        "- Validation (181-240): 0 vertical, 60 horizontal (all one class!)\n",
        "- Test (241-300): 0 vertical, 60 horizontal (all one class!)\n",
        "\n",
        "**The model would learn wrong patterns and evaluation would be meaningless!**\n",
        "\n",
        "Shuffling ensures each split has a representative mix of both classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# THE COMPLETE DATA PIPELINE\n",
        "# =============================================================================\n",
        "\n",
        "def generate_line_dataset(n_samples=100, noise_level=0.0, seed=None):\n",
        "    \"\"\"\n",
        "    Generate vertical (1) and horizontal (0) line images.\n",
        "    \n",
        "    This is the dataset we've been working with throughout the series.\n",
        "    Our \"mission\" from Part 0: classify these images correctly!\n",
        "    \n",
        "    Parameters:\n",
        "        n_samples: Total number of images to generate\n",
        "        noise_level: Amount of random noise (0.0 = clean, 0.3 = noisy)\n",
        "        seed: Random seed for reproducibility\n",
        "    \n",
        "    Returns:\n",
        "        X: Array of flattened 3x3 images, shape (n_samples, 9)\n",
        "        y: Labels (1=vertical, 0=horizontal), shape (n_samples,)\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    \n",
        "    X, y = [], []\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "        image = np.zeros((3, 3))\n",
        "        \n",
        "        if i < n_samples // 2:\n",
        "            # Vertical line - can be in ANY column\n",
        "            col = np.random.randint(0, 3)\n",
        "            image[:, col] = 1\n",
        "            label = 1\n",
        "        else:\n",
        "            # Horizontal line - can be in ANY row\n",
        "            row = np.random.randint(0, 3)\n",
        "            image[row, :] = 1\n",
        "            label = 0\n",
        "        \n",
        "        # Add noise if specified\n",
        "        if noise_level > 0:\n",
        "            image = np.clip(image + np.random.randn(3, 3) * noise_level, 0, 1)\n",
        "        \n",
        "        X.append(image.flatten())  # Flatten to 1D (Part 2)\n",
        "        y.append(label)\n",
        "    \n",
        "    X, y = np.array(X), np.array(y)\n",
        "    \n",
        "    # Shuffle (Part 5)\n",
        "    shuffle_idx = np.random.permutation(n_samples)\n",
        "    return X[shuffle_idx], y[shuffle_idx]\n",
        "\n",
        "\n",
        "def create_train_val_test_split(n_total=300, noise_level=0.1, seed=42):\n",
        "    \"\"\"\n",
        "    Create proper train/validation/test splits.\n",
        "    \n",
        "    Split ratios: 60% train, 20% validation, 20% test\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Generate all data\n",
        "    X, y = generate_line_dataset(n_total, noise_level=noise_level, seed=seed)\n",
        "    \n",
        "    # Calculate split indices\n",
        "    n_train = int(n_total * 0.6)\n",
        "    n_val = int(n_total * 0.2)\n",
        "    \n",
        "    # Split\n",
        "    X_train, y_train = X[:n_train], y[:n_train]\n",
        "    X_val, y_val = X[n_train:n_train+n_val], y[n_train:n_train+n_val]\n",
        "    X_test, y_test = X[n_train+n_val:], y[n_train+n_val:]\n",
        "    \n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
        "\n",
        "\n",
        "# Create our datasets\n",
        "print(\"=\"*70)\n",
        "print(\"CREATING THE COMPLETE DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "(X_train, y_train), (X_val, y_val), (X_test, y_test) = create_train_val_test_split(\n",
        "    n_total=300, noise_level=0.15, seed=42\n",
        ")\n",
        "\n",
        "print(f\"\\nDataset created with 15% noise:\")\n",
        "print(f\"  Training:   {len(X_train)} samples ({sum(y_train)} vertical, {len(y_train)-sum(y_train)} horizontal)\")\n",
        "print(f\"  Validation: {len(X_val)} samples ({sum(y_val)} vertical, {len(y_val)-sum(y_val)} horizontal)\")\n",
        "print(f\"  Test:       {len(X_test)} samples ({sum(y_test)} vertical, {len(y_test)-sum(y_test)} horizontal)\")\n",
        "print(f\"\\nTotal: {len(X_train) + len(X_val) + len(X_test)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZE SAMPLE IMAGES FROM OUR DATASET\n",
        "# =============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "\n",
        "# Show 5 vertical and 5 horizontal examples\n",
        "v_indices = np.where(y_train == 1)[0][:5]\n",
        "h_indices = np.where(y_train == 0)[0][:5]\n",
        "\n",
        "for i, idx in enumerate(v_indices):\n",
        "    ax = axes[0, i]\n",
        "    ax.imshow(X_train[idx].reshape(3, 3), cmap='Blues', vmin=0, vmax=1)\n",
        "    ax.set_title('VERTICAL', fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "for i, idx in enumerate(h_indices):\n",
        "    ax = axes[1, i]\n",
        "    ax.imshow(X_train[idx].reshape(3, 3), cmap='Oranges', vmin=0, vmax=1)\n",
        "    ax.set_title('HORIZONTAL', fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Our Mission: Classify These 3x3 Images\\n(With 15% Noise)', \n",
        "             fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\"\"\n",
        "OUR MISSION (from Part 0):\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "Build a neural network that can correctly classify these images as:\n",
        "  â€¢ VERTICAL (1) - line goes up-down\n",
        "  â€¢ HORIZONTAL (0) - line goes left-right\n",
        "\n",
        "The challenge: Noise makes the patterns harder to detect!\n",
        "The committee must learn to see through the noise.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 9.3 Training the Complete Network\n",
        "\n",
        "Now we train our neural network using everything we've learned:\n",
        "\n",
        "| Setting | Value | Why (Part Reference) |\n",
        "|---------|-------|---------------------|\n",
        "| Hidden neurons | 8 | Enough for patterns, not too many (Part 8 - overfitting) |\n",
        "| Learning rate | 0.5 | Fast but stable (Part 5) |\n",
        "| Epochs | 200 | Enough to learn, with early stopping (Part 8) |\n",
        "| Early stopping patience | 20 | Stop if no improvement for 20 epochs |\n",
        "| Activation (hidden) | ReLU | Prevents vanishing gradients (Parts 3, 8) |\n",
        "| Activation (output) | Sigmoid | Gives probability (Part 3) |\n",
        "\n",
        "### How We Chose These Values\n",
        "\n",
        "**Hidden neurons = 8:**\n",
        "\n",
        "Our data has 9 inputs and 2 classes. Rule of thumb:\n",
        "- Minimum: 2-4 (can represent basic patterns)\n",
        "- Our choice: 8 (room for multiple pattern detectors)\n",
        "- Maximum: ~20 for 180 training samples (avoid overfitting)\n",
        "\n",
        "**Why 8 works:** We need neurons to detect \"left column\", \"middle column\", \"right column\" for vertical, plus \"top row\", \"middle row\", \"bottom row\" for horizontal. 6-8 neurons can capture these patterns.\n",
        "\n",
        "**Learning rate = 0.5:**\n",
        "\n",
        "| Learning Rate | Behavior |\n",
        "|---------------|----------|\n",
        "| Too low (0.001) | Very slow, may not converge in 200 epochs |\n",
        "| Good (0.1 - 1.0) | Learns quickly, stable |\n",
        "| Too high (5.0) | Overshoots, unstable, may diverge |\n",
        "\n",
        "For small networks with BCE loss, 0.5 is often a good starting point.\n",
        "\n",
        "**Epochs = 200 with patience = 20:**\n",
        "\n",
        "- 200 is a maximum \"budget\" of training steps\n",
        "- Patience of 20 means: \"Stop if validation doesn't improve for 20 epochs\"\n",
        "- This combination lets us train long enough to converge, but stops early if we're overfitting\n",
        "\n",
        "### Understanding Parameter Count\n",
        "\n",
        "```\n",
        "Total parameters = (input Ã— hidden) + hidden + (hidden Ã— output) + output\n",
        "                 = (9 Ã— 8) + 8 + (8 Ã— 1) + 1\n",
        "                 = 72 + 8 + 8 + 1 = 89 parameters\n",
        "```\n",
        "\n",
        "**Rule of thumb:** You want at least 10Ã— more training samples than parameters.\n",
        "- We have 180 training samples\n",
        "- We have 89 parameters\n",
        "- Ratio: 180/89 â‰ˆ 2Ã— (borderline, which is why we use early stopping!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAIN THE COMPLETE NETWORK\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TRAINING THE NEURAL NETWORK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create the network\n",
        "model = NeuralNetwork(\n",
        "    n_inputs=9,      # 3x3 image = 9 pixels\n",
        "    n_hidden=8,      # 8 specialists in our committee\n",
        "    n_outputs=1,     # Binary output (V or H)\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(f\"\\nNetwork Architecture:\")\n",
        "print(f\"  Input layer:  {model.n_inputs} neurons (one per pixel)\")\n",
        "print(f\"  Hidden layer: {model.n_hidden} neurons (ReLU activation)\")\n",
        "print(f\"  Output layer: {model.n_outputs} neuron (Sigmoid activation)\")\n",
        "print(f\"  Total parameters: {9*8 + 8 + 8*1 + 1} = {9*8 + 8 + 8*1 + 1}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"Training with early stopping...\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Train!\n",
        "model.train(\n",
        "    X_train, y_train,\n",
        "    X_val, y_val,\n",
        "    learning_rate=0.5,\n",
        "    epochs=200,\n",
        "    early_stopping_patience=20,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How Training Works: The Complete Flow\n",
        "\n",
        "Here's what happens during each training epoch:\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                        ONE TRAINING EPOCH                           â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  1. FORWARD PASS (Make predictions)                                â”‚\n",
        "â”‚     Input X â†’ [W1Ã—X + b1] â†’ ReLU â†’ [W2Ã—H + b2] â†’ Sigmoid â†’ Output â”‚\n",
        "â”‚                    â†“                      â†“                        â”‚\n",
        "â”‚               Cache Z1, A1            Cache Z2, A2                 â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  2. COMPUTE LOSS                                                   â”‚\n",
        "â”‚     BCE = -mean(yÃ—log(Å·) + (1-y)Ã—log(1-Å·))                        â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  3. BACKWARD PASS (Compute gradients)                              â”‚\n",
        "â”‚     âˆ‚L/âˆ‚W2 â† output error Ã— hidden activations (from cache)       â”‚\n",
        "â”‚     âˆ‚L/âˆ‚W1 â† hidden error Ã— input (chain rule through ReLU)       â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  4. UPDATE WEIGHTS                                                 â”‚\n",
        "â”‚     W1 â† W1 - lr Ã— âˆ‚L/âˆ‚W1                                         â”‚\n",
        "â”‚     W2 â† W2 - lr Ã— âˆ‚L/âˆ‚W2                                         â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  5. EVALUATE                                                       â”‚\n",
        "â”‚     Compute train loss/accuracy                                    â”‚\n",
        "â”‚     Compute val loss/accuracy                                      â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  6. EARLY STOPPING CHECK                                           â”‚\n",
        "â”‚     If val_loss improved â†’ save weights                            â”‚\n",
        "â”‚     If no improvement for `patience` epochs â†’ stop & restore best â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "This process repeats until:\n",
        "- Maximum epochs reached, OR\n",
        "- Early stopping triggers (no validation improvement)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZE TRAINING PROGRESS\n",
        "# =============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "epochs = range(1, len(model.train_loss_history) + 1)\n",
        "\n",
        "# Plot 1: Loss curves\n",
        "ax = axes[0]\n",
        "ax.plot(epochs, model.train_loss_history, 'b-', label='Training Loss', linewidth=2)\n",
        "ax.plot(epochs, model.val_loss_history, 'r-', label='Validation Loss', linewidth=2)\n",
        "ax.axvline(x=model.best_epoch+1, color='green', linestyle='--', linewidth=2,\n",
        "           label=f'Best epoch ({model.best_epoch+1})')\n",
        "ax.set_xlabel('Epoch', fontsize=12)\n",
        "ax.set_ylabel('Loss (BCE)', fontsize=12)\n",
        "ax.set_title('Training Progress: Loss', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Accuracy curves\n",
        "ax = axes[1]\n",
        "ax.plot(epochs, [a*100 for a in model.train_acc_history], 'b-', \n",
        "        label='Training Accuracy', linewidth=2)\n",
        "ax.plot(epochs, [a*100 for a in model.val_acc_history], 'r-', \n",
        "        label='Validation Accuracy', linewidth=2)\n",
        "ax.axvline(x=model.best_epoch+1, color='green', linestyle='--', linewidth=2,\n",
        "           label=f'Best epoch ({model.best_epoch+1})')\n",
        "ax.set_xlabel('Epoch', fontsize=12)\n",
        "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "ax.set_title('Training Progress: Accuracy', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_ylim(40, 105)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\"\"\n",
        "TRAINING INSIGHTS:\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "â€¢ Training and validation curves should stay close (no overfitting!)\n",
        "â€¢ Early stopping saved the best model before potential overfitting\n",
        "â€¢ The committee learned the V/H pattern effectively\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 9.4 Complete Evaluation\n",
        "\n",
        "Now we evaluate our trained model on the **test set** - data it has NEVER seen during training or validation. This is the true measure of generalization.\n",
        "\n",
        "### Evaluation Metrics (Part 6)\n",
        "\n",
        "| Metric | What It Measures |\n",
        "|--------|------------------|\n",
        "| Accuracy | Overall correctness |\n",
        "| Precision | Of predicted positives, how many are correct? |\n",
        "| Recall | Of actual positives, how many did we find? |\n",
        "| F1 Score | Harmonic mean of precision and recall |\n",
        "| Confusion Matrix | Detailed breakdown of TP, TN, FP, FN |\n",
        "\n",
        "### What Do \"Good\" Values Look Like?\n",
        "\n",
        "| Metric | Poor | Okay | Good | Excellent |\n",
        "|--------|------|------|------|-----------|\n",
        "| Accuracy | <60% | 60-75% | 75-90% | >90% |\n",
        "| F1 Score | <0.5 | 0.5-0.7 | 0.7-0.9 | >0.9 |\n",
        "\n",
        "**For our V/H classifier:**\n",
        "- With 15% noise, >85% accuracy is quite good\n",
        "- Balanced precision/recall indicates no systematic bias\n",
        "- Similar train/val/test accuracy indicates good generalization\n",
        "\n",
        "### Reading the Confusion Matrix for Insights\n",
        "\n",
        "The confusion matrix tells us not just HOW MANY errors, but WHAT KIND:\n",
        "\n",
        "| Scenario | Meaning | Possible Cause |\n",
        "|----------|---------|----------------|\n",
        "| High FP (false alarm) | Saying \"vertical\" too often | Model is too sensitive to vertical patterns |\n",
        "| High FN (misses) | Missing vertical lines | Model isn't detecting vertical patterns well |\n",
        "| Balanced errors | FP â‰ˆ FN | Model is \"confused\" by noise, not biased |\n",
        "\n",
        "**Ideal:** Most values on the diagonal (TN, TP), minimal off-diagonal (FP, FN).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPLETE EVALUATION ON TEST SET\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL EVALUATION ON TEST SET\")\n",
        "print(\"(Data the model has NEVER seen!)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get predictions\n",
        "test_predictions = model.predict(X_test)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = model.confusion_matrix(X_test, y_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = (cm['TP'] + cm['TN']) / len(y_test)\n",
        "precision = cm['TP'] / (cm['TP'] + cm['FP']) if (cm['TP'] + cm['FP']) > 0 else 0\n",
        "recall = cm['TP'] / (cm['TP'] + cm['FN']) if (cm['TP'] + cm['FN']) > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "print(f\"\\nğŸ“Š PERFORMANCE METRICS:\")\n",
        "print(\"-\"*40)\n",
        "print(f\"  Accuracy:  {accuracy*100:.1f}%\")\n",
        "print(f\"  Precision: {precision*100:.1f}%\")\n",
        "print(f\"  Recall:    {recall*100:.1f}%\")\n",
        "print(f\"  F1 Score:  {f1*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nğŸ“‹ CONFUSION MATRIX:\")\n",
        "print(\"-\"*40)\n",
        "print(f\"                  Predicted\")\n",
        "print(f\"              HORIZ    VERT\")\n",
        "print(f\"  Actual HORIZ  {cm['TN']:3d}     {cm['FP']:3d}\")\n",
        "print(f\"  Actual VERT   {cm['FN']:3d}     {cm['TP']:3d}\")\n",
        "\n",
        "print(f\"\\n  True Negatives (TN):  {cm['TN']:3d} - Correctly identified horizontal\")\n",
        "print(f\"  True Positives (TP):  {cm['TP']:3d} - Correctly identified vertical\")\n",
        "print(f\"  False Positives (FP): {cm['FP']:3d} - Horizontal wrongly called vertical\")\n",
        "print(f\"  False Negatives (FN): {cm['FN']:3d} - Vertical wrongly called horizontal\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZE EVALUATION RESULTS\n",
        "# =============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Confusion Matrix Heatmap\n",
        "ax = axes[0]\n",
        "cm_matrix = np.array([[cm['TN'], cm['FP']], [cm['FN'], cm['TP']]])\n",
        "im = ax.imshow(cm_matrix, cmap='Blues')\n",
        "ax.set_xticks([0, 1])\n",
        "ax.set_yticks([0, 1])\n",
        "ax.set_xticklabels(['HORIZ (0)', 'VERT (1)'])\n",
        "ax.set_yticklabels(['HORIZ (0)', 'VERT (1)'])\n",
        "ax.set_xlabel('Predicted', fontsize=12)\n",
        "ax.set_ylabel('Actual', fontsize=12)\n",
        "ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Add text annotations\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        text = ax.text(j, i, cm_matrix[i, j], ha='center', va='center', \n",
        "                      fontsize=20, fontweight='bold',\n",
        "                      color='white' if cm_matrix[i, j] > cm_matrix.max()/2 else 'black')\n",
        "\n",
        "# Plot 2: Metrics Bar Chart\n",
        "ax = axes[1]\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "values = [accuracy*100, precision*100, recall*100, f1*100]\n",
        "colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c']\n",
        "bars = ax.bar(metrics, values, color=colors)\n",
        "ax.set_ylim(0, 105)\n",
        "ax.set_ylabel('Percentage (%)', fontsize=12)\n",
        "ax.set_title('Performance Metrics', fontsize=14, fontweight='bold')\n",
        "for bar, val in zip(bars, values):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
        "            f'{val:.1f}%', ha='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "# Plot 3: Sample Predictions\n",
        "ax = axes[2]\n",
        "ax.axis('off')\n",
        "\n",
        "# Show some predictions\n",
        "sample_text = \"SAMPLE PREDICTIONS:\\n\" + \"=\"*40 + \"\\n\\n\"\n",
        "for i in range(min(6, len(X_test))):\n",
        "    actual = \"VERT\" if y_test[i] == 1 else \"HORIZ\"\n",
        "    predicted = \"VERT\" if test_predictions[i] == 1 else \"HORIZ\"\n",
        "    prob = model.forward(X_test[i:i+1])[0, 0]\n",
        "    status = \"âœ“\" if actual == predicted else \"âœ—\"\n",
        "    sample_text += f\"  {status} Actual: {actual:5s}  Predicted: {predicted:5s}  (prob={prob:.2f})\\n\"\n",
        "\n",
        "ax.text(0.05, 0.5, sample_text, fontsize=11, family='monospace',\n",
        "        verticalalignment='center', transform=ax.transAxes,\n",
        "        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.9))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 9.5 Saliency: What Did the Network Learn?\n",
        "\n",
        "Let's peek inside the trained committee's brain - what features do the hidden neurons look for?\n",
        "\n",
        "Each hidden neuron learned to detect specific patterns. By visualizing their weights (reshaped to 3x3), we can see what they're \"looking for.\"\n",
        "\n",
        "### How to Read the Saliency Visualizations\n",
        "\n",
        "**Each 3Ã—3 grid shows ONE hidden neuron's \"template\":**\n",
        "\n",
        "| Color | Weight | Meaning |\n",
        "|-------|--------|---------|\n",
        "| Red | Positive | \"I get excited when this pixel is bright\" |\n",
        "| Blue | Negative | \"I get suppressed when this pixel is bright\" |\n",
        "| White/Gray | Near zero | \"I don't care about this pixel\" |\n",
        "\n",
        "### Patterns to Look For\n",
        "\n",
        "**Good learning:** Hidden neurons specialize in different features:\n",
        "\n",
        "| Pattern Type | What You'll See | What It Detects |\n",
        "|--------------|-----------------|-----------------|\n",
        "| **Column detector** | One column red, others blue | Vertical lines in that column |\n",
        "| **Row detector** | One row red, others blue | Horizontal lines in that row |\n",
        "| **Edge detector** | Mixed red/blue pattern | Edges or transitions |\n",
        "| **General detector** | Mostly red or mostly blue | Overall brightness level |\n",
        "\n",
        "**Signs of good learning:**\n",
        "- Different neurons have different patterns (diversity!)\n",
        "- Some neurons clearly detect vertical patterns\n",
        "- Some neurons clearly detect horizontal patterns\n",
        "- The W2 weights show which neurons \"vote\" for which class\n",
        "\n",
        "**Signs of poor learning:**\n",
        "- All neurons look similar (no specialization)\n",
        "- Random-looking patterns (didn't converge)\n",
        "- All weights near zero (vanishing gradients)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SALIENCY: VISUALIZE WHAT THE NETWORK LEARNED\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"INSIDE THE COMMITTEE'S BRAIN: What Each Specialist Looks For\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get the input-to-hidden weights\n",
        "W1 = model.W1  # Shape: (n_hidden, n_inputs) = (8, 9)\n",
        "\n",
        "# Get the hidden-to-output weights (tells us how each specialist contributes to final decision)\n",
        "W2 = model.W2.flatten()  # Shape: (8,)\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
        "\n",
        "for i in range(model.n_hidden):\n",
        "    ax = axes[i // 4, i % 4]\n",
        "    \n",
        "    # Reshape this neuron's weights to 3x3\n",
        "    weights = W1[i].reshape(3, 3)\n",
        "    \n",
        "    # Visualize\n",
        "    im = ax.imshow(weights, cmap='RdBu_r', vmin=-np.abs(weights).max(), vmax=np.abs(weights).max())\n",
        "    \n",
        "    # Title with contribution direction\n",
        "    direction = \"â†’VERT\" if W2[i] > 0 else \"â†’HORIZ\"\n",
        "    ax.set_title(f'Specialist {i+1} {direction}\\n(W2={W2[i]:.2f})', fontsize=10)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # Add colorbar for first one\n",
        "    if i == 3:\n",
        "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "plt.suptitle('Hidden Neuron Weights: Red = positive, Blue = negative\\n'\n",
        "             'â†’VERT means this neuron votes for VERTICAL, â†’HORIZ for HORIZONTAL', \n",
        "             fontsize=12, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\"\"\n",
        "INTERPRETATION:\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "Each 3x3 heatmap shows what ONE hidden neuron \"looks for\":\n",
        "  â€¢ RED pixels: This neuron gets EXCITED when these pixels are bright\n",
        "  â€¢ BLUE pixels: This neuron gets INHIBITED when these pixels are bright\n",
        "\n",
        "The \"â†’VERT\" or \"â†’HORIZ\" shows how this specialist votes in the final decision:\n",
        "  â€¢ â†’VERT specialists contribute to \"vertical\" prediction when activated\n",
        "  â€¢ â†’HORIZ specialists contribute to \"horizontal\" prediction when activated\n",
        "\n",
        "Look for patterns! Some specialists might look for:\n",
        "  â€¢ Vertical column patterns (bright red in one column)\n",
        "  â€¢ Horizontal row patterns (bright red in one row)\n",
        "  â€¢ Edge detectors (mixed red/blue patterns)\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 9.6 Interactive Dashboard: Experiment Yourself!\n",
        "\n",
        "Try different hyperparameters and see how they affect performance.\n",
        "\n",
        "| Hyperparameter | What It Controls | Trade-off |\n",
        "|----------------|------------------|-----------|\n",
        "| Hidden neurons | Model complexity | More = can learn more, but risk overfitting |\n",
        "| Learning rate | Step size | Higher = faster but less stable |\n",
        "| Noise level | Data difficulty | Higher = harder to learn |\n",
        "\n",
        "### Experiments to Try\n",
        "\n",
        "**Experiment 1: Varying Model Complexity**\n",
        "\n",
        "| Hidden Neurons | Expected Result |\n",
        "|----------------|-----------------|\n",
        "| 2 | May underfit - not enough capacity |\n",
        "| 8 | Good balance - our default |\n",
        "| 32 | May overfit - watch train/val gap |\n",
        "\n",
        "**Experiment 2: Varying Learning Rate**\n",
        "\n",
        "| Learning Rate | Expected Result |\n",
        "|---------------|-----------------|\n",
        "| 0.01 | Very slow convergence |\n",
        "| 0.5 | Fast, stable (our default) |\n",
        "| 2.0 | May oscillate or diverge |\n",
        "\n",
        "**Experiment 3: Varying Noise Level**\n",
        "\n",
        "| Noise Level | Expected Result |\n",
        "|-------------|-----------------|\n",
        "| 0.0 | Near-perfect accuracy (too easy!) |\n",
        "| 0.15 | Challenging but learnable |\n",
        "| 0.4 | Very difficult, accuracy drops |\n",
        "\n",
        "### What to Watch For\n",
        "\n",
        "- **Healthy training:** Train and val curves decrease together, then flatten\n",
        "- **Overfitting:** Train keeps improving, val gets worse (gap grows)\n",
        "- **Underfitting:** Both curves stay high and flat\n",
        "- **Instability:** Curves jump around wildly (reduce learning rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# INTERACTIVE DASHBOARD: EXPERIMENT WITH HYPERPARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "def run_experiment(n_hidden=8, learning_rate=0.5, noise_level=0.15, n_samples=300, seed=42):\n",
        "    \"\"\"Run a complete experiment with given hyperparameters.\"\"\"\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(f\"EXPERIMENT: hidden={n_hidden}, lr={learning_rate}, noise={noise_level}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Create data\n",
        "    (X_tr, y_tr), (X_v, y_v), (X_te, y_te) = create_train_val_test_split(\n",
        "        n_total=n_samples, noise_level=noise_level, seed=seed\n",
        "    )\n",
        "    \n",
        "    # Create and train model\n",
        "    exp_model = NeuralNetwork(n_inputs=9, n_hidden=n_hidden, n_outputs=1, seed=seed)\n",
        "    exp_model.train(X_tr, y_tr, X_v, y_v, \n",
        "                    learning_rate=learning_rate, epochs=200, \n",
        "                    early_stopping_patience=20, verbose=False)\n",
        "    \n",
        "    # Evaluate\n",
        "    test_loss, test_acc = exp_model.evaluate(X_te, y_te)\n",
        "    \n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    epochs = range(1, len(exp_model.train_loss_history) + 1)\n",
        "    \n",
        "    ax = axes[0]\n",
        "    ax.plot(epochs, exp_model.train_loss_history, 'b-', label='Train')\n",
        "    ax.plot(epochs, exp_model.val_loss_history, 'r-', label='Val')\n",
        "    ax.axvline(exp_model.best_epoch+1, color='g', linestyle='--', label=f'Best: {exp_model.best_epoch+1}')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_title(f'Training Progress\\nFinal Test Acc: {test_acc*100:.1f}%', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    ax = axes[1]\n",
        "    ax.plot(epochs, [a*100 for a in exp_model.train_acc_history], 'b-', label='Train')\n",
        "    ax.plot(epochs, [a*100 for a in exp_model.val_acc_history], 'r-', label='Val')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Accuracy (%)')\n",
        "    ax.set_title(f'Accuracy Progress\\nStopped at epoch {len(epochs)}', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_ylim(40, 105)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return test_acc\n",
        "\n",
        "# Interactive widgets (if available)\n",
        "if WIDGETS_AVAILABLE:\n",
        "    print(\"Interactive dashboard available! Adjust sliders and click 'Run Experiment'.\\n\")\n",
        "    \n",
        "    hidden_slider = widgets.IntSlider(value=8, min=2, max=32, step=2, description='Hidden:')\n",
        "    lr_slider = widgets.FloatSlider(value=0.5, min=0.01, max=2.0, step=0.1, description='Learn Rate:')\n",
        "    noise_slider = widgets.FloatSlider(value=0.15, min=0.0, max=0.5, step=0.05, description='Noise:')\n",
        "    \n",
        "    def on_button_click(b):\n",
        "        clear_output(wait=True)\n",
        "        display(widgets.VBox([hidden_slider, lr_slider, noise_slider, run_button]))\n",
        "        run_experiment(hidden_slider.value, lr_slider.value, noise_slider.value)\n",
        "    \n",
        "    run_button = widgets.Button(description='Run Experiment')\n",
        "    run_button.on_click(on_button_click)\n",
        "    \n",
        "    display(widgets.VBox([hidden_slider, lr_slider, noise_slider, run_button]))\n",
        "else:\n",
        "    print(\"Widgets not available. Running preset experiments instead.\\n\")\n",
        "    \n",
        "    # Run a few preset experiments\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PRESET EXPERIMENTS\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    experiments = [\n",
        "        {\"n_hidden\": 4, \"learning_rate\": 0.5, \"noise_level\": 0.1, \"desc\": \"Simple model, low noise\"},\n",
        "        {\"n_hidden\": 16, \"learning_rate\": 0.5, \"noise_level\": 0.3, \"desc\": \"Complex model, high noise\"},\n",
        "    ]\n",
        "    \n",
        "    for exp in experiments:\n",
        "        print(f\"\\n>>> {exp['desc']}\")\n",
        "        run_experiment(exp['n_hidden'], exp['learning_rate'], exp['noise_level'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 9 Summary: The Complete Journey\n",
        "\n",
        "### Mission Accomplished!\n",
        "\n",
        "We set out in Part 0 to build a neural network that could classify vertical and horizontal lines. Now we have:\n",
        "\n",
        "| Component | Implementation | Part Referenced |\n",
        "|-----------|---------------|-----------------|\n",
        "| **Data representation** | 3x3 images â†’ 9-element vectors | Part 1 (Matrices) |\n",
        "| **Network architecture** | 9 â†’ 8 (ReLU) â†’ 1 (Sigmoid) | Parts 2, 3, 7 |\n",
        "| **Forward propagation** | Matrix operations + activations | Parts 4, 7 |\n",
        "| **Loss function** | Binary Cross-Entropy | Part 5 |\n",
        "| **Training** | Backpropagation + Gradient Descent | Part 5 |\n",
        "| **Evaluation** | Accuracy, Confusion Matrix, F1 | Part 6 |\n",
        "| **Overfitting prevention** | Early stopping + proper sizing | Part 8 |\n",
        "| **Interpretability** | Weight visualization | Part 6 |\n",
        "\n",
        "### The Committee Analogy Complete\n",
        "\n",
        "| Part | Committee Story |\n",
        "|------|-----------------|\n",
        "| 0 | Introduced the committee concept |\n",
        "| 1 | Learned the language (matrices) |\n",
        "| 2 | First committee member joins |\n",
        "| 3 | Member learns to vote (activation) |\n",
        "| 4 | First attempt at decisions |\n",
        "| 5 | Learning from mistakes |\n",
        "| 6 | Evaluating performance |\n",
        "| 7 | Full committee assembled |\n",
        "| 8 | Growing pains addressed |\n",
        "| **9** | **Complete, working committee!** |\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Neural networks are simple at their core** - Just matrix multiplications and non-linear functions\n",
        "2. **Training is optimization** - Find weights that minimize loss on training data\n",
        "3. **Generalization is the goal** - Performance on unseen data is what matters\n",
        "4. **Architecture matters** - Right-sized models with proper activations work best\n",
        "5. **Monitoring is essential** - Track train AND validation metrics\n",
        "\n",
        "### Common Mistakes to Avoid\n",
        "\n",
        "| Mistake | Consequence | How to Avoid |\n",
        "|---------|-------------|--------------|\n",
        "| No validation set | Can't detect overfitting | Always split your data |\n",
        "| Using test data to tune | Overly optimistic results | Keep test data completely separate |\n",
        "| Wrong activation for output | Invalid predictions | Sigmoid for binary, softmax for multi-class |\n",
        "| Too large model for data | Overfitting | Start small, increase if underfitting |\n",
        "| No shuffling | Biased splits | Always shuffle before splitting |\n",
        "| Ignoring learning curves | Miss problems | Plot train/val loss every time |\n",
        "\n",
        "### The Complete Neural Network Checklist\n",
        "\n",
        "**Before Training:**\n",
        "- [ ] Data shuffled and split (train/val/test)\n",
        "- [ ] Model architecture chosen (appropriate size)\n",
        "- [ ] Activation functions set (ReLU hidden, sigmoid output)\n",
        "- [ ] Weights initialized (He for ReLU, Xavier for sigmoid)\n",
        "\n",
        "**During Training:**\n",
        "- [ ] Monitoring both train AND validation loss\n",
        "- [ ] Early stopping configured\n",
        "- [ ] Learning rate reasonable (start with 0.1-1.0)\n",
        "\n",
        "**After Training:**\n",
        "- [ ] Evaluate on TEST set (not validation!)\n",
        "- [ ] Check confusion matrix for error patterns\n",
        "- [ ] Visualize learned features if possible\n",
        "- [ ] Compare train/val/test accuracy for overfitting signs\n",
        "\n",
        "---\n",
        "\n",
        "## Knowledge Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# KNOWLEDGE CHECK - Part 9 (Final Review)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"FINAL KNOWLEDGE CHECK - Complete Neural Network Understanding\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "questions = [\n",
        "    {\n",
        "        \"q\": \"1. In our complete network (9â†’8â†’1), what does the '8' represent?\",\n",
        "        \"options\": [\n",
        "            \"A) The number of training examples\",\n",
        "            \"B) The number of hidden neurons (specialists)\",\n",
        "            \"C) The learning rate\",\n",
        "            \"D) The number of epochs\"\n",
        "        ],\n",
        "        \"answer\": \"B\",\n",
        "        \"explanation\": \"The 8 represents hidden neurons - the 'specialists' in our committee who detect different patterns in the input.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"2. Why do we use ReLU for hidden layers and Sigmoid for output?\",\n",
        "        \"options\": [\n",
        "            \"A) Random choice - they're interchangeable\",\n",
        "            \"B) ReLU prevents vanishing gradients; Sigmoid gives probability output\",\n",
        "            \"C) Sigmoid is faster than ReLU\",\n",
        "            \"D) ReLU only works for hidden layers\"\n",
        "        ],\n",
        "        \"answer\": \"B\",\n",
        "        \"explanation\": \"ReLU (derivative=1 when active) prevents vanishing gradients in deep networks. Sigmoid maps to (0,1) which we interpret as probability.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"3. What is the purpose of the validation set?\",\n",
        "        \"options\": [\n",
        "            \"A) Extra training data\",\n",
        "            \"B) Final performance evaluation\",\n",
        "            \"C) Tune hyperparameters and detect overfitting\",\n",
        "            \"D) Test the code works\"\n",
        "        ],\n",
        "        \"answer\": \"C\",\n",
        "        \"explanation\": \"Validation set is used during training to tune hyperparameters and detect overfitting (early stopping). Test set is for final evaluation.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"4. What does early stopping prevent?\",\n",
        "        \"options\": [\n",
        "            \"A) Underfitting\",\n",
        "            \"B) Overfitting\",\n",
        "            \"C) Slow training\",\n",
        "            \"D) Memory issues\"\n",
        "        ],\n",
        "        \"answer\": \"B\",\n",
        "        \"explanation\": \"Early stopping stops training when validation loss starts increasing, preventing the model from memorizing training data (overfitting).\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"5. In the saliency visualization, what do red pixels in a hidden neuron's weights mean?\",\n",
        "        \"options\": [\n",
        "            \"A) Errors in that pixel\",\n",
        "            \"B) The neuron is broken\",\n",
        "            \"C) The neuron gets excited when those pixels are bright\",\n",
        "            \"D) Those pixels are ignored\"\n",
        "        ],\n",
        "        \"answer\": \"C\",\n",
        "        \"explanation\": \"Positive (red) weights mean the neuron responds strongly when those input pixels are bright. Negative (blue) weights mean inhibition.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"6. What's the complete pipeline for using a neural network?\",\n",
        "        \"options\": [\n",
        "            \"A) Train â†’ Test â†’ Deploy\",\n",
        "            \"B) Data â†’ Train â†’ Evaluate â†’ Deploy\",\n",
        "            \"C) Code â†’ Train â†’ Done\",\n",
        "            \"D) Data (split) â†’ Train (with val monitoring) â†’ Evaluate (on test) â†’ Interpret\"\n",
        "        ],\n",
        "        \"answer\": \"D\",\n",
        "        \"explanation\": \"The complete pipeline: Split data (train/val/test), train with validation monitoring, evaluate on test set, then interpret/deploy.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(f\"\\n{q['q']}\")\n",
        "    for opt in q[\"options\"]:\n",
        "        print(f\"   {opt}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Scroll down for answers...\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ANSWERS\n",
        "print(\"ANSWERS - Final Knowledge Check\")\n",
        "print(\"=\"*70)\n",
        "for i, q in enumerate(questions, 1):\n",
        "    print(f\"\\n{i}. Answer: {q['answer']}\")\n",
        "    print(f\"   {q['explanation']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## What's Next?\n",
        "\n",
        "**Congratulations!** You've completed the full implementation of a neural network from scratch!\n",
        "\n",
        "You now understand:\n",
        "- How neural networks represent and process data\n",
        "- How they learn through backpropagation\n",
        "- How to evaluate and interpret their decisions\n",
        "- How to prevent common pitfalls like overfitting\n",
        "\n",
        "### Coming Up in Part 10: The Future\n",
        "\n",
        "The final notebook will explore:\n",
        "\n",
        "- **What other problems can neural networks solve?**\n",
        "- **CNNs** - Convolutional Neural Networks for images\n",
        "- **RNNs** - Recurrent Neural Networks for sequences\n",
        "- **Transformers** - The architecture behind modern AI\n",
        "- **Resources for continued learning**\n",
        "\n",
        "---\n",
        "\n",
        "**Continue to Part 10:** `part_10_whats_next.ipynb`\n",
        "\n",
        "---\n",
        "\n",
        "## Congratulations!\n",
        "\n",
        "You've built a working neural network from absolute scratch!\n",
        "\n",
        "```\n",
        "              ğŸ‰ MISSION ACCOMPLISHED! ğŸ‰\n",
        "    \n",
        "    From matrices to mastery in 9 parts:\n",
        "    \n",
        "    Part 0: The Mission          â†’ Introduced the problem\n",
        "    Part 1: Matrices             â†’ The language of data\n",
        "    Part 2: Single Neuron        â†’ The building block\n",
        "    Part 3: Activations          â†’ Making decisions\n",
        "    Part 4: Perceptron           â†’ First predictions\n",
        "    Part 5: Training             â†’ Learning from mistakes\n",
        "    Part 6: Evaluation           â†’ Measuring success\n",
        "    Part 7: Hidden Layers        â†’ The full committee\n",
        "    Part 8: Challenges           â†’ Overcoming obstacles\n",
        "    Part 9: Implementation       â†’ COMPLETE SYSTEM!\n",
        "    \n",
        "    You are now ready for deep learning frameworks\n",
        "    like PyTorch and TensorFlow!\n",
        "```\n",
        "\n",
        "*\"The Brain's Decision Committee is fully operational.\"*\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
