{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Network Fundamentals\n",
        "\n",
        "## Part 4: The Perceptron - First Prediction\n",
        "\n",
        "### The Brain's Decision Committee - Chapter 4\n",
        "\n",
        "---\n",
        "\n",
        "**Previously:** In Parts 1-3, our committee member learned:\n",
        "- How to read images as numbers (matrices)\n",
        "- How to weigh evidence and apply personal thresholds (weights & bias)\n",
        "- How to cast meaningful votes (activation functions)\n",
        "\n",
        "**Today's Mission:** Our committee member is now **fully equipped**. It's time for their first real attempt at classifying lines! We'll build a complete Perceptron—the original neural network from 1958—and watch it make predictions. \n",
        "\n",
        "*Spoiler: It won't go well at first. And that's exactly the point.*\n",
        "\n",
        "---\n",
        "\n",
        "### What You'll Learn in Part 4\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. **Understand the Perceptron** - The first working neural network (Rosenblatt, 1958)\n",
        "2. **Generate a dataset** - Create V/H line examples on-the-fly\n",
        "3. **Implement the forward pass** - Input → Weighted Sum → Activation → Output\n",
        "4. **Build a Perceptron class** - Clean, reusable code\n",
        "5. **Make predictions** - Watch the untrained network guess\n",
        "6. **Understand why it fails** - Random weights = random guesses\n",
        "\n",
        "---\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "Make sure you've completed:\n",
        "- **Part 0 & 1:** Welcome & Matrices (`neural_network_fundamentals.ipynb`)\n",
        "- **Part 2:** The First Committee Member (`part_2_single_neuron.ipynb`)\n",
        "- **Part 3:** Activation Functions (`part_3_activation_functions.ipynb`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Setup: Import Dependencies\n",
        "\n",
        "Let's import our tools and recreate the building blocks from previous notebooks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PART 4: THE PERCEPTRON - SETUP\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Try to import ipywidgets for interactive features\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    WIDGETS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WIDGETS_AVAILABLE = False\n",
        "    print(\"Note: ipywidgets not installed. Interactive features will be limited.\")\n",
        "\n",
        "# Set up matplotlib style\n",
        "style_options = ['seaborn-v0_8-whitegrid', 'seaborn-whitegrid', 'ggplot', 'default']\n",
        "for style in style_options:\n",
        "    try:\n",
        "        plt.style.use(style)\n",
        "        break\n",
        "    except OSError:\n",
        "        continue\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "plt.rcParams['font.size'] = 12\n",
        "np.random.seed(42)  # For reproducible random numbers\n",
        "\n",
        "# =============================================================================\n",
        "# RECREATE OUR CANONICAL LINE IMAGES (from Parts 1-3)\n",
        "# =============================================================================\n",
        "\n",
        "# Vertical line: bright pixels in the middle column\n",
        "vertical_line = np.array([\n",
        "    [0, 1, 0],\n",
        "    [0, 1, 0],\n",
        "    [0, 1, 0]\n",
        "])\n",
        "\n",
        "# Horizontal line: bright pixels in the middle row\n",
        "horizontal_line = np.array([\n",
        "    [0, 0, 0],\n",
        "    [1, 1, 1],\n",
        "    [0, 0, 0]\n",
        "])\n",
        "\n",
        "# Flattened versions (9 pixels as a 1D array)\n",
        "vertical_flat = vertical_line.flatten()\n",
        "horizontal_flat = horizontal_line.flatten()\n",
        "\n",
        "print(\"Setup complete!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nOur canonical images (as 3x3 matrices):\")\n",
        "print(f\"\\nVertical Line:            Horizontal Line:\")\n",
        "print(f\"  {vertical_line[0]}                 {horizontal_line[0]}\")\n",
        "print(f\"  {vertical_line[1]}                 {horizontal_line[1]}\")\n",
        "print(f\"  {vertical_line[2]}                 {horizontal_line[2]}\")\n",
        "print(f\"\\nAs flattened vectors (9 pixels):\")\n",
        "print(f\"  Vertical:   {vertical_flat}\")\n",
        "print(f\"  Horizontal: {horizontal_flat}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4.1 What is a Perceptron?\n",
        "\n",
        "The **Perceptron** is the original neural network, invented by Frank Rosenblatt in 1958. It's the simplest possible neural network—just a single neuron!\n",
        "\n",
        "### Why Start with the Perceptron?\n",
        "\n",
        "Before diving in, let's understand why the Perceptron matters:\n",
        "\n",
        "| Question | Answer |\n",
        "|----------|--------|\n",
        "| **What problem does it solve?** | Binary classification (yes/no, cat/dog, vertical/horizontal) |\n",
        "| **Why is it fundamental?** | ALL neural networks are built from Perceptron-like units |\n",
        "| **Why learn it first?** | Simple enough to understand completely, complex enough to be useful |\n",
        "\n",
        "**The Key Insight:** Once you understand ONE neuron, you understand the building block of ALL deep learning. Everything else is just more neurons connected together!\n",
        "\n",
        "### Historical Significance\n",
        "\n",
        "The Perceptron was revolutionary. For the first time, a machine could **learn** to classify patterns without being explicitly programmed. Rosenblatt famously predicted it would eventually \"be able to walk, talk, see, write, reproduce itself and be conscious of its existence.\"\n",
        "\n",
        "(Spoiler: We're still working on most of those.)\n",
        "\n",
        "### Why This Architecture?\n",
        "\n",
        "The Perceptron's design is inspired by biological neurons:\n",
        "\n",
        "| Biological Neuron | Perceptron Equivalent | Purpose |\n",
        "|-------------------|----------------------|---------|\n",
        "| Dendrites (receive signals) | Inputs (x) | Receive information |\n",
        "| Synapses (connection strength) | Weights (w) | Determine importance |\n",
        "| Cell body (integrates) | Weighted sum (Σ) | Combine all inputs |\n",
        "| Axon hillock (threshold) | Bias (b) | Decision threshold |\n",
        "| Axon (fires/doesn't fire) | Activation (f) | Output a decision |\n",
        "\n",
        "This isn't just an analogy - it's the actual inspiration! Rosenblatt was trying to model how real neurons make decisions.\n",
        "\n",
        "### The Architecture\n",
        "\n",
        "A Perceptron is exactly what we built in Parts 2-3:\n",
        "\n",
        "```\n",
        "    INPUTS (x)           WEIGHTS (w)              SUM               ACTIVATION          OUTPUT\n",
        "    ┌─────┐              ┌─────┐                                    \n",
        "    │ x₁  │──────────────│ w₁  │─────┐                              \n",
        "    └─────┘              └─────┘     │                              \n",
        "    ┌─────┐              ┌─────┐     │         ┌─────┐              ┌─────┐\n",
        "    │ x₂  │──────────────│ w₂  │─────┼────────▶│  Σ  │──────────────│ f() │────────▶  ŷ\n",
        "    └─────┘              └─────┘     │         │+bias│              └─────┘\n",
        "    ┌─────┐              ┌─────┐     │         └─────┘              \n",
        "    │ x₃  │──────────────│ w₃  │─────┘                              \n",
        "    └─────┘              └─────┘                                    \n",
        "```\n",
        "\n",
        "### The Complete Formula (Everything Together!)\n",
        "\n",
        "$$\\hat{y} = f\\left(\\sum_{i=1}^{n} w_i \\cdot x_i + b\\right) = f(\\mathbf{w} \\cdot \\mathbf{x} + b)$$\n",
        "\n",
        "Where:\n",
        "- **x** = input vector (our flattened 9-pixel image)\n",
        "- **w** = weight vector (9 weights, one per pixel)\n",
        "- **b** = bias (the personal threshold)\n",
        "- **Σ** = weighted sum (dot product + bias)\n",
        "- **f()** = activation function (sigmoid for us)\n",
        "- **ŷ** = predicted output (probability it's a vertical line)\n",
        "\n",
        "### Committee Analogy: The First Working Committee Member\n",
        "\n",
        "*\"Our committee member is now fully trained in procedure. They can:*\n",
        "1. *Read the evidence (input)*\n",
        "2. *Weigh each piece by importance (weights)*\n",
        "3. *Apply their personal threshold (bias)*\n",
        "4. *Cast a meaningful vote (activation)*\n",
        "\n",
        "*Now it's time for their first REAL case!\"*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4.2 Generating Our Dataset\n",
        "\n",
        "To test our Perceptron, we need examples to classify. Instead of loading a dataset from a file, we'll **generate one on-the-fly**. This is a powerful technique!\n",
        "\n",
        "### First, What IS a Dataset?\n",
        "\n",
        "A **dataset** is a collection of examples used to train or test a machine learning model. Each example has:\n",
        "- **Features (X)**: The input data (for us, 9 pixel values)\n",
        "- **Label (y)**: The correct answer (for us, 0 or 1)\n",
        "\n",
        "This is called **supervised learning** because we \"supervise\" the model by giving it the right answers to learn from.\n",
        "\n",
        "| Term | Meaning | Our Example |\n",
        "|------|---------|-------------|\n",
        "| **Sample** | One example (input + label) | One 3x3 image + whether it's vertical |\n",
        "| **Feature** | One piece of input data | One pixel value |\n",
        "| **Label** | The correct answer | 0 (horizontal) or 1 (vertical) |\n",
        "| **Dataset** | Collection of samples | 100 images with their labels |\n",
        "\n",
        "### Why Do We Need Datasets?\n",
        "\n",
        "Machine learning models learn by example, not by rules:\n",
        "\n",
        "| Traditional Programming | Machine Learning |\n",
        "|------------------------|------------------|\n",
        "| Human writes rules | Human provides examples |\n",
        "| \"If middle column is bright, it's vertical\" | Model sees 50 vertical + 50 horizontal lines |\n",
        "| Rules are explicit | Model discovers patterns itself |\n",
        "| Hard to handle edge cases | Learns from variety in data |\n",
        "\n",
        "**The magic:** Instead of us figuring out the rules, the model discovers them from data!\n",
        "\n",
        "### Our Classification Task\n",
        "\n",
        "| Image Type | Label (y) | Meaning |\n",
        "|------------|-----------|---------|\n",
        "| Vertical Line | 1 | \"This is a vertical line\" |\n",
        "| Horizontal Line | 0 | \"This is a horizontal line\" |\n",
        "\n",
        "### Dataset Requirements\n",
        "\n",
        "For a proper machine learning experiment, we need:\n",
        "1. **Multiple examples** - Not just 2 images, but many variations\n",
        "2. **Balanced classes** - Equal numbers of vertical and horizontal\n",
        "3. **Some variety** - Lines in different positions\n",
        "4. **Optional noise** - To make the problem harder (later)\n",
        "\n",
        "### The Dataset Generator Function\n",
        "\n",
        "We'll create a function that generates any number of V/H line examples:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATASET GENERATOR: Create V/H Line Examples On-The-Fly\n",
        "# =============================================================================\n",
        "\n",
        "def generate_line_dataset(n_samples=100, noise_level=0.0, seed=None):\n",
        "    \"\"\"\n",
        "    Generate a dataset of vertical and horizontal line images.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    n_samples : int\n",
        "        Total number of samples (will be split evenly between V and H)\n",
        "    noise_level : float (0.0 to 0.5)\n",
        "        Amount of random noise to add (0.0 = clean, 0.3 = noisy)\n",
        "    seed : int or None\n",
        "        Random seed for reproducibility\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    X : numpy array of shape (n_samples, 9)\n",
        "        Flattened 3x3 images\n",
        "    y : numpy array of shape (n_samples,)\n",
        "        Labels: 1 for vertical, 0 for horizontal\n",
        "    \"\"\"\n",
        "    \n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    \n",
        "    X = []  # Will hold all images (as flattened arrays)\n",
        "    y = []  # Will hold all labels\n",
        "    \n",
        "    # Generate n_samples/2 vertical lines and n_samples/2 horizontal lines\n",
        "    for i in range(n_samples):\n",
        "        \n",
        "        if i < n_samples // 2:\n",
        "            # ----- VERTICAL LINE (label = 1) -----\n",
        "            # Pick a random column (0, 1, or 2) for variety\n",
        "            col = np.random.randint(0, 3)\n",
        "            \n",
        "            # Create blank 3x3 image\n",
        "            image = np.zeros((3, 3))\n",
        "            \n",
        "            # Fill the chosen column with 1s\n",
        "            image[:, col] = 1\n",
        "            \n",
        "            # Add noise if requested\n",
        "            if noise_level > 0:\n",
        "                image = image + np.random.randn(3, 3) * noise_level\n",
        "                image = np.clip(image, 0, 1)  # Keep values in [0, 1]\n",
        "            \n",
        "            X.append(image.flatten())\n",
        "            y.append(1)  # Label: Vertical\n",
        "            \n",
        "        else:\n",
        "            # ----- HORIZONTAL LINE (label = 0) -----\n",
        "            # Pick a random row (0, 1, or 2) for variety\n",
        "            row = np.random.randint(0, 3)\n",
        "            \n",
        "            # Create blank 3x3 image\n",
        "            image = np.zeros((3, 3))\n",
        "            \n",
        "            # Fill the chosen row with 1s\n",
        "            image[row, :] = 1\n",
        "            \n",
        "            # Add noise if requested\n",
        "            if noise_level > 0:\n",
        "                image = image + np.random.randn(3, 3) * noise_level\n",
        "                image = np.clip(image, 0, 1)\n",
        "            \n",
        "            X.append(image.flatten())\n",
        "            y.append(0)  # Label: Horizontal\n",
        "    \n",
        "    # Convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    \n",
        "    # Shuffle the dataset (so V and H are mixed, not grouped)\n",
        "    shuffle_idx = np.random.permutation(n_samples)\n",
        "    X = X[shuffle_idx]\n",
        "    y = y[shuffle_idx]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "print(\"Dataset generator function created!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# GENERATE AND VISUALIZE OUR DATASET\n",
        "# =============================================================================\n",
        "\n",
        "# Generate 20 clean examples for visualization\n",
        "X_small, y_small = generate_line_dataset(n_samples=20, noise_level=0.0, seed=42)\n",
        "\n",
        "print(\"DATASET GENERATED!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nDataset shape: X = {X_small.shape}, y = {y_small.shape}\")\n",
        "print(f\"  - {X_small.shape[0]} total samples\")\n",
        "print(f\"  - Each sample has {X_small.shape[1]} features (3x3 = 9 pixels)\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(f\"  - Vertical lines (y=1): {np.sum(y_small == 1)} samples\")\n",
        "print(f\"  - Horizontal lines (y=0): {np.sum(y_small == 0)} samples\")\n",
        "\n",
        "# Show first few samples\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FIRST 6 SAMPLES:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i in range(6):\n",
        "    image = X_small[i].reshape(3, 3)\n",
        "    label = y_small[i]\n",
        "    label_name = \"VERTICAL\" if label == 1 else \"HORIZONTAL\"\n",
        "    print(f\"\\nSample {i}: Label = {label} ({label_name})\")\n",
        "    print(f\"  {image[0]}\")\n",
        "    print(f\"  {image[1]}\")\n",
        "    print(f\"  {image[2]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZE SAMPLE IMAGES FROM OUR DATASET\n",
        "# =============================================================================\n",
        "\n",
        "# Show a grid of 10 sample images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    image = X_small[i].reshape(3, 3)\n",
        "    label = y_small[i]\n",
        "    label_name = \"VERTICAL\" if label == 1 else \"HORIZONTAL\"\n",
        "    \n",
        "    ax.imshow(image, cmap='Blues', vmin=0, vmax=1)\n",
        "    ax.set_title(f\"{label_name}\\n(y={label})\", fontsize=10)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # Add grid lines\n",
        "    for j in range(4):\n",
        "        ax.axhline(j - 0.5, color='gray', linewidth=0.5)\n",
        "        ax.axvline(j - 0.5, color='gray', linewidth=0.5)\n",
        "\n",
        "plt.suptitle('Sample Images from Our Generated Dataset', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nNotice: The lines can appear in different positions (left/center/right columns,\")\n",
        "print(\"top/center/bottom rows). This variety makes our dataset more realistic!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4.3 The Forward Pass: Step-by-Step\n",
        "\n",
        "The **forward pass** is how a neural network makes a prediction. Information flows **forward** from input to output.\n",
        "\n",
        "### What is the Forward Pass?\n",
        "\n",
        "The term \"forward pass\" comes from the direction information flows:\n",
        "\n",
        "```\n",
        "INPUT → WEIGHTS × INPUT → ADD BIAS → ACTIVATION → OUTPUT\n",
        "  x    →    w · x       →   + b    →    f(z)    →   ŷ\n",
        "```\n",
        "\n",
        "| Term | Meaning |\n",
        "|------|---------|\n",
        "| **Forward** | Information flows left-to-right, input-to-output |\n",
        "| **Pass** | One complete journey through the network |\n",
        "| **Inference** | Another name for making predictions (vs. training) |\n",
        "\n",
        "**Why \"Forward\"?** Later in Part 5, we'll see the **backward pass** where error flows in the opposite direction. Together, they form the complete learning process!\n",
        "\n",
        "### Forward Pass vs Training\n",
        "\n",
        "It's important to understand when each happens:\n",
        "\n",
        "| Forward Pass (Inference) | Training |\n",
        "|--------------------------|----------|\n",
        "| Make a prediction | Learn from mistakes |\n",
        "| Uses current weights | Updates the weights |\n",
        "| Fast (one direction) | Slower (forward + backward) |\n",
        "| Used after training | Used to create the model |\n",
        "| \"What do I think this is?\" | \"How can I do better?\" |\n",
        "\n",
        "Right now, we're just doing the forward pass - making predictions. Training comes in Part 5!\n",
        "\n",
        "### The Four Steps of a Forward Pass\n",
        "\n",
        "| Step | Operation | Formula | Purpose |\n",
        "|------|-----------|---------|---------|\n",
        "| 1 | Receive Input | x | Get the flattened image (9 values) |\n",
        "| 2 | Weighted Sum | z = w · x | Compute dot product with weights |\n",
        "| 3 | Add Bias | z = z + b | Add the personal threshold |\n",
        "| 4 | Apply Activation | ŷ = f(z) | Convert score to meaningful output |\n",
        "\n",
        "Let's trace through this step-by-step with actual numbers.\n",
        "\n",
        "### Committee Analogy\n",
        "\n",
        "*\"The forward pass is the committee member reading a case file:*\n",
        "1. *They receive the evidence (input)*\n",
        "2. *They multiply each piece by their priority (weights)*\n",
        "3. *They add their personal standard (bias)*\n",
        "4. *They cast their vote (activation)\"*\n",
        "\n",
        "Let's see this in code with EVERY step shown:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# THE FORWARD PASS: Step-by-Step Walkthrough\n",
        "# =============================================================================\n",
        "\n",
        "# Define the sigmoid activation function (from Part 3)\n",
        "def sigmoid(z):\n",
        "    \"\"\"Sigmoid activation: squashes any value to range (0, 1).\"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Let's use our canonical vertical line as the input\n",
        "x = vertical_flat.copy()\n",
        "\n",
        "# Create some random weights (as if the Perceptron is untrained)\n",
        "np.random.seed(123)  # For reproducibility\n",
        "w = np.random.randn(9) * 0.5  # 9 random weights\n",
        "b = np.random.randn() * 0.1    # 1 random bias\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FORWARD PASS: Step-by-Step with Real Numbers\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ----- STEP 1: Receive Input -----\n",
        "print(\"\\n┌─────────────────────────────────────────────────────────────────────┐\")\n",
        "print(\"│ STEP 1: Receive Input                                               │\")\n",
        "print(\"└─────────────────────────────────────────────────────────────────────┘\")\n",
        "print(f\"\\nInput image (as 3x3 grid):\")\n",
        "print(f\"  {x.reshape(3,3)[0]}\")\n",
        "print(f\"  {x.reshape(3,3)[1]}\")\n",
        "print(f\"  {x.reshape(3,3)[2]}\")\n",
        "print(f\"\\nFlattened input vector x:\")\n",
        "print(f\"  x = {x}\")\n",
        "\n",
        "# ----- STEP 2: Weighted Sum (Dot Product) -----\n",
        "print(\"\\n┌─────────────────────────────────────────────────────────────────────┐\")\n",
        "print(\"│ STEP 2: Weighted Sum (Dot Product)                                  │\")\n",
        "print(\"└─────────────────────────────────────────────────────────────────────┘\")\n",
        "print(f\"\\nWeights vector w:\")\n",
        "print(f\"  w = [{', '.join([f'{wi:.3f}' for wi in w])}]\")\n",
        "\n",
        "# Show element-wise multiplication\n",
        "print(f\"\\nElement-wise products (x[i] × w[i]):\")\n",
        "products = x * w\n",
        "print(f\"  = [{', '.join([f'{p:.3f}' for p in products])}]\")\n",
        "\n",
        "# Sum the products\n",
        "dot_product = np.sum(products)\n",
        "print(f\"\\nSum of products (the dot product):\")\n",
        "print(f\"  w · x = {dot_product:.4f}\")\n",
        "\n",
        "# ----- STEP 3: Add Bias -----\n",
        "print(\"\\n┌─────────────────────────────────────────────────────────────────────┐\")\n",
        "print(\"│ STEP 3: Add Bias                                                    │\")\n",
        "print(\"└─────────────────────────────────────────────────────────────────────┘\")\n",
        "print(f\"\\nBias value:\")\n",
        "print(f\"  b = {b:.4f}\")\n",
        "print(f\"\\nPre-activation value z:\")\n",
        "z = dot_product + b\n",
        "print(f\"  z = (w · x) + b\")\n",
        "print(f\"  z = {dot_product:.4f} + {b:.4f}\")\n",
        "print(f\"  z = {z:.4f}\")\n",
        "\n",
        "# ----- STEP 4: Apply Activation -----\n",
        "print(\"\\n┌─────────────────────────────────────────────────────────────────────┐\")\n",
        "print(\"│ STEP 4: Apply Activation (Sigmoid)                                  │\")\n",
        "print(\"└─────────────────────────────────────────────────────────────────────┘\")\n",
        "print(f\"\\nApplying sigmoid to z = {z:.4f}:\")\n",
        "y_hat = sigmoid(z)\n",
        "print(f\"  ŷ = sigmoid(z) = 1 / (1 + e^(-z))\")\n",
        "print(f\"  ŷ = 1 / (1 + e^(-{z:.4f}))\")\n",
        "print(f\"  ŷ = {y_hat:.4f}\")\n",
        "\n",
        "# ----- FINAL RESULT -----\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FORWARD PASS COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nFinal output: ŷ = {y_hat:.4f}\")\n",
        "print(f\"\\nInterpretation: The Perceptron is {y_hat*100:.1f}% confident this is a VERTICAL line.\")\n",
        "print(f\"\\nPrediction: {'VERTICAL (y=1)' if y_hat >= 0.5 else 'HORIZONTAL (y=0)'}\")\n",
        "print(f\"Actual label: VERTICAL (y=1)\")\n",
        "print(f\"{'✓ Correct!' if y_hat >= 0.5 else '✗ Wrong!'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4.4 Building the Perceptron Class\n",
        "\n",
        "Now let's package everything into a clean, reusable **Perceptron class**. This is how real neural networks are implemented - as modular, reusable code.\n",
        "\n",
        "### Why Use a Class?\n",
        "\n",
        "In programming, a **class** is a blueprint for creating objects. For neural networks, classes help us:\n",
        "\n",
        "| Benefit | Explanation |\n",
        "|---------|-------------|\n",
        "| **Organization** | Keep weights, bias, and methods together |\n",
        "| **Reusability** | Create multiple Perceptrons easily |\n",
        "| **State** | Remember weights between method calls |\n",
        "| **Readability** | `perceptron.predict(x)` is clearer than raw math |\n",
        "\n",
        "### What Our Perceptron Needs\n",
        "\n",
        "| Component | What It Does |\n",
        "|-----------|--------------|\n",
        "| `__init__()` | Initialize weights and bias (randomly) |\n",
        "| `forward()` | Compute the forward pass (returns probability) |\n",
        "| `predict()` | Make a binary decision (0 or 1) |\n",
        "\n",
        "### Why Random Initialization?\n",
        "\n",
        "Before training, we need some starting values for weights. Why random?\n",
        "\n",
        "| Alternative | Problem |\n",
        "|-------------|---------|\n",
        "| All zeros | All neurons would output the same thing! |\n",
        "| All ones | Would overwhelm the activation function |\n",
        "| Same value everywhere | All weights would update identically |\n",
        "| **Random small values** | ✓ Breaks symmetry, allows diverse learning |\n",
        "\n",
        "**Key Insight:** The SPECIFIC random values don't matter much - training will adjust them. But they must be:\n",
        "- **Small** (typically between -0.1 and 0.1) to avoid saturating the sigmoid\n",
        "- **Different** from each other to allow diverse learning\n",
        "\n",
        "The scale `* 0.1` keeps initial outputs near 0.5 (middle of sigmoid), where learning is fastest.\n",
        "\n",
        "### The Core Math (Keep It Simple!)\n",
        "\n",
        "All the math fits in just two lines:\n",
        "\n",
        "**Forward pass:** `z = np.dot(weights, x) + bias`\n",
        "\n",
        "**Activation:** `output = 1 / (1 + np.exp(-z))`\n",
        "\n",
        "**Prediction:** `1 if output >= 0.5 else 0`\n",
        "\n",
        "### Understanding the Threshold (0.5)\n",
        "\n",
        "The sigmoid outputs a **probability** between 0 and 1. To make a **decision**, we need a threshold:\n",
        "\n",
        "| Output | Decision Rule | Prediction |\n",
        "|--------|---------------|------------|\n",
        "| 0.0 - 0.49 | \"Probably NOT vertical\" | 0 (Horizontal) |\n",
        "| 0.50 - 1.0 | \"Probably IS vertical\" | 1 (Vertical) |\n",
        "\n",
        "**Why 0.5?** It's the natural midpoint - if the model is >50% confident it's vertical, we call it vertical.\n",
        "\n",
        "Note: In some applications, you might use a different threshold (e.g., 0.7 for \"high confidence only\"). But 0.5 is the standard starting point.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# THE PERCEPTRON CLASS: Clean, Reusable Implementation\n",
        "# =============================================================================\n",
        "\n",
        "class Perceptron:\n",
        "    \"\"\"\n",
        "    A single-layer Perceptron for binary classification.\n",
        "    \n",
        "    This is the simplest possible neural network - just one neuron!\n",
        "    \n",
        "    Attributes:\n",
        "        n_inputs (int): Number of input features (9 for our 3x3 images)\n",
        "        weights (array): One weight per input feature\n",
        "        bias (float): The threshold/offset term\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_inputs):\n",
        "        \"\"\"\n",
        "        Initialize the Perceptron with random weights and bias.\n",
        "        \n",
        "        Parameters:\n",
        "            n_inputs: Number of input features (pixels in our image)\n",
        "        \"\"\"\n",
        "        # Random weights, small values centered around 0\n",
        "        self.weights = np.random.randn(n_inputs) * 0.1\n",
        "        \n",
        "        # Bias starts at 0\n",
        "        self.bias = 0.0\n",
        "        \n",
        "        # Store for reference\n",
        "        self.n_inputs = n_inputs\n",
        "        \n",
        "        # Storage for debugging/visualization\n",
        "        self.last_z = None    # Pre-activation value\n",
        "        self.last_output = None  # Final output\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute the forward pass - make a prediction.\n",
        "        \n",
        "        Parameters:\n",
        "            x: Input array (can be 2D image or 1D flattened)\n",
        "        \n",
        "        Returns:\n",
        "            float: Probability between 0 and 1\n",
        "        \"\"\"\n",
        "        # Ensure x is a 1D array\n",
        "        x = np.array(x).flatten()\n",
        "        \n",
        "        # STEP 1 & 2: Weighted sum + bias\n",
        "        # Formula: z = w · x + b\n",
        "        self.last_z = np.dot(self.weights, x) + self.bias\n",
        "        \n",
        "        # STEP 3: Apply sigmoid activation\n",
        "        # Formula: output = 1 / (1 + e^(-z))\n",
        "        self.last_output = 1 / (1 + np.exp(-self.last_z))\n",
        "        \n",
        "        return self.last_output\n",
        "    \n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Make a binary prediction (0 or 1).\n",
        "        \n",
        "        Parameters:\n",
        "            x: Input array\n",
        "        \n",
        "        Returns:\n",
        "            int: 0 (horizontal) or 1 (vertical)\n",
        "        \"\"\"\n",
        "        probability = self.forward(x)\n",
        "        return 1 if probability >= 0.5 else 0\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f\"Perceptron(inputs={self.n_inputs})\"\n",
        "\n",
        "\n",
        "# Create our Perceptron!\n",
        "print(\"=\"*60)\n",
        "print(\"PERCEPTRON CLASS CREATED!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Instantiate a Perceptron for 9 inputs (3x3 = 9 pixels)\n",
        "perceptron = Perceptron(n_inputs=9)\n",
        "\n",
        "print(f\"\\nOur Perceptron: {perceptron}\")\n",
        "print(f\"\\nInitial weights (random, untrained):\")\n",
        "print(f\"  Shape: {perceptron.weights.shape}\")\n",
        "print(f\"  Values: [{', '.join([f'{w:.3f}' for w in perceptron.weights])}]\")\n",
        "print(f\"\\nInitial bias: {perceptron.bias}\")\n",
        "print(\"\\nThe Perceptron is ready, but completely UNTRAINED!\")\n",
        "print(\"Its weights are random - it doesn't know what a vertical line looks like.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4.5 Initial Predictions: The Confused Perceptron\n",
        "\n",
        "Now the moment of truth! Let's see how our untrained Perceptron performs.\n",
        "\n",
        "### What is Accuracy?\n",
        "\n",
        "**Accuracy** is the simplest way to measure how well a model performs:\n",
        "\n",
        "$$\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} \\times 100\\%$$\n",
        "\n",
        "For example:\n",
        "- 80 correct out of 100 = 80% accuracy\n",
        "- 50 correct out of 100 = 50% accuracy\n",
        "\n",
        "### The Baseline: What's \"Random Guessing\"?\n",
        "\n",
        "For any classification task, there's a **baseline accuracy** - what you'd get by guessing randomly:\n",
        "\n",
        "| Task Type | Classes | Random Baseline |\n",
        "|-----------|---------|-----------------|\n",
        "| Binary (yes/no) | 2 | 50% |\n",
        "| 3-way choice | 3 | 33% |\n",
        "| 10-way choice | 10 | 10% |\n",
        "\n",
        "**Our task is binary** (vertical vs horizontal), so random guessing gives 50%.\n",
        "\n",
        "**Why this matters:** If your model gets 50% on binary classification, it's learned NOTHING. It's no better than flipping a coin!\n",
        "\n",
        "### What We Expect\n",
        "\n",
        "Since the weights are random, the Perceptron has no idea what it's doing. It's like asking someone who's never seen a line before to classify them.\n",
        "\n",
        "**Expected accuracy:** Around 50% (random guessing for binary classification)\n",
        "\n",
        "### Committee Analogy\n",
        "\n",
        "*\"Our committee member has been trained in procedure, but has never seen an actual case. They're about to make judgments based on completely arbitrary priorities. The results won't be pretty...\"*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TESTING THE UNTRAINED PERCEPTRON ON OUR CANONICAL EXAMPLES\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TESTING UNTRAINED PERCEPTRON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Test on our canonical vertical line\n",
        "print(\"\\n┌─────────────────────────────────────────────────────────────────────┐\")\n",
        "print(\"│ Test 1: VERTICAL LINE                                               │\")\n",
        "print(\"└─────────────────────────────────────────────────────────────────────┘\")\n",
        "print(f\"\\nImage (3x3):\")\n",
        "print(f\"  {vertical_line[0]}\")\n",
        "print(f\"  {vertical_line[1]}\")\n",
        "print(f\"  {vertical_line[2]}\")\n",
        "\n",
        "prob_vertical = perceptron.forward(vertical_flat)\n",
        "pred_vertical = perceptron.predict(vertical_flat)\n",
        "actual_vertical = 1\n",
        "\n",
        "print(f\"\\nForward pass calculation:\")\n",
        "print(f\"  z = w · x + b = {perceptron.last_z:.4f}\")\n",
        "print(f\"  output = sigmoid(z) = {prob_vertical:.4f}\")\n",
        "print(f\"\\nPrediction: {pred_vertical} ({'VERTICAL' if pred_vertical == 1 else 'HORIZONTAL'})\")\n",
        "print(f\"Actual:     {actual_vertical} (VERTICAL)\")\n",
        "print(f\"Result:     {'CORRECT!' if pred_vertical == actual_vertical else 'WRONG!'}\")\n",
        "\n",
        "# Test on our canonical horizontal line\n",
        "print(\"\\n┌─────────────────────────────────────────────────────────────────────┐\")\n",
        "print(\"│ Test 2: HORIZONTAL LINE                                             │\")\n",
        "print(\"└─────────────────────────────────────────────────────────────────────┘\")\n",
        "print(f\"\\nImage (3x3):\")\n",
        "print(f\"  {horizontal_line[0]}\")\n",
        "print(f\"  {horizontal_line[1]}\")\n",
        "print(f\"  {horizontal_line[2]}\")\n",
        "\n",
        "prob_horizontal = perceptron.forward(horizontal_flat)\n",
        "pred_horizontal = perceptron.predict(horizontal_flat)\n",
        "actual_horizontal = 0\n",
        "\n",
        "print(f\"\\nForward pass calculation:\")\n",
        "print(f\"  z = w · x + b = {perceptron.last_z:.4f}\")\n",
        "print(f\"  output = sigmoid(z) = {prob_horizontal:.4f}\")\n",
        "print(f\"\\nPrediction: {pred_horizontal} ({'VERTICAL' if pred_horizontal == 1 else 'HORIZONTAL'})\")\n",
        "print(f\"Actual:     {actual_horizontal} (HORIZONTAL)\")\n",
        "print(f\"Result:     {'CORRECT!' if pred_horizontal == actual_horizontal else 'WRONG!'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TESTING ON THE FULL DATASET: Calculate Accuracy\n",
        "# =============================================================================\n",
        "\n",
        "# Generate a larger dataset for proper testing\n",
        "X_test, y_test = generate_line_dataset(n_samples=100, noise_level=0.0, seed=99)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FULL DATASET EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nDataset: {len(y_test)} samples ({sum(y_test)} vertical, {len(y_test) - sum(y_test)} horizontal)\")\n",
        "\n",
        "# Make predictions on all samples\n",
        "predictions = []\n",
        "correct = 0\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    pred = perceptron.predict(X_test[i])\n",
        "    predictions.append(pred)\n",
        "    if pred == y_test[i]:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / len(y_test) * 100\n",
        "\n",
        "# Display results table (first 10 samples)\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"FIRST 10 PREDICTIONS:\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Sample':<8} {'Actual':<12} {'Predicted':<12} {'Result':<10}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for i in range(10):\n",
        "    actual_name = \"VERTICAL\" if y_test[i] == 1 else \"HORIZONTAL\"\n",
        "    pred_name = \"VERTICAL\" if predictions[i] == 1 else \"HORIZONTAL\"\n",
        "    result = \"Correct\" if predictions[i] == y_test[i] else \"WRONG\"\n",
        "    symbol = \"+\" if predictions[i] == y_test[i] else \"X\"\n",
        "    print(f\"  {i:<6} {actual_name:<12} {pred_name:<12} {symbol} {result}\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ACCURACY SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n  Total samples:  {len(y_test)}\")\n",
        "print(f\"  Correct:        {correct}\")\n",
        "print(f\"  Wrong:          {len(y_test) - correct}\")\n",
        "print(f\"\\n  ACCURACY: {accuracy:.1f}%\")\n",
        "print(f\"\\n  Expected (random guessing): ~50%\")\n",
        "print(f\"  Difference from random: {abs(accuracy - 50):.1f}%\")\n",
        "\n",
        "if accuracy > 55:\n",
        "    print(\"\\n  Hmm, slightly better than random - got lucky with the random weights!\")\n",
        "elif accuracy < 45:\n",
        "    print(\"\\n  Worse than random! The weights are actually hurting performance.\")\n",
        "else:\n",
        "    print(\"\\n  As expected: basically random guessing. The Perceptron is CONFUSED!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4.6 Why It's Wrong: Understanding the Problem\n",
        "\n",
        "Our Perceptron performed around 50% accuracy - basically coin-flipping. Why?\n",
        "\n",
        "### Understanding What Weights Actually DO\n",
        "\n",
        "The weights are the Perceptron's **knowledge**. Each weight answers the question:\n",
        "\n",
        "> \"How important is this input for making the decision?\"\n",
        "\n",
        "| Weight Value | Meaning |\n",
        "|--------------|---------|\n",
        "| **Large positive** (+1.0) | \"This input STRONGLY suggests class 1\" |\n",
        "| **Small positive** (+0.1) | \"This input slightly suggests class 1\" |\n",
        "| **Near zero** (0.0) | \"This input doesn't matter\" |\n",
        "| **Small negative** (-0.1) | \"This input slightly suggests class 0\" |\n",
        "| **Large negative** (-1.0) | \"This input STRONGLY suggests class 0\" |\n",
        "\n",
        "### What We WANT the Perceptron to Learn\n",
        "\n",
        "For detecting vertical lines, the ideal weights would encode this knowledge:\n",
        "\n",
        "```\n",
        "    \"Pixels in columns = IMPORTANT for vertical detection\"\n",
        "    \"Pixels in rows = NOT important (or negative) for vertical detection\"\n",
        "```\n",
        "\n",
        "In weight terms:\n",
        "- Middle column pixels → HIGH positive weights (vertical lines have these lit up)\n",
        "- Other pixels → LOW or NEGATIVE weights (don't indicate verticality)\n",
        "\n",
        "### The Problem: Random Weights = No Knowledge\n",
        "\n",
        "Our current weights are random - they encode NO knowledge about vertical lines:\n",
        "- Some weights are positive when they should be negative\n",
        "- Some weights are large when they should be small\n",
        "- There's no pattern that matches \"vertical line detection\"\n",
        "\n",
        "### Feature Detection: What the Perceptron is Trying to Become\n",
        "\n",
        "A **feature detector** is a model that responds strongly to specific patterns. Our goal:\n",
        "\n",
        "| Input Pattern | Ideal Perceptron Response |\n",
        "|---------------|---------------------------|\n",
        "| Vertical line (any column) | High output (close to 1.0) |\n",
        "| Horizontal line (any row) | Low output (close to 0.0) |\n",
        "\n",
        "**Right now:** The Perceptron is NOT a feature detector - it's just random noise.\n",
        "\n",
        "**After training:** It WILL become a vertical line feature detector!\n",
        "\n",
        "### The Problem: Random Weights = Random Decisions\n",
        "\n",
        "Let's visualize what our random weights actually look like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZING THE PROBLEM: Random Weights vs Ideal Weights\n",
        "# =============================================================================\n",
        "\n",
        "# What ideal weights for a vertical detector should look like\n",
        "ideal_weights = np.array([\n",
        "    [-1,  2, -1],   # Top row: look for middle\n",
        "    [-1,  2, -1],   # Middle row: look for middle\n",
        "    [-1,  2, -1]    # Bottom row: look for middle\n",
        "]).flatten() * 0.5\n",
        "\n",
        "# Our actual (random) weights\n",
        "actual_weights = perceptron.weights\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "\n",
        "# Plot 1: Random weights (what we have)\n",
        "ax1 = axes[0]\n",
        "weights_grid = actual_weights.reshape(3, 3)\n",
        "im1 = ax1.imshow(weights_grid, cmap='RdBu', vmin=-0.5, vmax=0.5)\n",
        "ax1.set_title('Our Random Weights\\n(Untrained)', fontsize=12, fontweight='bold')\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        ax1.text(j, i, f'{weights_grid[i,j]:.2f}', ha='center', va='center', fontsize=10)\n",
        "plt.colorbar(im1, ax=ax1, label='Weight value')\n",
        "\n",
        "# Plot 2: Ideal weights (what we need)\n",
        "ax2 = axes[1]\n",
        "ideal_grid = ideal_weights.reshape(3, 3)\n",
        "im2 = ax2.imshow(ideal_grid, cmap='RdBu', vmin=-0.5, vmax=0.5)\n",
        "ax2.set_title('Ideal Weights\\n(What we need)', fontsize=12, fontweight='bold')\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        ax2.text(j, i, f'{ideal_grid[i,j]:.2f}', ha='center', va='center', fontsize=10)\n",
        "plt.colorbar(im2, ax=ax2, label='Weight value')\n",
        "\n",
        "# Plot 3: A vertical line (what we're trying to detect)\n",
        "ax3 = axes[2]\n",
        "im3 = ax3.imshow(vertical_line, cmap='Blues', vmin=0, vmax=1)\n",
        "ax3.set_title('Vertical Line\\n(What we detect)', fontsize=12, fontweight='bold')\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        ax3.text(j, i, f'{vertical_line[i,j]}', ha='center', va='center', fontsize=10)\n",
        "plt.colorbar(im3, ax=ax3, label='Pixel value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show the key insight\n",
        "print(\"\\nKEY INSIGHT: Why Random Weights Fail\")\n",
        "print(\"=\"*60)\n",
        "print(\"\"\"\n",
        "IDEAL weights for vertical detection should have:\n",
        "  - HIGH values in the middle column (where vertical lines are)\n",
        "  - LOW or NEGATIVE values elsewhere\n",
        "\n",
        "Our RANDOM weights have no pattern - they're just noise!\n",
        "\n",
        "The Perceptron doesn't KNOW what vertical lines look like yet.\n",
        "It needs to LEARN the right weights through TRAINING.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# WHAT IF WE HAD IDEAL WEIGHTS? (Sneak Preview)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"WHAT IF WE HAD THE RIGHT WEIGHTS? (A Preview)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create a new Perceptron and give it ideal weights\n",
        "ideal_perceptron = Perceptron(n_inputs=9)\n",
        "ideal_perceptron.weights = ideal_weights.copy()\n",
        "ideal_perceptron.bias = -1.5  # A good threshold\n",
        "\n",
        "print(\"\\nIdeal weights (as 3x3 grid):\")\n",
        "print(f\"  {ideal_perceptron.weights.reshape(3,3)[0]}\")\n",
        "print(f\"  {ideal_perceptron.weights.reshape(3,3)[1]}\")\n",
        "print(f\"  {ideal_perceptron.weights.reshape(3,3)[2]}\")\n",
        "print(f\"\\nBias: {ideal_perceptron.bias}\")\n",
        "\n",
        "# Test on the same dataset\n",
        "correct_ideal = 0\n",
        "for i in range(len(X_test)):\n",
        "    if ideal_perceptron.predict(X_test[i]) == y_test[i]:\n",
        "        correct_ideal += 1\n",
        "\n",
        "accuracy_ideal = correct_ideal / len(y_test) * 100\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"COMPARISON:\")\n",
        "print(\"-\"*70)\n",
        "print(f\"\\n  Random weights accuracy:  {accuracy:.1f}%\")\n",
        "print(f\"  Ideal weights accuracy:   {accuracy_ideal:.1f}%\")\n",
        "print(f\"\\n  Improvement: +{accuracy_ideal - accuracy:.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"THE BIG QUESTION:\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "How do we get from RANDOM weights to IDEAL weights?\n",
        "\n",
        "We don't want to hand-design them (that defeats the purpose!).\n",
        "We want the Perceptron to LEARN them automatically.\n",
        "\n",
        "This is what TRAINING does - and it's the topic of Part 5!\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 4 Summary: What We've Learned\n",
        "\n",
        "### Key Concepts Mastered\n",
        "\n",
        "| Concept | What It Is | Why It Matters |\n",
        "|---------|------------|----------------|\n",
        "| **Perceptron** | Single-neuron neural network | Simplest possible NN, building block for larger networks |\n",
        "| **Dataset Generation** | Creating training examples | We can test our models without external data |\n",
        "| **Forward Pass** | Input → Output computation | This is how predictions are made |\n",
        "| **Random Initialization** | Starting with random weights | The beginning state before learning |\n",
        "\n",
        "### The Complete Perceptron Formula\n",
        "\n",
        "$$\\hat{y} = \\sigma(w \\cdot x + b) = \\frac{1}{1 + e^{-(w \\cdot x + b)}}$$\n",
        "\n",
        "Or in code:\n",
        "```python\n",
        "z = np.dot(weights, x) + bias    # Weighted sum\n",
        "output = 1 / (1 + np.exp(-z))    # Sigmoid activation\n",
        "prediction = 1 if output >= 0.5 else 0\n",
        "```\n",
        "\n",
        "### Committee Analogy Progress\n",
        "\n",
        "| Part | What Happened |\n",
        "|------|---------------|\n",
        "| Part 1 | Committee learned to read evidence (matrices) |\n",
        "| Part 2 | First member learned to weigh evidence (weights/bias) |\n",
        "| Part 3 | Member learned to cast meaningful votes (activation) |\n",
        "| **Part 4** | **Member attempted their first case - and FAILED!** |\n",
        "| Part 5 | (Next) Member learns from their mistakes |\n",
        "\n",
        "### Key Insight\n",
        "\n",
        "**Random weights = Random guessing**\n",
        "\n",
        "An untrained Perceptron has no knowledge. Its weights are just noise. To become useful, it must **learn** the right weights by seeing examples and adjusting based on its mistakes.\n",
        "\n",
        "---\n",
        "\n",
        "## Knowledge Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# KNOWLEDGE CHECK - Part 4\n",
        "# =============================================================================\n",
        "\n",
        "print(\"KNOWLEDGE CHECK - Part 4: The Perceptron\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nAnswer these questions to test your understanding:\\n\")\n",
        "\n",
        "questions = [\n",
        "    {\n",
        "        \"q\": \"1. What are the steps of a forward pass (in order)?\",\n",
        "        \"options\": [\n",
        "            \"A) Activation -> Weighted Sum -> Output\",\n",
        "            \"B) Weighted Sum -> Add Bias -> Activation -> Output\",\n",
        "            \"C) Input -> Output -> Activation\",\n",
        "            \"D) Bias -> Weights -> Sigmoid\"\n",
        "        ],\n",
        "        \"answer\": \"B\",\n",
        "        \"explanation\": \"The forward pass is: (1) compute weighted sum of inputs, (2) add bias, (3) apply activation function, (4) get output.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"2. Why does an untrained Perceptron get ~50% accuracy?\",\n",
        "        \"options\": [\n",
        "            \"A) Because sigmoid always outputs 0.5\",\n",
        "            \"B) Because the dataset is unbalanced\",\n",
        "            \"C) Because random weights give random predictions\",\n",
        "            \"D) Because the bias is always 0\"\n",
        "        ],\n",
        "        \"answer\": \"C\",\n",
        "        \"explanation\": \"Random weights have no meaningful pattern, so the Perceptron essentially guesses randomly. For binary classification, random guessing gives ~50% accuracy.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"3. What does the forward pass output for binary classification?\",\n",
        "        \"options\": [\n",
        "            \"A) Always 0 or 1 exactly\",\n",
        "            \"B) A probability between 0 and 1\",\n",
        "            \"C) Any real number\",\n",
        "            \"D) The raw weighted sum\"\n",
        "        ],\n",
        "        \"answer\": \"B\",\n",
        "        \"explanation\": \"The sigmoid activation squashes the output to a probability between 0 and 1. We then threshold at 0.5 to get a binary prediction.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"4. For a vertical line detector, where should the weights be highest?\",\n",
        "        \"options\": [\n",
        "            \"A) In the corners\",\n",
        "            \"B) In the middle column\",\n",
        "            \"C) In the middle row\",\n",
        "            \"D) Equally everywhere\"\n",
        "        ],\n",
        "        \"answer\": \"B\",\n",
        "        \"explanation\": \"Vertical lines appear in columns. High weights in the middle column will give high scores when vertical pixels align with them.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"5. Who invented the Perceptron?\",\n",
        "        \"options\": [\n",
        "            \"A) Geoffrey Hinton\",\n",
        "            \"B) Frank Rosenblatt\",\n",
        "            \"C) Yann LeCun\",\n",
        "            \"D) Alan Turing\"\n",
        "        ],\n",
        "        \"answer\": \"B\",\n",
        "        \"explanation\": \"Frank Rosenblatt invented the Perceptron in 1958 at Cornell. It was the first neural network that could learn!\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(q[\"q\"])\n",
        "    for opt in q[\"options\"]:\n",
        "        print(f\"   {opt}\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Scroll down for answers...\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ANSWERS - Knowledge Check Part 4\n",
        "# =============================================================================\n",
        "\n",
        "print(\"ANSWERS - Part 4 Knowledge Check\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, q in enumerate(questions, 1):\n",
        "    print(f\"\\n{i}. Answer: {q['answer']}\")\n",
        "    print(f\"   Explanation: {q['explanation']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"How did you do?\")\n",
        "print(\"  5/5: Perceptron Expert!\")\n",
        "print(\"  4/5: Great understanding!\")\n",
        "print(\"  3/5: Review the sections you missed\")\n",
        "print(\"  <3:  Re-read Part 4 before continuing\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## What's Next?\n",
        "\n",
        "You've completed Part 4! Our Perceptron is built but confused - it makes random guesses because its weights are random.\n",
        "\n",
        "### Coming Up in Part 5: Training - Learning from Mistakes\n",
        "\n",
        "In Part 5, we'll cover:\n",
        "- **Loss Functions** - Measuring \"how wrong\" a prediction is\n",
        "- **Gradient Descent** - Finding better weights\n",
        "- **Backpropagation** - How errors flow backward\n",
        "- **The Training Loop** - Iteratively improving weights\n",
        "- **Watch It Learn** - See accuracy improve from 50% to 90%+!\n",
        "\n",
        "---\n",
        "\n",
        "**Continue to Part 5:** `part_5_training.ipynb`\n",
        "\n",
        "---\n",
        "\n",
        "*\"The Perceptron is ready. The data is ready. Now it's time to LEARN.\"*\n",
        "\n",
        "**The Brain's Decision Committee** - Learning to See, One Step at a Time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
