{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Network Fundamentals\n",
        "\n",
        "## Part 7: Hidden Layers - The Full Committee\n",
        "\n",
        "### The Brain's Decision Committee - Chapter 7\n",
        "\n",
        "---\n",
        "\n",
        "## The Story So Far...\n",
        "\n",
        "In Parts 1-6, we built and trained a single neuron (Perceptron) that became an expert at detecting vertical vs horizontal lines. We evaluated its performance, understood its decision-making through saliency maps, and confirmed it learned the right patterns.\n",
        "\n",
        "**But our expert has a limitation.**\n",
        "\n",
        "A single neuron can only draw ONE straight line to separate categories. Some problems require more complex boundaries - curves, multiple regions, or intricate patterns.\n",
        "\n",
        "*\"Our single committee member has done well, but some problems are too complex for one person. It's time to assemble a **full committee** with specialists.\"*\n",
        "\n",
        "---\n",
        "\n",
        "## What You'll Learn in Part 7\n",
        "\n",
        "By the end of this notebook, you will understand:\n",
        "\n",
        "1. **Why single neurons fail** - The famous XOR problem AND challenging V/H variations\n",
        "2. **What hidden layers are** - Adding neurons between input and output\n",
        "3. **How hidden neurons specialize** - Different neurons detect different features\n",
        "4. **The Multi-Layer Perceptron (MLP)** - A complete neural network architecture\n",
        "5. **Forward propagation** - How data flows through multiple layers\n",
        "6. **Backpropagation through layers** - Training with chain rule\n",
        "7. **Universal approximation** - Why deep networks can learn (almost) anything\n",
        "\n",
        "### Two Complementary Examples\n",
        "\n",
        "In this notebook, we'll explore limitations of single neurons through **two lenses**:\n",
        "\n",
        "| Example | Why Include It? |\n",
        "|---------|-----------------|\n",
        "| **XOR Problem** | The famous textbook example - you'll encounter this everywhere in ML literature |\n",
        "| **Challenging V/H Lines** | Our continuing story - noisy images, multiple positions, harder patterns |\n",
        "\n",
        "Both examples teach the same lesson: **some problems need multiple neurons working together**.\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Make sure you've completed:\n",
        "- **Parts 0-1:** Matrices (`neural_network_fundamentals.ipynb`)\n",
        "- **Part 2:** Single Neuron (`part_2_single_neuron.ipynb`)\n",
        "- **Part 3:** Activation Functions (`part_3_activation_functions.ipynb`)\n",
        "- **Part 4:** The Perceptron (`part_4_perceptron.ipynb`)\n",
        "- **Part 5:** Training (`part_5_training.ipynb`)\n",
        "- **Part 6:** Evaluation (`part_6_evaluation.ipynb`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Setup: Import Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PART 7: HIDDEN LAYERS - SETUP AND IMPORTS\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Try to import ipywidgets for interactive features\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    WIDGETS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WIDGETS_AVAILABLE = False\n",
        "    print(\"Note: ipywidgets not installed. Interactive features will be limited.\")\n",
        "\n",
        "# Set up matplotlib style\n",
        "style_options = ['seaborn-v0_8-whitegrid', 'seaborn-whitegrid', 'ggplot', 'default']\n",
        "for style in style_options:\n",
        "    try:\n",
        "        plt.style.use(style)\n",
        "        break\n",
        "    except OSError:\n",
        "        continue\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "plt.rcParams['font.size'] = 12\n",
        "np.random.seed(42)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Helper functions from previous notebooks\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"Sigmoid activation: maps any value to range (0, 1).\"\"\"\n",
        "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    \"\"\"Derivative of sigmoid: Ïƒ(z) * (1 - Ïƒ(z))\"\"\"\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def relu(z):\n",
        "    \"\"\"ReLU activation: max(0, z)\"\"\"\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    \"\"\"Derivative of ReLU: 1 if z > 0, else 0\"\"\"\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "print(\"Setup complete!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7.1 The Limitation of Single Neurons: The XOR Problem\n",
        "\n",
        "Our Perceptron works great for vertical vs horizontal lines. But there's a famous problem that NO single neuron can solve: the **XOR problem**.\n",
        "\n",
        "### What IS XOR?\n",
        "\n",
        "**XOR** (exclusive OR) is a logical operation that outputs TRUE when inputs are DIFFERENT:\n",
        "\n",
        "| Input A | Input B | XOR Output |\n",
        "|---------|---------|------------|\n",
        "| 0 | 0 | 0 |\n",
        "| 0 | 1 | 1 |\n",
        "| 1 | 0 | 1 |\n",
        "| 1 | 1 | 0 |\n",
        "\n",
        "In words: \"TRUE if one or the other, but not both.\"\n",
        "\n",
        "**Real-world examples of XOR:**\n",
        "- A light switch: Flip EITHER switch to change the light, but if BOTH are up (or both down), it's off\n",
        "- Password requirements: \"Use uppercase OR numbers\" (but having BOTH doesn't double-satisfy it)\n",
        "\n",
        "### Why Can't a Single Neuron Solve XOR?\n",
        "\n",
        "A single neuron creates a **linear decision boundary** - a straight line that separates the two classes.\n",
        "\n",
        "### What IS a Decision Boundary?\n",
        "\n",
        "A **decision boundary** is the line (in 2D), plane (in 3D), or hyperplane (in higher dimensions) where the model switches from predicting one class to another.\n",
        "\n",
        "**For a single neuron:**\n",
        "$$z = w_1 x_1 + w_2 x_2 + b = 0$$\n",
        "\n",
        "This equation defines a **straight line**. Points on one side get z > 0 (predict class 1), points on the other side get z < 0 (predict class 0).\n",
        "\n",
        "**Why is this a line?** Rearranging:\n",
        "$$x_2 = -\\frac{w_1}{w_2} x_1 - \\frac{b}{w_2}$$\n",
        "\n",
        "This is the equation of a line with slope $-\\frac{w_1}{w_2}$ and intercept $-\\frac{b}{w_2}$.\n",
        "\n",
        "**The Problem:** No matter what values we choose for $w_1$, $w_2$, and $b$, we can only draw ONE straight line!\n",
        "\n",
        "Let's visualize the problem:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# THE XOR PROBLEM: Visualizing Why Single Neurons Fail\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"THE XOR PROBLEM: A Single Neuron's Nightmare\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# XOR data\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([0, 1, 1, 0])\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: The XOR problem\n",
        "ax = axes[0]\n",
        "colors = ['red' if y == 0 else 'blue' for y in y_xor]\n",
        "ax.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=300, edgecolor='black', linewidth=2)\n",
        "for i, (x, y, label) in enumerate(zip(X_xor[:, 0], X_xor[:, 1], y_xor)):\n",
        "    ax.annotate(f'({x},{y})â†’{label}', (x, y), xytext=(10, 10), \n",
        "                textcoords='offset points', fontsize=10)\n",
        "ax.set_xlim(-0.5, 1.5)\n",
        "ax.set_ylim(-0.5, 1.5)\n",
        "ax.set_xlabel('Input A', fontsize=12)\n",
        "ax.set_ylabel('Input B', fontsize=12)\n",
        "ax.set_title('XOR Data Points\\n(Red=0, Blue=1)', fontsize=14, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Can you draw ONE line to separate them?\n",
        "ax = axes[1]\n",
        "ax.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=300, edgecolor='black', linewidth=2)\n",
        "\n",
        "# Try some lines\n",
        "x_line = np.linspace(-0.5, 1.5, 100)\n",
        "ax.plot(x_line, x_line, 'g--', linewidth=2, label='Diagonal?')\n",
        "ax.plot(x_line, 0.5 * np.ones_like(x_line), 'm--', linewidth=2, label='Horizontal?')\n",
        "ax.plot(0.5 * np.ones_like(x_line), x_line, 'c--', linewidth=2, label='Vertical?')\n",
        "\n",
        "ax.set_xlim(-0.5, 1.5)\n",
        "ax.set_ylim(-0.5, 1.5)\n",
        "ax.set_xlabel('Input A', fontsize=12)\n",
        "ax.set_ylabel('Input B', fontsize=12)\n",
        "ax.set_title('Try to Draw ONE Line\\nto Separate Red from Blue', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='upper right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: The solution requires TWO lines (or a curve)\n",
        "ax = axes[2]\n",
        "ax.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=300, edgecolor='black', linewidth=2)\n",
        "\n",
        "# Two lines that together solve XOR\n",
        "ax.plot(x_line, x_line - 0.3, 'g-', linewidth=2, label='Line 1')\n",
        "ax.plot(x_line, x_line + 0.3, 'g-', linewidth=2, label='Line 2')\n",
        "ax.fill_between(x_line, x_line - 0.3, x_line + 0.3, alpha=0.2, color='blue', label='Blue region')\n",
        "\n",
        "ax.set_xlim(-0.5, 1.5)\n",
        "ax.set_ylim(-0.5, 1.5)\n",
        "ax.set_xlabel('Input A', fontsize=12)\n",
        "ax.set_ylabel('Input B', fontsize=12)\n",
        "ax.set_title('Solution: TWO Lines\\n(Requires Hidden Layer!)', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='upper right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\"\"\n",
        "KEY INSIGHT: The XOR Problem\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "The red points (0) are at corners (0,0) and (1,1).\n",
        "The blue points (1) are at corners (0,1) and (1,0).\n",
        "\n",
        "NO SINGLE STRAIGHT LINE can separate red from blue!\n",
        "\n",
        "This is called being \"not linearly separable.\"\n",
        "\n",
        "Why it matters:\n",
        "â€¢ A single neuron can only create ONE linear boundary\n",
        "â€¢ XOR requires a more complex, non-linear boundary\n",
        "â€¢ This was proven impossible for Perceptrons in 1969 (Minsky & Papert)\n",
        "â€¢ The solution: ADD MORE NEURONS â†’ Hidden Layers!\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What IS Linear Separability?\n",
        "\n",
        "**Linear Separability** is a property of a dataset where the classes can be separated by a straight line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions).\n",
        "\n",
        "| Problem Type | Linearly Separable? | Single Neuron Can Solve? |\n",
        "|--------------|---------------------|--------------------------|\n",
        "| AND gate | Yes | âœ“ |\n",
        "| OR gate | Yes | âœ“ |\n",
        "| XOR gate | No | âœ— |\n",
        "| Vertical vs Horizontal lines (clean) | Yes | âœ“ |\n",
        "| Noisy/partial V/H lines | Harder | Struggles! |\n",
        "| Complex overlapping patterns | No | âœ— |\n",
        "\n",
        "### Why Does Linear Separability Matter?\n",
        "\n",
        "This is the **fundamental limit** of single-layer neural networks:\n",
        "\n",
        "| Model | Decision Boundary | What It Can Learn |\n",
        "|-------|-------------------|-------------------|\n",
        "| Single neuron | One line/plane | Only linearly separable patterns |\n",
        "| MLP (hidden layer) | Multiple lines â†’ curves | Non-linear patterns |\n",
        "| Deep MLP | Very complex shapes | Almost anything! |\n",
        "\n",
        "**Mathematically:** A single neuron computes $\\sigma(w \\cdot x + b)$. The activation function $\\sigma$ is monotonic (always increasing or flat), so it can only split the input space with ONE hyperplane. That's the fundamental constraint.\n",
        "\n",
        "### The Historical \"AI Winter\"\n",
        "\n",
        "In 1969, Marvin Minsky and Seymour Papert published a book called \"Perceptrons\" proving that single-layer networks couldn't solve XOR or any non-linearly-separable problem.\n",
        "\n",
        "**Why was this so damaging?** They proved it was a MATHEMATICAL impossibility, not just a training difficulty. No amount of training could make a single neuron learn XOR - it literally cannot represent that function.\n",
        "\n",
        "---\n",
        "\n",
        "## 7.1.5 Back to Our Story: When V/H Classification Gets Hard\n",
        "\n",
        "XOR is the famous textbook example, but let's see how the **same limitation** affects our vertical/horizontal line detection problem.\n",
        "\n",
        "### Our Perceptron's Success... and Its Limits\n",
        "\n",
        "In Parts 4-6, our single-neuron Perceptron achieved ~95-100% accuracy on clean V/H lines. But what happens when the problem gets harder?\n",
        "\n",
        "| Challenge | What Changes | Why It's Harder |\n",
        "|-----------|--------------|-----------------|\n",
        "| **Noisy images** | Random pixels added | Pattern obscured |\n",
        "| **Lines in ANY position** | Not just middle | One \"middle detector\" isn't enough |\n",
        "| **Partial/broken lines** | Missing pixels | Incomplete evidence |\n",
        "| **Thin vs thick lines** | Different widths | Multiple patterns to detect |\n",
        "\n",
        "Let's see if our single neuron can handle these challenges:\n",
        "\n",
        "### The Historical \"AI Winter\"\n",
        "\n",
        "This caused the first **\"AI Winter\"** - a period where funding for neural network research dried up because people thought they were fundamentally limited.\n",
        "\n",
        "### The Solution Was Simple: Add More Layers!\n",
        "\n",
        "The fix was known all along but computationally difficult:\n",
        "\n",
        "**Instead of one expert, use a TEAM of experts (neurons) working together!**\n",
        "\n",
        "---\n",
        "\n",
        "## 7.2 The Panel of Experts: Hidden Layers\n",
        "\n",
        "### What IS a Hidden Layer?\n",
        "\n",
        "A **hidden layer** is a layer of neurons that sits **between** the input and output:\n",
        "\n",
        "```\n",
        "INPUT (9 pixels) â†’ [HIDDEN LAYER] â†’ OUTPUT (1 prediction)\n",
        "                   (multiple neurons)\n",
        "```\n",
        "\n",
        "Why \"hidden\"? Because we never directly see their values during normal use - they're internal to the network.\n",
        "\n",
        "### Why Multiple Neurons Help\n",
        "\n",
        "Each neuron in the hidden layer can detect a **different feature**:\n",
        "\n",
        "| Hidden Neuron | What It Might Detect |\n",
        "|---------------|---------------------|\n",
        "| Neuron 1 | \"Is there a vertical pattern on the LEFT?\" |\n",
        "| Neuron 2 | \"Is there a vertical pattern in the MIDDLE?\" |\n",
        "| Neuron 3 | \"Is there a vertical pattern on the RIGHT?\" |\n",
        "| Neuron 4 | \"Is there a horizontal pattern on TOP?\" |\n",
        "\n",
        "The output neuron then **combines** these feature detections to make a final decision.\n",
        "\n",
        "### The Critical Role of Activation Functions\n",
        "\n",
        "**Why do we NEED activation functions between layers?**\n",
        "\n",
        "Without activations, stacking layers does nothing! Here's why:\n",
        "\n",
        "**Without activation:**\n",
        "$$\\text{Output} = W_2 \\cdot (W_1 \\cdot x) = (W_2 \\cdot W_1) \\cdot x = W_{combined} \\cdot x$$\n",
        "\n",
        "The composition of two linear transformations is just... another linear transformation! We could replace the entire network with a single layer.\n",
        "\n",
        "**With activation:**\n",
        "$$\\text{Output} = W_2 \\cdot \\sigma(W_1 \\cdot x)$$\n",
        "\n",
        "The non-linear $\\sigma$ \"breaks\" the linearity. Now we have:\n",
        "- Layer 1 creates multiple linear boundaries\n",
        "- Activation function \"bends\" these boundaries\n",
        "- Layer 2 combines the bent boundaries\n",
        "\n",
        "**This is how MLPs create curves from straight lines!**\n",
        "\n",
        "### Committee Analogy: The Sub-Committee\n",
        "\n",
        "*\"Before, we had ONE committee member who had to look at everything. Now we have a **sub-committee of specialists**:*\n",
        "\n",
        "- *Specialist 1 checks for patterns in the left region*\n",
        "- *Specialist 2 checks the middle region*  \n",
        "- *Specialist 3 checks the right region*\n",
        "- *The final committee member listens to all specialists and makes the decision*\n",
        "\n",
        "*This division of labor lets us solve more complex problems!\"*\n",
        "\n",
        "### Diversity of Opinion\n",
        "\n",
        "**Key insight from Part 1.7:** If all hidden neurons look for the same thing, they're redundant!\n",
        "\n",
        "We need **diversity** - each hidden neuron should specialize in detecting something different. This happens naturally during training as they adjust to minimize error.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# THE V/H CHALLENGE: When Our Perceptron Struggles\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"BACK TO OUR STORY: Challenging V/H Classification\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Dataset generator from previous parts\n",
        "def generate_line_dataset(n_samples=100, noise_level=0.0, seed=None):\n",
        "    \"\"\"Generate vertical (1) and horizontal (0) line images.\"\"\"\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    \n",
        "    X, y = [], []\n",
        "    for i in range(n_samples):\n",
        "        image = np.zeros((3, 3))\n",
        "        if i < n_samples // 2:  # Vertical\n",
        "            col = np.random.randint(0, 3)  # ANY column, not just middle\n",
        "            image[:, col] = 1\n",
        "            if noise_level > 0:\n",
        "                image = np.clip(image + np.random.randn(3, 3) * noise_level, 0, 1)\n",
        "            X.append(image.flatten())\n",
        "            y.append(1)\n",
        "        else:  # Horizontal\n",
        "            row = np.random.randint(0, 3)\n",
        "            image[row, :] = 1\n",
        "            if noise_level > 0:\n",
        "                image = np.clip(image + np.random.randn(3, 3) * noise_level, 0, 1)\n",
        "            X.append(image.flatten())\n",
        "            y.append(0)\n",
        "    \n",
        "    X, y = np.array(X), np.array(y)\n",
        "    shuffle_idx = np.random.permutation(n_samples)\n",
        "    return X[shuffle_idx], y[shuffle_idx]\n",
        "\n",
        "# Simple Perceptron from Part 5\n",
        "class SimplePerceptron:\n",
        "    def __init__(self, n_inputs):\n",
        "        self.weights = np.random.randn(n_inputs) * 0.1\n",
        "        self.bias = 0.0\n",
        "    \n",
        "    def forward(self, x):\n",
        "        z = np.dot(self.weights, x.flatten()) + self.bias\n",
        "        return sigmoid(z)\n",
        "    \n",
        "    def predict(self, x):\n",
        "        return 1 if self.forward(x) >= 0.5 else 0\n",
        "    \n",
        "    def train(self, X, y, lr=0.5, epochs=100):\n",
        "        for _ in range(epochs):\n",
        "            for xi, yi in zip(X, y):\n",
        "                pred = self.forward(xi)\n",
        "                error = pred - yi\n",
        "                self.weights -= lr * error * xi.flatten()\n",
        "                self.bias -= lr * error\n",
        "        return self\n",
        "\n",
        "# Test on different difficulty levels\n",
        "print(\"\\nTesting Single Neuron on Increasingly Difficult V/H Problems:\\n\")\n",
        "\n",
        "difficulties = [\n",
        "    (\"Clean (0% noise)\", 0.0),\n",
        "    (\"Light noise (10%)\", 0.1),\n",
        "    (\"Medium noise (20%)\", 0.2),\n",
        "    (\"Heavy noise (30%)\", 0.3)\n",
        "]\n",
        "\n",
        "results = []\n",
        "for name, noise in difficulties:\n",
        "    np.random.seed(42)\n",
        "    X_train, y_train = generate_line_dataset(100, noise_level=noise, seed=42)\n",
        "    X_test, y_test = generate_line_dataset(50, noise_level=noise, seed=999)\n",
        "    \n",
        "    perceptron = SimplePerceptron(9)\n",
        "    perceptron.train(X_train, y_train, epochs=100)\n",
        "    \n",
        "    correct = sum(1 for x, y in zip(X_test, y_test) if perceptron.predict(x) == y)\n",
        "    accuracy = correct / len(y_test) * 100\n",
        "    results.append((name, accuracy))\n",
        "    print(f\"  {name:25s} â†’ Accuracy: {accuracy:5.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"KEY OBSERVATION:\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "As noise increases, our single neuron struggles more!\n",
        "\n",
        "Why? The single neuron learned ONE pattern (e.g., \"middle column = vertical\").\n",
        "But noisy images have:\n",
        "  â€¢ Extra bright pixels confusing the detector\n",
        "  â€¢ Lines in different positions the single \"template\" doesn't match\n",
        "  â€¢ Partial patterns that need multiple feature detectors\n",
        "\n",
        "Just like XOR, complex V/H patterns need MULTIPLE SPECIALISTS!\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZING THE CHALLENGE: Clean vs Noisy V/H Images\n",
        "# =============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "\n",
        "# Generate examples at different noise levels\n",
        "np.random.seed(123)\n",
        "\n",
        "# Top row: Vertical lines with increasing noise\n",
        "noises = [0.0, 0.1, 0.2, 0.3, 0.4]\n",
        "for i, noise in enumerate(noises):\n",
        "    ax = axes[0, i]\n",
        "    image = np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]], dtype=float)\n",
        "    if noise > 0:\n",
        "        image = np.clip(image + np.random.randn(3, 3) * noise, 0, 1)\n",
        "    ax.imshow(image, cmap='Blues', vmin=0, vmax=1)\n",
        "    ax.set_title(f'{int(noise*100)}% Noise', fontsize=11)\n",
        "    ax.axis('off')\n",
        "    if i == 0:\n",
        "        ax.set_ylabel('VERTICAL', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Bottom row: Horizontal lines with increasing noise  \n",
        "for i, noise in enumerate(noises):\n",
        "    ax = axes[1, i]\n",
        "    image = np.array([[0, 0, 0], [1, 1, 1], [0, 0, 0]], dtype=float)\n",
        "    if noise > 0:\n",
        "        image = np.clip(image + np.random.randn(3, 3) * noise, 0, 1)\n",
        "    ax.imshow(image, cmap='Blues', vmin=0, vmax=1)\n",
        "    ax.axis('off')\n",
        "    if i == 0:\n",
        "        ax.set_ylabel('HORIZONTAL', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.suptitle('The Challenge: As Noise Increases, Patterns Become Harder to Detect', \n",
        "             fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\"\"\n",
        "With heavy noise, even WE have trouble seeing the pattern!\n",
        "\n",
        "A single neuron that learned \"middle column bright = vertical\" will struggle\n",
        "when noise makes OTHER pixels bright too.\n",
        "\n",
        "SOLUTION: Multiple specialists, each detecting different aspects of the pattern.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZING THE MLP ARCHITECTURE\n",
        "# =============================================================================\n",
        "\n",
        "def draw_neural_network(ax, layer_sizes, layer_names=None):\n",
        "    \"\"\"Draw a neural network architecture diagram.\"\"\"\n",
        "    n_layers = len(layer_sizes)\n",
        "    max_neurons = max(layer_sizes)\n",
        "    \n",
        "    # Spacing\n",
        "    layer_spacing = 1.5\n",
        "    neuron_spacing = 0.8\n",
        "    \n",
        "    positions = []\n",
        "    \n",
        "    for layer_idx, n_neurons in enumerate(layer_sizes):\n",
        "        layer_positions = []\n",
        "        x = layer_idx * layer_spacing\n",
        "        \n",
        "        # Center the neurons vertically\n",
        "        start_y = (max_neurons - n_neurons) * neuron_spacing / 2\n",
        "        \n",
        "        for neuron_idx in range(n_neurons):\n",
        "            y = start_y + neuron_idx * neuron_spacing\n",
        "            layer_positions.append((x, y))\n",
        "            \n",
        "            # Draw neuron\n",
        "            color = '#3498db' if layer_idx == 0 else '#e74c3c' if layer_idx == n_layers - 1 else '#27ae60'\n",
        "            circle = plt.Circle((x, y), 0.15, color=color, ec='black', linewidth=2, zorder=3)\n",
        "            ax.add_patch(circle)\n",
        "        \n",
        "        positions.append(layer_positions)\n",
        "    \n",
        "    # Draw connections\n",
        "    for layer_idx in range(n_layers - 1):\n",
        "        for start_pos in positions[layer_idx]:\n",
        "            for end_pos in positions[layer_idx + 1]:\n",
        "                ax.plot([start_pos[0], end_pos[0]], [start_pos[1], end_pos[1]], \n",
        "                       'gray', alpha=0.3, linewidth=0.5, zorder=1)\n",
        "    \n",
        "    # Add layer labels\n",
        "    if layer_names:\n",
        "        for layer_idx, name in enumerate(layer_names):\n",
        "            x = layer_idx * layer_spacing\n",
        "            ax.text(x, -1, name, ha='center', fontsize=10, fontweight='bold')\n",
        "    \n",
        "    ax.set_xlim(-0.5, (n_layers - 1) * layer_spacing + 0.5)\n",
        "    ax.set_ylim(-1.5, max_neurons * neuron_spacing)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.axis('off')\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "# Plot 1: Perceptron (Part 4-6)\n",
        "ax = axes[0]\n",
        "draw_neural_network(ax, [9, 1], ['Input\\n(9 pixels)', 'Output\\n(1 neuron)'])\n",
        "ax.set_title('PERCEPTRON (Parts 4-6)\\nSingle Layer', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Plot 2: Simple MLP\n",
        "ax = axes[1]\n",
        "draw_neural_network(ax, [9, 4, 1], ['Input\\n(9 pixels)', 'Hidden\\n(4 neurons)', 'Output\\n(1 neuron)'])\n",
        "ax.set_title('MLP: One Hidden Layer\\nThe \"Panel of Experts\"', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Plot 3: Deeper MLP\n",
        "ax = axes[2]\n",
        "draw_neural_network(ax, [9, 6, 4, 1], ['Input\\n(9)', 'Hidden 1\\n(6)', 'Hidden 2\\n(4)', 'Output\\n(1)'])\n",
        "ax.set_title('DEEP MLP: Two Hidden Layers\\n\"Hierarchy of Specialists\"', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Evolution of Neural Network Architectures', fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\"\"\n",
        "ARCHITECTURE COMPARISON:\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "PERCEPTRON (Parts 4-6):\n",
        "  â€¢ Input â†’ Output directly\n",
        "  â€¢ Can only learn linear boundaries\n",
        "  â€¢ Limited to simple problems\n",
        "\n",
        "MLP WITH ONE HIDDEN LAYER:\n",
        "  â€¢ Input â†’ Hidden â†’ Output\n",
        "  â€¢ Each hidden neuron detects different features\n",
        "  â€¢ Can learn non-linear boundaries (like XOR!)\n",
        "\n",
        "DEEP MLP (Multiple Hidden Layers):\n",
        "  â€¢ Input â†’ Hidden 1 â†’ Hidden 2 â†’ ... â†’ Output\n",
        "  â€¢ Each layer builds on the previous layer's features\n",
        "  â€¢ Can learn very complex patterns\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "Color Legend: ðŸ”µ Input | ðŸŸ¢ Hidden | ðŸ”´ Output\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7.3 The Multi-Layer Perceptron (MLP): Architecture and Math\n",
        "\n",
        "Now let's understand the mathematics behind multi-layer networks.\n",
        "\n",
        "### What IS an MLP?\n",
        "\n",
        "A **Multi-Layer Perceptron (MLP)** is a neural network with:\n",
        "- One input layer\n",
        "- One or more hidden layers\n",
        "- One output layer\n",
        "\n",
        "Each layer is **fully connected** to the next (every neuron connects to every neuron in the next layer).\n",
        "\n",
        "### The Math: Forward Propagation\n",
        "\n",
        "For an MLP with one hidden layer, the computation flows in two stages:\n",
        "\n",
        "**Stage 1: Input â†’ Hidden**\n",
        "$$\\mathbf{h} = \\sigma(\\mathbf{W}_1 \\cdot \\mathbf{x} + \\mathbf{b}_1)$$\n",
        "\n",
        "**Stage 2: Hidden â†’ Output**\n",
        "$$\\hat{y} = \\sigma(\\mathbf{W}_2 \\cdot \\mathbf{h} + \\mathbf{b}_2)$$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{x}$ = input vector (our 9 pixels)\n",
        "- $\\mathbf{W}_1$ = weights from input to hidden layer (matrix!)\n",
        "- $\\mathbf{b}_1$ = biases for hidden neurons\n",
        "- $\\mathbf{h}$ = hidden layer activations\n",
        "- $\\mathbf{W}_2$ = weights from hidden to output\n",
        "- $\\mathbf{b}_2$ = bias for output neuron\n",
        "- $\\sigma$ = activation function (sigmoid, ReLU, etc.)\n",
        "- $\\hat{y}$ = final prediction\n",
        "\n",
        "### Breaking It Down Step by Step\n",
        "\n",
        "Let's trace through with concrete dimensions:\n",
        "\n",
        "| Component | Shape | Example |\n",
        "|-----------|-------|---------|\n",
        "| Input $\\mathbf{x}$ | (9,) | 9 pixels |\n",
        "| Weights $\\mathbf{W}_1$ | (4, 9) | 4 hidden neurons, each with 9 weights |\n",
        "| Biases $\\mathbf{b}_1$ | (4,) | 4 biases, one per hidden neuron |\n",
        "| Hidden $\\mathbf{h}$ | (4,) | 4 hidden activations |\n",
        "| Weights $\\mathbf{W}_2$ | (1, 4) | 1 output neuron, 4 weights (from hidden) |\n",
        "| Bias $\\mathbf{b}_2$ | (1,) | 1 bias for output |\n",
        "| Output $\\hat{y}$ | (1,) | Final prediction |\n",
        "\n",
        "### Why These Specific Shapes?\n",
        "\n",
        "**Matrix multiplication rule:** $(m \\times n) \\cdot (n \\times 1) = (m \\times 1)$\n",
        "\n",
        "The shapes MUST align:\n",
        "- $W_1$ is $(4, 9)$ because we have 4 hidden neurons, each looking at 9 inputs\n",
        "- $W_1 \\cdot x$ gives us $(4, 9) \\cdot (9, 1) = (4, 1)$ - one value per hidden neuron âœ“\n",
        "- $W_2$ is $(1, 4)$ because we have 1 output looking at 4 hidden neurons\n",
        "- $W_2 \\cdot h$ gives us $(1, 4) \\cdot (4, 1) = (1, 1)$ - our single output âœ“\n",
        "\n",
        "**The key insight:** Each row of $W_1$ represents ONE hidden neuron's \"view\" of the input. Each column of $W_2$ represents how much the output trusts each hidden neuron.\n",
        "\n",
        "### Why This Works for XOR\n",
        "\n",
        "Each hidden neuron can learn ONE linear boundary. With multiple hidden neurons, we can combine their boundaries to create complex, non-linear decision regions!\n",
        "\n",
        "**Concrete XOR example with 2 hidden neurons:**\n",
        "- Hidden neuron 1 might learn: \"A OR B\" (draw diagonal from bottom-left)\n",
        "- Hidden neuron 2 might learn: \"A AND B\" (draw diagonal from top-right)\n",
        "- Output combines them: \"(A OR B) AND NOT (A AND B)\" = XOR!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BUILDING THE MLP: Step by Step Implementation\n",
        "# =============================================================================\n",
        "\n",
        "class MLP:\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron with one hidden layer.\n",
        "    \n",
        "    Architecture: Input â†’ Hidden (with activation) â†’ Output (with sigmoid)\n",
        "    \n",
        "    This is the \"Full Committee\" - multiple experts working together!\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_inputs, n_hidden, n_outputs=1):\n",
        "        \"\"\"\n",
        "        Initialize the MLP with random weights.\n",
        "        \n",
        "        Parameters:\n",
        "            n_inputs: Number of input features (e.g., 9 for 3x3 image)\n",
        "            n_hidden: Number of neurons in hidden layer (the \"specialists\")\n",
        "            n_outputs: Number of output neurons (1 for binary classification)\n",
        "        \"\"\"\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_outputs = n_outputs\n",
        "        \n",
        "        # Initialize weights with small random values (Xavier initialization)\n",
        "        # W1: weights from input to hidden (shape: n_hidden x n_inputs)\n",
        "        self.W1 = np.random.randn(n_hidden, n_inputs) * np.sqrt(2.0 / n_inputs)\n",
        "        self.b1 = np.zeros(n_hidden)\n",
        "        \n",
        "        # W2: weights from hidden to output (shape: n_outputs x n_hidden)\n",
        "        self.W2 = np.random.randn(n_outputs, n_hidden) * np.sqrt(2.0 / n_hidden)\n",
        "        self.b2 = np.zeros(n_outputs)\n",
        "        \n",
        "        # For storing values during forward pass (needed for backprop)\n",
        "        self.z1 = None  # Pre-activation of hidden layer\n",
        "        self.h = None   # Hidden layer activations\n",
        "        self.z2 = None  # Pre-activation of output\n",
        "        self.output = None\n",
        "        \n",
        "        # Training history\n",
        "        self.loss_history = []\n",
        "        self.accuracy_history = []\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward propagation: Input â†’ Hidden â†’ Output\n",
        "        \n",
        "        This is like the \"Committee Meeting\" where:\n",
        "        1. Each specialist (hidden neuron) examines the evidence\n",
        "        2. The final decision maker combines their opinions\n",
        "        \"\"\"\n",
        "        x = np.array(x).flatten()\n",
        "        \n",
        "        # Stage 1: Input â†’ Hidden\n",
        "        # Each hidden neuron computes its weighted sum and activates\n",
        "        self.z1 = np.dot(self.W1, x) + self.b1  # (n_hidden,)\n",
        "        self.h = sigmoid(self.z1)               # (n_hidden,)\n",
        "        \n",
        "        # Stage 2: Hidden â†’ Output\n",
        "        # The output neuron combines hidden activations\n",
        "        self.z2 = np.dot(self.W2, self.h) + self.b2  # (n_outputs,)\n",
        "        self.output = sigmoid(self.z2)               # (n_outputs,)\n",
        "        \n",
        "        return self.output[0] if self.n_outputs == 1 else self.output\n",
        "    \n",
        "    def predict(self, x):\n",
        "        \"\"\"Make a binary prediction (0 or 1).\"\"\"\n",
        "        return 1 if self.forward(x) >= 0.5 else 0\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"MLP CLASS: The Full Committee Implementation\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create an example MLP\n",
        "mlp = MLP(n_inputs=9, n_hidden=4, n_outputs=1)\n",
        "\n",
        "print(f\"\"\"\n",
        "MLP Architecture Created:\n",
        "  â€¢ Input layer: {mlp.n_inputs} neurons (our 9 pixels)\n",
        "  â€¢ Hidden layer: {mlp.n_hidden} neurons (the specialists)\n",
        "  â€¢ Output layer: {mlp.n_outputs} neuron (final decision)\n",
        "\n",
        "Weight Shapes:\n",
        "  â€¢ W1 (inputâ†’hidden): {mlp.W1.shape} = {mlp.n_hidden} hidden neurons Ã— {mlp.n_inputs} inputs\n",
        "  â€¢ b1 (hidden biases): {mlp.b1.shape} = {mlp.n_hidden} biases\n",
        "  â€¢ W2 (hiddenâ†’output): {mlp.W2.shape} = {mlp.n_outputs} output Ã— {mlp.n_hidden} hidden\n",
        "  â€¢ b2 (output bias): {mlp.b2.shape} = {mlp.n_outputs} bias\n",
        "\n",
        "Total Parameters: {mlp.W1.size + mlp.b1.size + mlp.W2.size + mlp.b2.size}\n",
        "  (Compare to Perceptron: {9 + 1} parameters)\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Xavier Initialization\n",
        "\n",
        "In the code above, we used:\n",
        "```python\n",
        "self.W1 = np.random.randn(n_hidden, n_inputs) * np.sqrt(2.0 / n_inputs)\n",
        "```\n",
        "\n",
        "**What IS Xavier initialization and WHY do we need it?**\n",
        "\n",
        "| Initialization | Formula | Problem It Solves |\n",
        "|----------------|---------|-------------------|\n",
        "| **All zeros** | w = 0 | All neurons learn same thing! (symmetry) |\n",
        "| **Large random** | w ~ N(0, 1) | Signals explode or vanish |\n",
        "| **Xavier** | w ~ N(0, âˆš(2/n)) | Keeps signal variance stable |\n",
        "\n",
        "**The Math Behind Xavier:**\n",
        "\n",
        "When we compute $z = w_1 x_1 + w_2 x_2 + ... + w_n x_n$:\n",
        "- Each $w_i x_i$ term has variance â‰ˆ $\\text{Var}(w) \\times \\text{Var}(x)$\n",
        "- With n terms, total variance â‰ˆ $n \\times \\text{Var}(w) \\times \\text{Var}(x)$\n",
        "\n",
        "**The problem:** If $\\text{Var}(w) = 1$, then variance grows by factor of n each layer!\n",
        "- Layer 1: variance Ã— 9\n",
        "- Layer 2: variance Ã— 9 Ã— 4\n",
        "- Values explode exponentially!\n",
        "\n",
        "**The solution:** Set $\\text{Var}(w) = 2/n$ so that output variance â‰ˆ input variance.\n",
        "\n",
        "This keeps signals \"healthy\" as they flow through the network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tracing Through the Forward Pass: What's Actually Happening?\n",
        "\n",
        "Before we run the code, let's understand what the forward pass computes at each step:\n",
        "\n",
        "**Stage 1: Input â†’ Hidden (What each specialist sees)**\n",
        "\n",
        "For hidden neuron $i$:\n",
        "1. **Weighted sum:** $z_1[i] = W_1[i,0] \\cdot x[0] + W_1[i,1] \\cdot x[1] + ... + W_1[i,8] \\cdot x[8] + b_1[i]$\n",
        "2. **Activation:** $h[i] = \\sigma(z_1[i])$ â†’ transforms to range (0, 1)\n",
        "\n",
        "Each hidden neuron is essentially asking: \"How strongly does this input match MY pattern?\"\n",
        "\n",
        "**Stage 2: Hidden â†’ Output (The final vote)**\n",
        "\n",
        "1. **Combine opinions:** $z_2 = W_2[0] \\cdot h[0] + W_2[1] \\cdot h[1] + ... + W_2[3] \\cdot h[3] + b_2$\n",
        "2. **Final decision:** $\\text{output} = \\sigma(z_2)$ â†’ probability of class 1\n",
        "\n",
        "The output neuron asks: \"Given what all specialists reported, what's my final decision?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FORWARD PASS: Step-by-Step Demonstration\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FORWARD PASS: Tracing Data Through the Network\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create test input (vertical line from Part 1)\n",
        "vertical_line = np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]])\n",
        "x = vertical_line.flatten()\n",
        "\n",
        "print(f\"\\nInput (vertical line as 9 pixels):\")\n",
        "print(f\"  x = {x}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"STAGE 1: Input â†’ Hidden Layer\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Stage 1: Compute hidden layer\n",
        "z1 = np.dot(mlp.W1, x) + mlp.b1\n",
        "h = sigmoid(z1)\n",
        "\n",
        "print(f\"\"\"\n",
        "Step 1a: Compute weighted sums for each hidden neuron\n",
        "  z1 = W1 Â· x + b1\n",
        "  \n",
        "  For each hidden neuron i:\n",
        "    z1[i] = Î£(W1[i,j] Ã— x[j]) + b1[i]\n",
        "    \n",
        "  z1 = {z1}\n",
        "\n",
        "Step 1b: Apply activation function\n",
        "  h = sigmoid(z1)\n",
        "  \n",
        "  For each hidden neuron:\n",
        "    h[i] = 1 / (1 + e^(-z1[i]))\n",
        "    \n",
        "  h = {h}\n",
        "  \n",
        "  These are the \"opinions\" from our {mlp.n_hidden} specialists!\n",
        "\"\"\")\n",
        "\n",
        "print(\"-\"*70)\n",
        "print(\"STAGE 2: Hidden Layer â†’ Output\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Stage 2: Compute output\n",
        "z2 = np.dot(mlp.W2, h) + mlp.b2\n",
        "output = sigmoid(z2)\n",
        "\n",
        "print(f\"\"\"\n",
        "Step 2a: Combine hidden activations\n",
        "  z2 = W2 Â· h + b2\n",
        "  \n",
        "  The output neuron combines all specialist opinions:\n",
        "    z2 = Î£(W2[j] Ã— h[j]) + b2\n",
        "    \n",
        "  z2 = {z2}\n",
        "\n",
        "Step 2b: Apply sigmoid for final prediction\n",
        "  output = sigmoid(z2)\n",
        "  \n",
        "  output = {output}\n",
        "  \n",
        "  Final decision: {\"VERTICAL\" if output[0] >= 0.5 else \"HORIZONTAL\"}\n",
        "  (With random weights, this is just a guess!)\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7.4 Backpropagation Through Multiple Layers\n",
        "\n",
        "In Part 5, we learned backpropagation for a single neuron. With multiple layers, we need to **chain** the gradients - passing blame backward through each layer.\n",
        "\n",
        "### The Challenge: Who's Responsible for the Error?\n",
        "\n",
        "When the network makes a mistake, we need to figure out:\n",
        "1. How much should we adjust the **output weights** (W2)?\n",
        "2. How much should we adjust the **hidden weights** (W1)?\n",
        "\n",
        "The difficulty: W1 doesn't directly produce the output! It influences the hidden layer, which THEN influences the output. This is like asking: \"If a manager's employee made a mistake, how much is the manager responsible?\"\n",
        "\n",
        "### The Chain Rule: Passing Blame Backward\n",
        "\n",
        "The key mathematical tool is the **chain rule** from calculus:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial h} \\cdot \\frac{\\partial h}{\\partial W_1}$$\n",
        "\n",
        "**What IS the Chain Rule?**\n",
        "\n",
        "The chain rule says: if A affects B, and B affects C, then A's effect on C is:\n",
        "\n",
        "$$\\frac{dC}{dA} = \\frac{dC}{dB} \\times \\frac{dB}{dA}$$\n",
        "\n",
        "**Intuitive example:** If increasing temperature by 1Â°C increases pressure by 2 units, and increasing pressure by 1 unit increases volume by 3 units, then increasing temperature by 1Â°C increases volume by 2 Ã— 3 = 6 units.\n",
        "\n",
        "Think of it as a blame chain:\n",
        "- **Loss** depends on **output prediction** (how wrong is the answer?)\n",
        "- **Output prediction** depends on **hidden activations** (what did specialists say?)\n",
        "- **Hidden activations** depend on **hidden weights** (what were specialists looking for?)\n",
        "\n",
        "### Committee Analogy: Tracing Blame\n",
        "\n",
        "*\"When the committee makes a wrong decision:*\n",
        "\n",
        "1. *First, we see how wrong the final decision was (output error)*\n",
        "2. *Then we ask: 'Which specialists contributed to this error?' (hidden layer blame)*\n",
        "3. *Finally: 'What evidence did each specialist focus on that led them astray?' (input weights)*\n",
        "\n",
        "*The blame flows BACKWARD through the committee hierarchy.\"*\n",
        "\n",
        "### The Backpropagation Steps\n",
        "\n",
        "**Step 1: Output Error**\n",
        "$$\\delta_2 = \\hat{y} - y$$\n",
        "\n",
        "**Step 2: Hidden Layer Error (via chain rule)**\n",
        "$$\\delta_1 = (W_2^T \\cdot \\delta_2) \\odot \\sigma'(z_1)$$\n",
        "\n",
        "Where $\\odot$ is element-wise multiplication and $\\sigma'$ is the derivative of sigmoid.\n",
        "\n",
        "**Step 3: Update Weights**\n",
        "$$W_2 = W_2 - \\alpha \\cdot \\delta_2 \\cdot h^T$$\n",
        "$$W_1 = W_1 - \\alpha \\cdot \\delta_1 \\cdot x^T$$\n",
        "\n",
        "### Why We Store Values During Forward Pass\n",
        "\n",
        "Notice that backpropagation needs values computed during forward pass:\n",
        "- $h$ (hidden activations) - needed to update W2\n",
        "- $z_1$ (pre-activation) - needed for sigmoid derivative\n",
        "- $x$ (input) - needed to update W1\n",
        "\n",
        "**This is why neural networks use memory!** We can't compute gradients without remembering what happened during the forward pass. This creates a fundamental trade-off:\n",
        "\n",
        "| Memory Usage | Gradient Computation |\n",
        "|--------------|---------------------|\n",
        "| Store all intermediate values | Exact gradients (standard backprop) |\n",
        "| Store some values | Approximate gradients (gradient checkpointing) |\n",
        "\n",
        "For deep networks with billions of parameters, memory management becomes critical!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPLETE MLP WITH TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "class TrainableMLP:\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron with training capability.\n",
        "    \n",
        "    This is the complete \"Full Committee\" that can learn!\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_inputs, n_hidden, n_outputs=1):\n",
        "        \"\"\"Initialize the MLP with Xavier initialization.\"\"\"\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_outputs = n_outputs\n",
        "        \n",
        "        # Xavier initialization for better training\n",
        "        self.W1 = np.random.randn(n_hidden, n_inputs) * np.sqrt(2.0 / n_inputs)\n",
        "        self.b1 = np.zeros(n_hidden)\n",
        "        self.W2 = np.random.randn(n_outputs, n_hidden) * np.sqrt(2.0 / n_hidden)\n",
        "        self.b2 = np.zeros(n_outputs)\n",
        "        \n",
        "        # Cache for forward pass values\n",
        "        self.x = None\n",
        "        self.z1 = None\n",
        "        self.h = None\n",
        "        self.z2 = None\n",
        "        self.output = None\n",
        "        \n",
        "        # Training history\n",
        "        self.loss_history = []\n",
        "        self.accuracy_history = []\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward propagation.\"\"\"\n",
        "        self.x = np.array(x).flatten()\n",
        "        \n",
        "        # Hidden layer\n",
        "        self.z1 = np.dot(self.W1, self.x) + self.b1\n",
        "        self.h = sigmoid(self.z1)\n",
        "        \n",
        "        # Output layer\n",
        "        self.z2 = np.dot(self.W2, self.h) + self.b2\n",
        "        self.output = sigmoid(self.z2)\n",
        "        \n",
        "        return self.output[0] if self.n_outputs == 1 else self.output\n",
        "    \n",
        "    def predict(self, x):\n",
        "        \"\"\"Binary prediction.\"\"\"\n",
        "        return 1 if self.forward(x) >= 0.5 else 0\n",
        "    \n",
        "    def backward(self, y_true, learning_rate):\n",
        "        \"\"\"\n",
        "        Backpropagation: compute gradients and update weights.\n",
        "        \n",
        "        This is where the \"blame assignment\" happens!\n",
        "        \"\"\"\n",
        "        # Output layer error\n",
        "        delta2 = self.output - y_true  # Shape: (1,) or (n_outputs,)\n",
        "        \n",
        "        # Hidden layer error (chain rule!)\n",
        "        # delta1 = (W2.T @ delta2) * sigmoid_derivative(z1)\n",
        "        delta1 = np.dot(self.W2.T, delta2) * sigmoid_derivative(self.z1)\n",
        "        \n",
        "        # Update output weights (W2, b2)\n",
        "        # dW2 = delta2 @ h.T (outer product)\n",
        "        dW2 = np.outer(delta2, self.h)\n",
        "        db2 = delta2\n",
        "        \n",
        "        # Update hidden weights (W1, b1)\n",
        "        # dW1 = delta1 @ x.T (outer product)\n",
        "        dW1 = np.outer(delta1, self.x)\n",
        "        db1 = delta1\n",
        "        \n",
        "        # Apply updates\n",
        "        self.W2 -= learning_rate * dW2\n",
        "        self.b2 -= learning_rate * db2\n",
        "        self.W1 -= learning_rate * dW1\n",
        "        self.b1 -= learning_rate * db1\n",
        "    \n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        \"\"\"Binary cross-entropy loss.\"\"\"\n",
        "        epsilon = 1e-15\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "        return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    \n",
        "    def train(self, X, y, learning_rate=0.5, epochs=100, verbose=True):\n",
        "        \"\"\"\n",
        "        Train the MLP on data.\n",
        "        \n",
        "        Parameters:\n",
        "            X: Training inputs (n_samples, n_features)\n",
        "            y: Training labels (n_samples,)\n",
        "            learning_rate: Step size for gradient descent\n",
        "            epochs: Number of passes through the dataset\n",
        "            verbose: Whether to print progress\n",
        "        \"\"\"\n",
        "        self.loss_history = []\n",
        "        self.accuracy_history = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            correct = 0\n",
        "            \n",
        "            for i in range(len(X)):\n",
        "                # Forward pass\n",
        "                y_pred = self.forward(X[i])\n",
        "                \n",
        "                # Compute loss\n",
        "                loss = self.compute_loss(y[i], y_pred)\n",
        "                total_loss += loss\n",
        "                \n",
        "                # Check accuracy\n",
        "                if (y_pred >= 0.5 and y[i] == 1) or (y_pred < 0.5 and y[i] == 0):\n",
        "                    correct += 1\n",
        "                \n",
        "                # Backward pass (this is where learning happens!)\n",
        "                self.backward(np.array([y[i]]), learning_rate)\n",
        "            \n",
        "            # Record history\n",
        "            avg_loss = total_loss / len(X)\n",
        "            accuracy = correct / len(X)\n",
        "            self.loss_history.append(avg_loss)\n",
        "            self.accuracy_history.append(accuracy)\n",
        "            \n",
        "            if verbose and (epoch + 1) % 20 == 0:\n",
        "                print(f\"  Epoch {epoch+1:3d}/{epochs}: Loss = {avg_loss:.4f}, Accuracy = {accuracy*100:.1f}%\")\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\nTraining complete! Final accuracy: {self.accuracy_history[-1]*100:.1f}%\")\n",
        "        \n",
        "        return self.loss_history\n",
        "\n",
        "print(\"TrainableMLP class defined!\")\n",
        "print(\"This MLP can learn through backpropagation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Backward Method\n",
        "\n",
        "Let's trace through what `backward` actually computes:\n",
        "\n",
        "**Output error:** `delta2 = self.output - y_true`\n",
        "- If predicted 0.8 but true is 0, error = +0.8 (need to decrease)\n",
        "- This comes from derivative of BCE loss with sigmoid\n",
        "\n",
        "**Hidden error:** `delta1 = np.dot(self.W2.T, delta2) * sigmoid_derivative(self.z1)`\n",
        "- First part: Distribute output error to hidden neurons based on their weights\n",
        "- Second part: Scale by how \"sensitive\" each neuron was\n",
        "\n",
        "**Why the outer product for updates?**\n",
        "\n",
        "`dW2 = np.outer(delta2, self.h)` computes: error Ã— what hidden neurons said\n",
        "\n",
        "Each weight connects ONE hidden neuron to output. If that hidden neuron was highly active AND error was large, that weight contributed a lot â†’ big update.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7.5 The MLP Solves XOR!\n",
        "\n",
        "Now let's prove that our MLP can solve the XOR problem that defeated single neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MLP SOLVES XOR: Proof That Hidden Layers Work!\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"MLP vs XOR: The Hidden Layer Advantage\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# XOR data\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([0, 1, 1, 0])\n",
        "\n",
        "# Create and train MLP with 2 hidden neurons\n",
        "np.random.seed(42)\n",
        "xor_mlp = TrainableMLP(n_inputs=2, n_hidden=4, n_outputs=1)\n",
        "\n",
        "print(\"\\nTraining MLP on XOR problem...\")\n",
        "print(\"(Remember: A single neuron CANNOT solve this!)\\n\")\n",
        "\n",
        "xor_mlp.train(X_xor, y_xor, learning_rate=1.0, epochs=1000, verbose=True)\n",
        "\n",
        "# Test predictions\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"XOR PREDICTIONS:\")\n",
        "print(\"-\"*70)\n",
        "print(\"\\n  Input A | Input B | Expected | Predicted | Correct?\")\n",
        "print(\"  \" + \"-\"*50)\n",
        "\n",
        "all_correct = True\n",
        "for i, (x, y_true) in enumerate(zip(X_xor, y_xor)):\n",
        "    y_pred = xor_mlp.predict(x)\n",
        "    prob = xor_mlp.forward(x)\n",
        "    correct = \"Yes\" if y_pred == y_true else \"No\"\n",
        "    if y_pred != y_true:\n",
        "        all_correct = False\n",
        "    print(f\"    {x[0]}     |    {x[1]}    |    {y_true}     |     {y_pred}     |   {correct}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if all_correct:\n",
        "    print(\"SUCCESS! The MLP solved XOR!\")\n",
        "    print(\"Hidden layers enable learning non-linear patterns!\")\n",
        "else:\n",
        "    print(\"Still learning... (try running training again)\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What XOR Taught Us\n",
        "\n",
        "The XOR success proves several important points:\n",
        "\n",
        "| Lesson | Why It Matters |\n",
        "|--------|----------------|\n",
        "| **Hidden layers enable non-linear boundaries** | We can now solve problems impossible for single neurons |\n",
        "| **4 hidden neurons > 2 for XOR** | Sometimes extra capacity helps training |\n",
        "| **Higher learning rate (1.0)** | XOR has sharp boundaries, needs aggressive updates |\n",
        "| **More epochs (1000)** | Non-linear problems can take longer to converge |\n",
        "\n",
        "**The key insight:** Each hidden neuron learned to detect one \"piece\" of the XOR pattern. The output neuron combined these pieces into the full solution.\n",
        "\n",
        "Now let's return to our V/H classification story and see if this same power translates to real image problems!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7.6 Back to Our Through-Line: MLP vs Perceptron on V/H\n",
        "\n",
        "We've proven the MLP can solve XOR. Now let's return to our **continuing V/H story** and see if the MLP can handle the challenging noisy images that stumped our single neuron.\n",
        "\n",
        "### The Comparison We've Been Building To\n",
        "\n",
        "| Model | Clean V/H | Noisy V/H (20%) | Why? |\n",
        "|-------|-----------|-----------------|------|\n",
        "| **Perceptron** | ~95-100% | ~70-80% | One pattern detector isn't enough |\n",
        "| **MLP** | ~95-100% | ? | Multiple specialists should help! |\n",
        "\n",
        "### Why Should MLP Help With Noise?\n",
        "\n",
        "**The Perceptron's problem with noise:**\n",
        "- It learned ONE template (e.g., \"middle column bright = vertical\")\n",
        "- Noise adds random bright pixels everywhere\n",
        "- Random brightness confuses the single template\n",
        "\n",
        "**How MLP specialists help:**\n",
        "\n",
        "| Specialist | What It Might Detect | Why Noise-Robust |\n",
        "|------------|---------------------|------------------|\n",
        "| Hidden 1 | Left column pattern | Noise in right columns doesn't affect it |\n",
        "| Hidden 2 | Middle column pattern | Noise in left columns doesn't affect it |\n",
        "| Hidden 3 | Vertical vs horizontal ratio | Looks at overall shape |\n",
        "| Hidden 4 | Edge patterns | Different view of same data |\n",
        "\n",
        "Even if noise confuses ONE specialist, the others can \"vote\" correctly!\n",
        "\n",
        "This is called **ensemble robustness** - multiple diverse detectors are more reliable than one.\n",
        "\n",
        "Let's find out:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MLP vs PERCEPTRON: The Showdown on Noisy V/H Images\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"THE SHOWDOWN: Perceptron vs MLP on Noisy V/H Images\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Compare Perceptron vs MLP at different noise levels\n",
        "print(\"\\nComparing performance at different noise levels:\\n\")\n",
        "print(\"  Noise Level | Perceptron | MLP (4 hidden) | Winner\")\n",
        "print(\"  \" + \"-\"*55)\n",
        "\n",
        "noise_levels = [0.0, 0.1, 0.2, 0.3]\n",
        "perceptron_scores = []\n",
        "mlp_scores = []\n",
        "\n",
        "for noise in noise_levels:\n",
        "    np.random.seed(42)\n",
        "    X_train, y_train = generate_line_dataset(100, noise_level=noise, seed=42)\n",
        "    X_test, y_test = generate_line_dataset(50, noise_level=noise, seed=999)\n",
        "    \n",
        "    # Train Perceptron\n",
        "    perceptron = SimplePerceptron(9)\n",
        "    perceptron.train(X_train, y_train, epochs=100)\n",
        "    p_correct = sum(1 for x, y in zip(X_test, y_test) if perceptron.predict(x) == y)\n",
        "    p_acc = p_correct / len(y_test) * 100\n",
        "    perceptron_scores.append(p_acc)\n",
        "    \n",
        "    # Train MLP\n",
        "    mlp_model = TrainableMLP(n_inputs=9, n_hidden=4, n_outputs=1)\n",
        "    mlp_model.train(X_train, y_train, learning_rate=0.5, epochs=100, verbose=False)\n",
        "    m_correct = sum(1 for x, y in zip(X_test, y_test) if mlp_model.predict(x) == y)\n",
        "    m_acc = m_correct / len(y_test) * 100\n",
        "    mlp_scores.append(m_acc)\n",
        "    \n",
        "    winner = \"TIE\" if abs(p_acc - m_acc) < 2 else (\"Perceptron\" if p_acc > m_acc else \"MLP âœ“\")\n",
        "    print(f\"    {int(noise*100):3d}%       |   {p_acc:5.1f}%   |    {m_acc:5.1f}%     | {winner}\")\n",
        "\n",
        "# Store the final trained MLP for later visualization\n",
        "np.random.seed(42)\n",
        "X_train, y_train = generate_line_dataset(100, noise_level=0.2, seed=42)\n",
        "X_test, y_test = generate_line_dataset(50, noise_level=0.2, seed=999)\n",
        "vh_mlp = TrainableMLP(n_inputs=9, n_hidden=4, n_outputs=1)\n",
        "vh_mlp.train(X_train, y_train, learning_rate=0.5, epochs=100, verbose=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"KEY RESULT:\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "As noise increases, the MLP maintains higher accuracy!\n",
        "\n",
        "WHY? The MLP has MULTIPLE SPECIALISTS:\n",
        "  â€¢ One hidden neuron might detect \"left column patterns\"\n",
        "  â€¢ Another detects \"middle column patterns\"  \n",
        "  â€¢ Another detects \"right column patterns\"\n",
        "  â€¢ The output combines their votes\n",
        "\n",
        "Even if noise confuses one specialist, others can still contribute!\n",
        "This is the power of the FULL COMMITTEE.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZING THE COMPARISON: Perceptron vs MLP\n",
        "# =============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Accuracy comparison bar chart\n",
        "ax = axes[0]\n",
        "x = np.arange(len(noise_levels))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar(x - width/2, perceptron_scores, width, label='Perceptron (1 neuron)', color='#e74c3c')\n",
        "bars2 = ax.bar(x + width/2, mlp_scores, width, label='MLP (4 hidden neurons)', color='#27ae60')\n",
        "\n",
        "ax.set_xlabel('Noise Level', fontsize=12)\n",
        "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "ax.set_title('The Showdown: Perceptron vs MLP\\non Noisy V/H Images', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([f'{int(n*100)}%' for n in noise_levels])\n",
        "ax.legend()\n",
        "ax.set_ylim(50, 105)\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars1:\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
        "            f'{bar.get_height():.0f}%', ha='center', va='bottom', fontsize=9)\n",
        "for bar in bars2:\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
        "            f'{bar.get_height():.0f}%', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Plot 2: The insight\n",
        "ax = axes[1]\n",
        "ax.axis('off')\n",
        "\n",
        "insight_text = \"\"\"\n",
        "WHY MLP WINS ON NOISY DATA\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "PERCEPTRON (Single Expert):\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  \"I look for ONE pattern:           â”‚\n",
        "â”‚   middle column = vertical\"         â”‚\n",
        "â”‚                                     â”‚\n",
        "â”‚  Problem: Noise activates other     â”‚\n",
        "â”‚  pixels, confusing my ONE detector  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "MLP (Committee of Specialists):\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Specialist 1: \"I check LEFT\"       â”‚\n",
        "â”‚  Specialist 2: \"I check MIDDLE\"     â”‚\n",
        "â”‚  Specialist 3: \"I check RIGHT\"      â”‚\n",
        "â”‚  Specialist 4: \"I check PATTERNS\"   â”‚\n",
        "â”‚                                     â”‚\n",
        "â”‚  Even if noise fools one of us,     â”‚\n",
        "â”‚  the others provide backup!         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "This is REDUNDANCY and SPECIALIZATION working together!\n",
        "\"\"\"\n",
        "\n",
        "ax.text(0.05, 0.5, insight_text, fontsize=10, family='monospace',\n",
        "        verticalalignment='center', transform=ax.transAxes,\n",
        "        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.9))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZING HIDDEN NEURON SPECIALIZATION\n",
        "# =============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
        "\n",
        "# Top row: Hidden neuron weights (what each specialist looks for)\n",
        "for i in range(min(4, vh_mlp.n_hidden)):\n",
        "    ax = axes[0, i] if i < 3 else axes[1, 0]\n",
        "    weights = vh_mlp.W1[i].reshape(3, 3)\n",
        "    im = ax.imshow(weights, cmap='RdBu', vmin=-2, vmax=2)\n",
        "    ax.set_title(f'Hidden Neuron {i+1}\\nWeights', fontsize=11, fontweight='bold')\n",
        "    for r in range(3):\n",
        "        for c in range(3):\n",
        "            color = 'white' if abs(weights[r,c]) > 1 else 'black'\n",
        "            ax.text(c, r, f'{weights[r,c]:.2f}', ha='center', va='center', fontsize=9, color=color)\n",
        "    ax.axis('off')\n",
        "    plt.colorbar(im, ax=ax, fraction=0.046)\n",
        "\n",
        "# Bottom row: Output weights and explanation\n",
        "ax = axes[1, 1]\n",
        "ax.bar(range(vh_mlp.n_hidden), vh_mlp.W2[0], color=['#e74c3c' if w < 0 else '#27ae60' for w in vh_mlp.W2[0]])\n",
        "ax.set_xlabel('Hidden Neuron', fontsize=11)\n",
        "ax.set_ylabel('Output Weight', fontsize=11)\n",
        "ax.set_title('How Output Combines\\nHidden Neurons', fontsize=11, fontweight='bold')\n",
        "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "\n",
        "# Explanation\n",
        "ax = axes[1, 2]\n",
        "ax.axis('off')\n",
        "explanation = \"\"\"\n",
        "WHAT EACH HIDDEN NEURON LEARNED\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "Each hidden neuron became a \"specialist\":\n",
        "\n",
        "â€¢ Some neurons learned to detect\n",
        "  VERTICAL patterns (strong middle column)\n",
        "  \n",
        "â€¢ Some neurons learned to detect  \n",
        "  HORIZONTAL patterns (strong middle row)\n",
        "\n",
        "â€¢ The output neuron COMBINES these\n",
        "  specialist opinions:\n",
        "  - Positive weight = \"trust this specialist\"\n",
        "  - Negative weight = \"opposite of this specialist\"\n",
        "\n",
        "This is DIVERSITY OF OPINION in action!\n",
        "\"\"\"\n",
        "ax.text(0.1, 0.5, explanation, fontsize=10, family='monospace',\n",
        "        verticalalignment='center', transform=ax.transAxes,\n",
        "        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
        "\n",
        "plt.suptitle('The Committee of Specialists: What Each Hidden Neuron Learned', \n",
        "             fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7.7 The Universal Approximation Theorem\n",
        "\n",
        "One of the most powerful results in neural network theory is the **Universal Approximation Theorem**.\n",
        "\n",
        "### What Does It Say?\n",
        "\n",
        "**\"A neural network with a single hidden layer containing enough neurons can approximate ANY continuous function to arbitrary accuracy.\"**\n",
        "\n",
        "In simpler terms: with enough hidden neurons, a neural network can learn to represent virtually any pattern!\n",
        "\n",
        "### What This Means\n",
        "\n",
        "| Statement | Implication |\n",
        "|-----------|-------------|\n",
        "| \"Any continuous function\" | Any smooth input-output relationship |\n",
        "| \"Single hidden layer\" | You only NEED one hidden layer (in theory) |\n",
        "| \"Enough neurons\" | May need many neurons for complex functions |\n",
        "| \"Arbitrary accuracy\" | Can get as close as you want to the true function |\n",
        "\n",
        "### The Catch\n",
        "\n",
        "The theorem tells us networks CAN represent any function, but NOT:\n",
        "- How to FIND the right weights (training is still hard!)\n",
        "- How MANY neurons are needed (could be huge!)\n",
        "- Whether training will converge\n",
        "\n",
        "### Why Add MORE Layers?\n",
        "\n",
        "If one hidden layer is theoretically enough, why do modern networks have many layers?\n",
        "\n",
        "**Deep networks (more layers) are more EFFICIENT:**\n",
        "\n",
        "| Architecture | Parameters Needed | Why? |\n",
        "|--------------|-------------------|------|\n",
        "| Wide (1 layer, many neurons) | Exponential | Each neuron works independently |\n",
        "| Deep (many layers, fewer neurons) | Polynomial | Layers build on each other |\n",
        "\n",
        "### The Compositionality Argument: Why Depth Wins\n",
        "\n",
        "**Key insight:** Complex functions often have *hierarchical structure*.\n",
        "\n",
        "Consider recognizing a face:\n",
        "1. **Layer 1:** Detect edges (simple lines, curves)\n",
        "2. **Layer 2:** Combine edges into parts (eyes, nose, mouth)\n",
        "3. **Layer 3:** Combine parts into faces\n",
        "\n",
        "Each layer REUSES what the previous layer learned!\n",
        "\n",
        "**With a single wide layer:** Each neuron must independently learn to detect \"face\" from raw pixels. No reuse.\n",
        "\n",
        "**With deep layers:** Edge detectors are shared across eye detectors, nose detectors, etc. Massive reuse!\n",
        "\n",
        "**Mathematical example:**\n",
        "- To represent $f(x) = x^{2^n}$ with wide network: need $2^n$ neurons\n",
        "- With deep network: just n layers, each computing $x^2$ of the previous layer\n",
        "\n",
        "### What Does \"Arbitrary Accuracy\" Mean?\n",
        "\n",
        "The theorem says we can get \"arbitrarily close\" to any function. Concretely:\n",
        "\n",
        "$$|f(x) - \\hat{f}(x)| < \\epsilon \\text{ for any } \\epsilon > 0$$\n",
        "\n",
        "Where $f$ is the true function and $\\hat{f}$ is the network's approximation.\n",
        "\n",
        "**Catch:** The number of neurons needed grows as $\\epsilon$ gets smaller. For very precise approximations, you might need astronomically many neurons!\n",
        "\n",
        "### Committee Analogy\n",
        "\n",
        "*\"One giant room of 1000 generalist committee members CAN solve any problem. But a hierarchical organization with specialists (layer 1: evidence gatherers, layer 2: pattern detectors, layer 3: decision makers) can solve it with fewer people and better organization.\"*\n",
        "\n",
        "---\n",
        "\n",
        "## Part 7 Summary: What We've Learned\n",
        "\n",
        "### Key Concepts Mastered\n",
        "\n",
        "| Concept | Definition | Why It Matters |\n",
        "|---------|------------|----------------|\n",
        "| **Linear Separability** | Can separate with one line | Determines what single neurons can learn |\n",
        "| **XOR Problem** | Non-linearly separable | Proves single neurons have limits |\n",
        "| **Hidden Layer** | Neurons between input and output | Enable non-linear boundaries |\n",
        "| **MLP** | Multi-Layer Perceptron | Network with hidden layers |\n",
        "| **Forward Propagation** | Input â†’ Hidden â†’ Output | How predictions are made |\n",
        "| **Backpropagation** | Chain rule through layers | How MLPs learn |\n",
        "| **Universal Approximation** | MLPs can learn anything | Theoretical foundation |\n",
        "\n",
        "### Architecture Comparison\n",
        "\n",
        "| Model | Layers | XOR | Clean V/H | Noisy V/H (20%) | Why? |\n",
        "|-------|--------|-----|-----------|-----------------|------|\n",
        "| **Perceptron** | 1 | âœ— | ~95% | ~70-80% | One detector isn't enough |\n",
        "| **MLP (4 hidden)** | 2 | âœ“ | ~95% | ~85-95% | Multiple specialists! |\n",
        "| **Deep MLP** | 3+ | âœ“ | âœ“ | âœ“ | Even more capacity |\n",
        "\n",
        "### Two Complementary Examples\n",
        "\n",
        "| Example | What We Learned |\n",
        "|---------|-----------------|\n",
        "| **XOR Problem** | Classic proof that single neurons have fundamental limits |\n",
        "| **Noisy V/H Lines** | Practical demonstration using our continuing story |\n",
        "\n",
        "Both examples taught the same lesson: **complex problems need multiple specialists working together**.\n",
        "\n",
        "### Committee Analogy Progress\n",
        "\n",
        "| Part | What Happened |\n",
        "|------|---------------|\n",
        "| Parts 1-3 | Single member learned procedures |\n",
        "| Part 4 | First case - confused |\n",
        "| Part 5 | Learned from feedback |\n",
        "| Part 6 | Performance review |\n",
        "| **Part 7** | **Assembled the full committee with specialists!** |\n",
        "| Part 8 | (Next) The committee faces growing pains |\n",
        "\n",
        "---\n",
        "\n",
        "## Knowledge Check\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How Many Hidden Neurons Do We Need?\n",
        "\n",
        "A natural question: \"Should I use 4 hidden neurons? 10? 100?\"\n",
        "\n",
        "**Understanding Network Capacity:**\n",
        "\n",
        "| Hidden Neurons | Capacity | Risk |\n",
        "|----------------|----------|------|\n",
        "| Too few (1-2) | Can't represent complex patterns | Underfitting |\n",
        "| Just right (4-8 for V/H) | Captures patterns without memorizing | Good generalization |\n",
        "| Too many (50+) | Can memorize training data | Overfitting |\n",
        "\n",
        "**Rules of Thumb:**\n",
        "\n",
        "1. **Start small, increase if needed** - Begin with 2-4 hidden neurons, add more if accuracy plateaus\n",
        "2. **Watch train vs test gap** - If training accuracy >> test accuracy, reduce neurons\n",
        "3. **Problem complexity guides size** - Simple patterns need fewer neurons\n",
        "\n",
        "**For our V/H problem:**\n",
        "- 9 input pixels\n",
        "- 2 classes (binary)\n",
        "- 4 hidden neurons is reasonable: enough for specialization, not so many that overfitting occurs\n",
        "\n",
        "We'll explore overfitting in detail in Part 8!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# KNOWLEDGE CHECK - Part 7\n",
        "# =============================================================================\n",
        "\n",
        "print(\"KNOWLEDGE CHECK - Part 7: Hidden Layers\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "questions = [\n",
        "    {\n",
        "        \"q\": \"1. Why can't a single neuron solve the XOR problem?\",\n",
        "        \"options\": [\n",
        "            \"A) XOR has too many inputs\",\n",
        "            \"B) XOR is not linearly separable - can't draw one line to separate classes\",\n",
        "            \"C) XOR requires too much memory\",\n",
        "            \"D) Single neurons can solve XOR, it just takes longer\"\n",
        "        ],\n",
        "        \"answer\": \"B\",\n",
        "        \"explanation\": \"XOR points cannot be separated by a single straight line. The (0,0) and (1,1) points are class 0, while (0,1) and (1,0) are class 1 - no line can separate them.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"2. Why does MLP outperform Perceptron on noisy V/H images?\",\n",
        "        \"options\": [\n",
        "            \"A) MLP runs faster\",\n",
        "            \"B) MLP has multiple specialists - if noise fools one, others provide backup\",\n",
        "            \"C) MLP uses less memory\",\n",
        "            \"D) Perceptron can't process images\"\n",
        "        ],\n",
        "        \"answer\": \"B\",\n",
        "        \"explanation\": \"MLP has multiple hidden neurons that each detect different features. Even if noise confuses one specialist, the others can still detect patterns and contribute to the correct answer.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"3. What is a 'hidden layer' in a neural network?\",\n",
        "        \"options\": [\n",
        "            \"A) A layer that is invisible to users\",\n",
        "            \"B) A layer of neurons between the input and output layers\",\n",
        "            \"C) A layer that stores hidden data\",\n",
        "            \"D) A layer that only activates sometimes\"\n",
        "        ],\n",
        "        \"answer\": \"B\",\n",
        "        \"explanation\": \"Hidden layers sit between input and output. They're 'hidden' because we don't directly observe their values - they're internal to the network.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"4. What does each hidden neuron typically learn to detect?\",\n",
        "        \"options\": [\n",
        "            \"A) The same pattern as other neurons\",\n",
        "            \"B) Random noise\",\n",
        "            \"C) Different features or patterns (specialization)\",\n",
        "            \"D) Only the output labels\"\n",
        "        ],\n",
        "        \"answer\": \"C\",\n",
        "        \"explanation\": \"Each hidden neuron specializes in detecting different features. This 'diversity of opinion' is what gives MLPs their power to learn complex patterns.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"5. In backpropagation through multiple layers, how does error flow?\",\n",
        "        \"options\": [\n",
        "            \"A) Forward, from input to output\",\n",
        "            \"B) Backward, from output to input via chain rule\",\n",
        "            \"C) Randomly through the network\",\n",
        "            \"D) Only through the hidden layer\"\n",
        "        ],\n",
        "        \"answer\": \"B\",\n",
        "        \"explanation\": \"Backpropagation passes error backward using the chain rule. Output error â†’ hidden layer error â†’ input weight updates.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"6. What does the Universal Approximation Theorem tell us?\",\n",
        "        \"options\": [\n",
        "            \"A) Neural networks always converge\",\n",
        "            \"B) One hidden layer with enough neurons can approximate any function\",\n",
        "            \"C) Deep networks are always better than shallow ones\",\n",
        "            \"D) Training is guaranteed to find optimal weights\"\n",
        "        ],\n",
        "        \"answer\": \"B\",\n",
        "        \"explanation\": \"The theorem says MLPs CAN represent any function, but doesn't guarantee we can find the weights or how many neurons we need.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(f\"\\n{q['q']}\")\n",
        "    for opt in q[\"options\"]:\n",
        "        print(f\"   {opt}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Scroll down for answers...\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ANSWERS\n",
        "print(\"ANSWERS - Part 7 Knowledge Check\")\n",
        "print(\"=\"*60)\n",
        "for i, q in enumerate(questions, 1):\n",
        "    print(f\"\\n{i}. Answer: {q['answer']}\")\n",
        "    print(f\"   {q['explanation']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## What's Next?\n",
        "\n",
        "**Congratulations!** You've completed Part 7!\n",
        "\n",
        "We've assembled the **full committee** - a Multi-Layer Perceptron with hidden layers that can solve problems single neurons cannot. We proved this by solving XOR and saw how hidden neurons specialize in detecting different features.\n",
        "\n",
        "### But There's a Problem...\n",
        "\n",
        "As neural networks grow deeper and more complex, they face new challenges:\n",
        "- **Overfitting:** The committee memorizes cases instead of learning patterns\n",
        "- **Vanishing Gradients:** Feedback becomes too weak in deep networks\n",
        "- **Dead Neurons:** Some specialists stop contributing entirely\n",
        "\n",
        "### Coming Up in Part 8: Deep Learning Challenges\n",
        "\n",
        "In the next notebook, we'll explore:\n",
        "\n",
        "- **Overfitting** - When the committee memorizes instead of learns\n",
        "- **Regularization** - Rules to prevent over-specialization\n",
        "- **Vanishing/Exploding Gradients** - The deep network dilemma\n",
        "- **Solutions** - Dropout, batch normalization, and more\n",
        "\n",
        "---\n",
        "\n",
        "**Continue to Part 8:** `part_8_deep_learning_challenges.ipynb`\n",
        "\n",
        "---\n",
        "\n",
        "*\"With great power comes great responsibility - and new challenges.\"*\n",
        "\n",
        "**The Brain's Decision Committee** - Growing Pains\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
