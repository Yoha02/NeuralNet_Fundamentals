{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Network Fundamentals\n",
        "\n",
        "## Part 6: Evaluation - The Trained Expert\n",
        "\n",
        "### The Brain's Decision Committee - Chapter 6\n",
        "\n",
        "---\n",
        "\n",
        "## The Story So Far...\n",
        "\n",
        "In Part 5, something remarkable happened: our committee member **learned**. Starting with random weights and ~50% accuracy, they adjusted their priorities through gradient descent until they became an expert vertical line detector with 95%+ accuracy.\n",
        "\n",
        "**But how do we know they're actually good?** Getting 95% on training data is one thing, but:\n",
        "- What kinds of mistakes do they still make?\n",
        "- Are some errors worse than others?\n",
        "- Can we understand *why* they make the decisions they do?\n",
        "\n",
        "This is **evaluation** - properly assessing our trained model and understanding what it has learned.\n",
        "\n",
        "---\n",
        "\n",
        "## What You'll Learn in Part 6\n",
        "\n",
        "By the end of this notebook, you will understand:\n",
        "\n",
        "1. **Training vs Inference** - The difference between learning mode and using mode\n",
        "2. **Accuracy** - The simplest metric (and its limitations)\n",
        "3. **Confusion Matrix** - A detailed breakdown of all prediction types\n",
        "4. **Precision & Recall** - Measuring different kinds of correctness\n",
        "5. **F1 Score** - Balancing precision and recall\n",
        "6. **Saliency/Interpretability** - What did the model actually learn?\n",
        "7. **Test Sets** - Why we need data the model has never seen\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Make sure you've completed:\n",
        "- **Parts 0-1:** Matrices (`neural_network_fundamentals.ipynb`)\n",
        "- **Part 2:** Single Neuron (`part_2_single_neuron.ipynb`)\n",
        "- **Part 3:** Activation Functions (`part_3_activation_functions.ipynb`)\n",
        "- **Part 4:** The Perceptron (`part_4_perceptron.ipynb`)\n",
        "- **Part 5:** Training (`part_5_training.ipynb`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Setup: Import Dependencies and Recreate Our Trained Model\n",
        "\n",
        "Let's bring in everything we need and train a model to evaluate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PART 6: EVALUATION - SETUP AND IMPORTS\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Try to import ipywidgets for interactive features\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    WIDGETS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WIDGETS_AVAILABLE = False\n",
        "    print(\"Note: ipywidgets not installed. Interactive features will be limited.\")\n",
        "\n",
        "# Set up matplotlib style\n",
        "style_options = ['seaborn-v0_8-whitegrid', 'seaborn-whitegrid', 'ggplot', 'default']\n",
        "for style in style_options:\n",
        "    try:\n",
        "        plt.style.use(style)\n",
        "        break\n",
        "    except OSError:\n",
        "        continue\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "plt.rcParams['font.size'] = 12\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Setup complete!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# RECREATE OUR TOOLS FROM PREVIOUS NOTEBOOKS\n",
        "# =============================================================================\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Our canonical line images (from Part 1)\n",
        "# -----------------------------------------------------------------------------\n",
        "vertical_line = np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]])\n",
        "horizontal_line = np.array([[0, 0, 0], [1, 1, 1], [0, 0, 0]])\n",
        "vertical_flat = vertical_line.flatten()\n",
        "horizontal_flat = horizontal_line.flatten()\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Dataset generator (from Part 4)\n",
        "# -----------------------------------------------------------------------------\n",
        "def generate_line_dataset(n_samples=100, noise_level=0.0, seed=None):\n",
        "    \"\"\"Generate vertical (label=1) and horizontal (label=0) line images.\"\"\"\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    \n",
        "    X, y = [], []\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "        image = np.zeros((3, 3))\n",
        "        \n",
        "        if i < n_samples // 2:  # Vertical lines\n",
        "            col = np.random.randint(0, 3)\n",
        "            image[:, col] = 1\n",
        "            if noise_level > 0:\n",
        "                image = np.clip(image + np.random.randn(3, 3) * noise_level, 0, 1)\n",
        "            X.append(image.flatten())\n",
        "            y.append(1)\n",
        "        else:  # Horizontal lines\n",
        "            row = np.random.randint(0, 3)\n",
        "            image[row, :] = 1\n",
        "            if noise_level > 0:\n",
        "                image = np.clip(image + np.random.randn(3, 3) * noise_level, 0, 1)\n",
        "            X.append(image.flatten())\n",
        "            y.append(0)\n",
        "    \n",
        "    X, y = np.array(X), np.array(y)\n",
        "    shuffle_idx = np.random.permutation(n_samples)\n",
        "    return X[shuffle_idx], y[shuffle_idx]\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Sigmoid activation function (from Part 3)\n",
        "# -----------------------------------------------------------------------------\n",
        "def sigmoid(z):\n",
        "    \"\"\"Sigmoid activation: maps any value to range (0, 1).\"\"\"\n",
        "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# TrainablePerceptron class (from Part 5)\n",
        "# -----------------------------------------------------------------------------\n",
        "class TrainablePerceptron:\n",
        "    \"\"\"A Perceptron that can learn from examples.\"\"\"\n",
        "    \n",
        "    def __init__(self, n_inputs):\n",
        "        self.weights = np.random.randn(n_inputs) * 0.1\n",
        "        self.bias = 0.0\n",
        "        self.n_inputs = n_inputs\n",
        "        self.loss_history = []\n",
        "        self.accuracy_history = []\n",
        "        self.is_trained = False  # Track if model has been trained\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = np.array(x).flatten()\n",
        "        z = np.dot(self.weights, x) + self.bias\n",
        "        return sigmoid(z)\n",
        "    \n",
        "    def predict(self, x):\n",
        "        return 1 if self.forward(x) >= 0.5 else 0\n",
        "    \n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        epsilon = 1e-15\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "        return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    \n",
        "    def train(self, X, y, learning_rate=0.1, epochs=100, verbose=True):\n",
        "        self.loss_history = []\n",
        "        self.accuracy_history = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            correct = 0\n",
        "            \n",
        "            for i in range(len(X)):\n",
        "                xi, yi = X[i], y[i]\n",
        "                y_pred = self.forward(xi)\n",
        "                loss = self.compute_loss(yi, y_pred)\n",
        "                total_loss += loss\n",
        "                \n",
        "                if (y_pred >= 0.5 and yi == 1) or (y_pred < 0.5 and yi == 0):\n",
        "                    correct += 1\n",
        "                \n",
        "                error = y_pred - yi\n",
        "                self.weights = self.weights - learning_rate * error * xi\n",
        "                self.bias = self.bias - learning_rate * error\n",
        "            \n",
        "            avg_loss = total_loss / len(X)\n",
        "            accuracy = correct / len(X)\n",
        "            self.loss_history.append(avg_loss)\n",
        "            self.accuracy_history.append(accuracy)\n",
        "            \n",
        "            if verbose and (epoch + 1) % 10 == 0:\n",
        "                print(f\"  Epoch {epoch+1:3d}/{epochs}: Loss = {avg_loss:.4f}, Accuracy = {accuracy*100:.1f}%\")\n",
        "        \n",
        "        self.is_trained = True\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\nTraining complete! Final accuracy: {self.accuracy_history[-1]*100:.1f}%\")\n",
        "        \n",
        "        return self.loss_history\n",
        "\n",
        "print(\"Tools recreated from previous notebooks!\")\n",
        "print(\"  - Line image templates\")\n",
        "print(\"  - Dataset generator\")\n",
        "print(\"  - Sigmoid activation\")\n",
        "print(\"  - TrainablePerceptron class\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAIN OUR MODEL (Quick recap from Part 5)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TRAINING OUR MODEL (to have something to evaluate)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Generate training data\n",
        "np.random.seed(42)\n",
        "X_train, y_train = generate_line_dataset(n_samples=100, noise_level=0.0, seed=42)\n",
        "\n",
        "# Generate TEST data (NEW! - data the model has never seen)\n",
        "X_test, y_test = generate_line_dataset(n_samples=50, noise_level=0.0, seed=999)\n",
        "\n",
        "print(f\"\\nTraining set: {len(X_train)} samples\")\n",
        "print(f\"Test set: {len(X_test)} samples (model has NEVER seen these!)\")\n",
        "\n",
        "# Create and train model\n",
        "model = TrainablePerceptron(n_inputs=9)\n",
        "print(\"\\nTraining...\")\n",
        "model.train(X_train, y_train, learning_rate=0.5, epochs=50, verbose=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Model is trained and ready for evaluation!\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6.1 Training vs Inference: The Committee's Memory\n",
        "\n",
        "Before we evaluate, let's understand an important distinction: **training mode** vs **inference mode**.\n",
        "\n",
        "### What IS Inference?\n",
        "\n",
        "The word **\"inference\"** comes from Latin *inferre* meaning \"to bring in\" or \"to conclude.\" In machine learning:\n",
        "\n",
        "**Inference = Using a trained model to make predictions on new data**\n",
        "\n",
        "Think of it like this:\n",
        "- **Training** = Teaching someone how to do a job\n",
        "- **Inference** = That person actually doing the job\n",
        "\n",
        "### Why Two Different Modes?\n",
        "\n",
        "| Aspect | Training Mode | Inference Mode |\n",
        "|--------|--------------|----------------|\n",
        "| **Purpose** | Learn from examples | Make predictions |\n",
        "| **Weights** | Being updated constantly | Frozen (fixed) |\n",
        "| **Data** | Training set (with labels) | New, unseen data |\n",
        "| **Speed** | Slower (computing gradients) | Fast (forward pass only) |\n",
        "| **Goal** | Minimize loss | Predict accurately |\n",
        "\n",
        "### Committee Analogy\n",
        "\n",
        "*\"During training, the committee is in a meeting room, debating cases, learning from mistakes, and updating their rulebook. Once trained, they compile their final rulebook and hand it to the front desk. The front desk uses this rulebook to make quick decisions without calling the committee for every case.\"*\n",
        "\n",
        "- **Training:** The committee meeting (slow, learning, updating)\n",
        "- **Inference:** The front desk using the final rulebook (fast, fixed, no learning)\n",
        "\n",
        "### Why Does This Distinction Matter?\n",
        "\n",
        "| Scenario | Why It Matters |\n",
        "|----------|----------------|\n",
        "| **Deployment** | In production, you use inference mode for speed |\n",
        "| **Evaluation** | We evaluate in inference mode (weights must be fixed!) |\n",
        "| **Consistency** | Same weights give same predictions every time |\n",
        "| **Resources** | Inference uses less memory (no gradients stored) |\n",
        "\n",
        "### The Key Insight\n",
        "\n",
        "During inference, the model does NOT learn anything new. The weights are \"frozen\" - they don't change. This is essential because:\n",
        "\n",
        "1. **Reproducibility**: Same input always gives same output\n",
        "2. **Speed**: No gradient computation needed\n",
        "3. **Fairness**: Test data doesn't influence the model\n",
        "\n",
        "### Why \"Frozen\" Weights Matter Mathematically\n",
        "\n",
        "During training, after each prediction, we do:\n",
        "```\n",
        "weights = weights - learning_rate × gradient\n",
        "```\n",
        "\n",
        "During inference, we SKIP this step entirely. The weights stay exactly as they were after training finished.\n",
        "\n",
        "**Why does this matter?**\n",
        "\n",
        "| If we kept updating during inference... | Consequence |\n",
        "|----------------------------------------|-------------|\n",
        "| Weights would change with each new input | Same input could give different outputs! |\n",
        "| Model would \"drift\" over time | Yesterday's predictions wouldn't match today's |\n",
        "| Hard to reproduce results | \"But it worked yesterday!\" |\n",
        "| Unfair for test evaluation | Test data would influence the model |\n",
        "\n",
        "**The mathematical guarantee:** With frozen weights, $f(x) = \\sigma(w \\cdot x + b)$ is a **deterministic function** - same input ALWAYS gives same output.\n",
        "\n",
        "### In Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAINING VS INFERENCE: Demonstration\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TRAINING vs INFERENCE MODE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Show the model's state\n",
        "print(f\"\\nModel state: {'TRAINED' if model.is_trained else 'UNTRAINED'}\")\n",
        "\n",
        "# In training mode, weights change after each sample\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"DURING TRAINING (weights change):\")\n",
        "print(\"-\"*70)\n",
        "print(\"  For each sample:\")\n",
        "print(\"    1. Forward pass → get prediction\")\n",
        "print(\"    2. Compute loss → how wrong?\")\n",
        "print(\"    3. Compute gradients → which direction?\")\n",
        "print(\"    4. Update weights → improve! (weights CHANGE)\")\n",
        "\n",
        "# In inference mode, weights are frozen\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"DURING INFERENCE (weights frozen):\")\n",
        "print(\"-\"*70)\n",
        "print(\"  For each sample:\")\n",
        "print(\"    1. Forward pass → get prediction\")\n",
        "print(\"    2. Done! (NO weight updates)\")\n",
        "\n",
        "# Demonstrate inference\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"INFERENCE EXAMPLE:\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Save weights before\n",
        "weights_before = model.weights.copy()\n",
        "\n",
        "# Make predictions (inference)\n",
        "pred_v = model.forward(vertical_flat)\n",
        "pred_h = model.forward(horizontal_flat)\n",
        "\n",
        "# Check weights after\n",
        "weights_after = model.weights.copy()\n",
        "\n",
        "print(f\"\\n  Vertical line:   {pred_v:.4f} ({pred_v*100:.1f}% confident it's vertical)\")\n",
        "print(f\"  Horizontal line: {pred_h:.4f} ({pred_h*100:.1f}% confident it's vertical)\")\n",
        "print(f\"\\n  Weights changed? {not np.allclose(weights_before, weights_after)}\")\n",
        "print(f\"  (In inference mode, weights stay fixed!)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6.2 Accuracy: The Simplest Metric\n",
        "\n",
        "We've been using accuracy throughout our notebooks, but let's formally define it and understand its limitations.\n",
        "\n",
        "### What IS Accuracy?\n",
        "\n",
        "**Accuracy** answers the question: \"Of all the predictions I made, what fraction was correct?\"\n",
        "\n",
        "$$\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}$$\n",
        "\n",
        "### Breaking Down the Formula\n",
        "\n",
        "Let's understand each part:\n",
        "\n",
        "| Component | What It Means | Our Example |\n",
        "|-----------|---------------|-------------|\n",
        "| **Correct Predictions** | Cases where prediction matches truth | Said \"vertical\" for vertical, \"horizontal\" for horizontal |\n",
        "| **Total Predictions** | All cases we predicted on | All 50 test images |\n",
        "| **Accuracy** | The ratio (0 to 1, or 0% to 100%) | 48/50 = 0.96 = 96% |\n",
        "\n",
        "### Computing Accuracy Step by Step\n",
        "\n",
        "```\n",
        "Step 1: Make predictions on all samples\n",
        "Step 2: Compare each prediction to the true label\n",
        "Step 3: Count how many match (correct)\n",
        "Step 4: Divide by total number of predictions\n",
        "```\n",
        "\n",
        "### Why Accuracy Can Be Misleading\n",
        "\n",
        "Accuracy has a hidden flaw: **it treats all mistakes equally** and **ignores class imbalance**.\n",
        "\n",
        "**Example - Fraud Detection:**\n",
        "\n",
        "Suppose 99% of transactions are legitimate, 1% are fraud.\n",
        "\n",
        "| Model Strategy | Accuracy | Is It Good? |\n",
        "|----------------|----------|-------------|\n",
        "| Say \"legitimate\" for EVERYTHING | 99% | NO! Catches 0% of fraud! |\n",
        "| Actually detect fraud | 97% | YES! Even though lower accuracy |\n",
        "\n",
        "The \"dumb\" model gets 99% accuracy by ignoring the problem entirely!\n",
        "\n",
        "**Example - Medical Diagnosis:**\n",
        "\n",
        "| Scenario | Type of Error | Consequence |\n",
        "|----------|---------------|-------------|\n",
        "| Say \"healthy\" when patient is sick | Miss a disease | Patient doesn't get treatment! (VERY bad) |\n",
        "| Say \"sick\" when patient is healthy | False alarm | Unnecessary tests (annoying but not dangerous) |\n",
        "\n",
        "Both are \"wrong\" but one is much worse! Accuracy treats them the same.\n",
        "\n",
        "### When Accuracy Works Well\n",
        "\n",
        "Accuracy is a good metric when:\n",
        "1. **Classes are balanced** (roughly 50/50 split)\n",
        "2. **All mistakes have equal cost**\n",
        "3. **You want a quick overall view**\n",
        "\n",
        "Our V/H classifier is a good case for accuracy: balanced classes, equal mistake costs.\n",
        "\n",
        "### Understanding Why Class Imbalance Breaks Accuracy\n",
        "\n",
        "Let's do the math to see WHY accuracy is misleading with imbalanced data:\n",
        "\n",
        "**Scenario: Fraud Detection (1% fraud, 99% legitimate)**\n",
        "\n",
        "| Strategy | Fraud Caught | Accuracy Calculation |\n",
        "|----------|-------------|---------------------|\n",
        "| **\"Always say legitimate\"** | 0 of 100 frauds | (0 + 9900) / 10000 = **99%** |\n",
        "| **Good detector** | 90 of 100 frauds | (90 + 9800) / 10000 = **98.9%** |\n",
        "\n",
        "The \"dumb\" strategy has HIGHER accuracy but catches ZERO fraud!\n",
        "\n",
        "**Why this happens mathematically:**\n",
        "\n",
        "$$\\text{Accuracy} = \\frac{TP + TN}{\\text{Total}}$$\n",
        "\n",
        "When 99% of data is class 0, you can get 99% accuracy by predicting 0 for everything (TN = 9900, everything else = 0).\n",
        "\n",
        "**The lesson:** When classes are imbalanced, accuracy is dominated by the majority class. We need metrics that focus on the minority class (precision, recall).\n",
        "\n",
        "### Let's Calculate Accuracy Properly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ACCURACY: Step-by-Step Calculation\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CALCULATING ACCURACY: Step by Step\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def calculate_accuracy(model, X, y, verbose=True):\n",
        "    \"\"\"\n",
        "    Calculate accuracy of model on given data.\n",
        "    \n",
        "    Parameters:\n",
        "        model: Trained model with predict() method\n",
        "        X: Input data (n_samples, n_features)\n",
        "        y: True labels (n_samples,)\n",
        "        verbose: Whether to print details\n",
        "    \n",
        "    Returns:\n",
        "        accuracy: Float between 0 and 1\n",
        "        predictions: Array of predicted labels\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    correct = 0\n",
        "    \n",
        "    for i in range(len(X)):\n",
        "        pred = model.predict(X[i])\n",
        "        predictions.append(pred)\n",
        "        if pred == y[i]:\n",
        "            correct += 1\n",
        "    \n",
        "    accuracy = correct / len(y)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\n  Total samples: {len(y)}\")\n",
        "        print(f\"  Correct: {correct}\")\n",
        "        print(f\"  Wrong: {len(y) - correct}\")\n",
        "        print(f\"  Accuracy: {correct}/{len(y)} = {accuracy:.4f} = {accuracy*100:.1f}%\")\n",
        "    \n",
        "    return accuracy, np.array(predictions)\n",
        "\n",
        "# Calculate accuracy on TRAINING data\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"TRAINING SET ACCURACY:\")\n",
        "print(\"-\"*70)\n",
        "train_accuracy, train_preds = calculate_accuracy(model, X_train, y_train)\n",
        "\n",
        "# Calculate accuracy on TEST data (NEW!)\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"TEST SET ACCURACY:\")\n",
        "print(\"-\"*70)\n",
        "test_accuracy, test_preds = calculate_accuracy(model, X_test, y_test)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"KEY INSIGHT: Training vs Test Accuracy\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\"\"\n",
        "Training accuracy: {train_accuracy*100:.1f}%\n",
        "Test accuracy:     {test_accuracy*100:.1f}%\n",
        "\n",
        "The TEST accuracy is what really matters!\n",
        "Training accuracy can be misleadingly high if the model \"memorizes\" the data.\n",
        "Test accuracy shows how well the model generalizes to NEW data.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6.3 The Confusion Matrix: A Detailed Report Card\n",
        "\n",
        "Accuracy gives us one number. But what if we want to understand **WHICH** mistakes the model makes?\n",
        "\n",
        "### What IS a Confusion Matrix?\n",
        "\n",
        "A **confusion matrix** is a table that breaks down all predictions into four categories based on two questions:\n",
        "1. What did we **predict**?\n",
        "2. What was the **actual** truth?\n",
        "\n",
        "```\n",
        "                      PREDICTED\n",
        "                    0        1\n",
        "              ┌─────────┬─────────┐\n",
        "        0     │   TN    │   FP    │\n",
        "   ACTUAL     ├─────────┼─────────┤\n",
        "        1     │   FN    │   TP    │\n",
        "              └─────────┴─────────┘\n",
        "```\n",
        "\n",
        "### Why \"Confusion\" Matrix?\n",
        "\n",
        "The name comes from the fact that it shows how the model gets \"confused\" - where it mixes up one class for another.\n",
        "\n",
        "### Understanding the Four Categories\n",
        "\n",
        "| Abbrev | Full Name | Meaning | Our Example |\n",
        "|--------|-----------|---------|-------------|\n",
        "| **TP** | True Positive | Predicted 1, was actually 1 | Said \"vertical\", WAS vertical ✓ |\n",
        "| **TN** | True Negative | Predicted 0, was actually 0 | Said \"horizontal\", WAS horizontal ✓ |\n",
        "| **FP** | False Positive | Predicted 1, was actually 0 | Said \"vertical\", was horizontal ✗ |\n",
        "| **FN** | False Negative | Predicted 0, was actually 1 | Said \"horizontal\", was vertical ✗ |\n",
        "\n",
        "### Memory Trick for TP/TN/FP/FN\n",
        "\n",
        "Think of it as TWO questions:\n",
        "\n",
        "1. **True/False:** Was the prediction **correct**?\n",
        "   - True = correct\n",
        "   - False = wrong\n",
        "\n",
        "2. **Positive/Negative:** What did we **predict**?\n",
        "   - Positive = predicted class 1 (vertical)\n",
        "   - Negative = predicted class 0 (horizontal)\n",
        "\n",
        "So:\n",
        "- **True Positive** = We were True (correct) when we predicted Positive (vertical)\n",
        "- **False Positive** = We were False (wrong) when we predicted Positive (vertical)\n",
        "- **True Negative** = We were True (correct) when we predicted Negative (horizontal)  \n",
        "- **False Negative** = We were False (wrong) when we predicted Negative (horizontal)\n",
        "\n",
        "### Committee Analogy\n",
        "\n",
        "*\"The confusion matrix is like a detailed performance review for our committee member:*\n",
        "- *TP: Cases they correctly identified as vertical*\n",
        "- *TN: Cases they correctly identified as NOT vertical*\n",
        "- *FP: Cases they wrongly called vertical (a false alarm!)*\n",
        "- *FN: Cases they missed (should have said vertical but didn't)\"*\n",
        "\n",
        "### Alternative Names You'll See\n",
        "\n",
        "| Our Term | Also Called | When Used |\n",
        "|----------|-------------|-----------|\n",
        "| False Positive | Type I Error | Statistics |\n",
        "| False Negative | Type II Error | Statistics |\n",
        "| True Positive Rate | Sensitivity, Recall | Medical |\n",
        "| True Negative Rate | Specificity | Medical |\n",
        "\n",
        "### Real-World Examples of Each Error Type\n",
        "\n",
        "Understanding these errors is easier with concrete examples:\n",
        "\n",
        "| Error Type | Medical Example | Email Example | Self-Driving Car |\n",
        "|------------|-----------------|---------------|------------------|\n",
        "| **TP** | Correctly diagnose sick patient | Correctly mark spam | Correctly detect pedestrian |\n",
        "| **TN** | Correctly clear healthy patient | Correctly allow good email | Correctly ignore false alarm |\n",
        "| **FP** | Diagnose healthy as sick | Mark good email as spam | Brake for nothing (annoying) |\n",
        "| **FN** | Miss a sick patient | Allow spam through | Miss a pedestrian (FATAL!) |\n",
        "\n",
        "**Notice:** The consequences of FP vs FN are very different depending on the application!\n",
        "- **Medical:** FN is worse (missed diagnosis can be fatal)\n",
        "- **Spam filter:** FP is worse (losing important emails)\n",
        "- **Self-driving:** FN is MUCH worse (hitting someone)\n",
        "\n",
        "This is why we have precision and recall - to measure these separately.\n",
        "\n",
        "### Let's Build a Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFUSION MATRIX: Implementation and Explanation\n",
        "# =============================================================================\n",
        "\n",
        "def confusion_matrix(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the confusion matrix.\n",
        "    \n",
        "    The logic behind each calculation:\n",
        "    - TP: prediction=1 AND truth=1 (both conditions true)\n",
        "    - TN: prediction=0 AND truth=0 (both conditions true)\n",
        "    - FP: prediction=1 AND truth=0 (predicted positive, was negative)\n",
        "    - FN: prediction=0 AND truth=1 (predicted negative, was positive)\n",
        "    \n",
        "    Parameters:\n",
        "        y_true: Array of true labels (0 or 1)\n",
        "        y_pred: Array of predicted labels (0 or 1)\n",
        "    \n",
        "    Returns:\n",
        "        dict with TP, TN, FP, FN counts\n",
        "    \"\"\"\n",
        "    # True Positive: We said 1, it was 1\n",
        "    TP = np.sum((y_pred == 1) & (y_true == 1))\n",
        "    \n",
        "    # True Negative: We said 0, it was 0\n",
        "    TN = np.sum((y_pred == 0) & (y_true == 0))\n",
        "    \n",
        "    # False Positive: We said 1, but it was 0 (false alarm!)\n",
        "    FP = np.sum((y_pred == 1) & (y_true == 0))\n",
        "    \n",
        "    # False Negative: We said 0, but it was 1 (missed it!)\n",
        "    FN = np.sum((y_pred == 0) & (y_true == 1))\n",
        "    \n",
        "    return {'TP': TP, 'TN': TN, 'FP': FP, 'FN': FN}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CONFUSION MATRIX: Step by Step\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Calculate confusion matrix for test set\n",
        "cm = confusion_matrix(y_test, test_preds)\n",
        "\n",
        "print(\"\\nFor our TEST set:\")\n",
        "print(f\"  Total samples: {len(y_test)}\")\n",
        "print(f\"  Vertical lines (label=1): {np.sum(y_test == 1)}\")\n",
        "print(f\"  Horizontal lines (label=0): {np.sum(y_test == 0)}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"CONFUSION MATRIX BREAKDOWN:\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "print(f\"\"\"\n",
        "                       PREDICTED\n",
        "                   Horizontal(0)  Vertical(1)\n",
        "              ┌─────────────────┬─────────────────┐\n",
        "   Horiz.(0)  │  TN = {cm['TN']:3d}       │  FP = {cm['FP']:3d}       │\n",
        "   ACTUAL     ├─────────────────┼─────────────────┤\n",
        "   Vert.(1)   │  FN = {cm['FN']:3d}       │  TP = {cm['TP']:3d}       │\n",
        "              └─────────────────┴─────────────────┘\n",
        "\"\"\")\n",
        "\n",
        "print(\"Interpretation (reading the matrix):\")\n",
        "print(f\"  ✓ True Positives (TP = {cm['TP']}): Correctly identified as VERTICAL\")\n",
        "print(f\"  ✓ True Negatives (TN = {cm['TN']}): Correctly identified as HORIZONTAL\")\n",
        "print(f\"  ✗ False Positives (FP = {cm['FP']}): Wrongly called VERTICAL (was horizontal)\")\n",
        "print(f\"  ✗ False Negatives (FN = {cm['FN']}): Wrongly called HORIZONTAL (was vertical)\")\n",
        "\n",
        "# Verify: TP + TN + FP + FN should equal total samples\n",
        "total = cm['TP'] + cm['TN'] + cm['FP'] + cm['FN']\n",
        "print(f\"\\n  Verification: TP + TN + FP + FN = {total} (should equal {len(y_test)}) ✓\")\n",
        "\n",
        "# Show how accuracy relates to confusion matrix\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"ACCURACY FROM CONFUSION MATRIX:\")\n",
        "print(\"-\"*70)\n",
        "print(f\"\"\"\n",
        "  Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "           = ({cm['TP']} + {cm['TN']}) / ({cm['TP']} + {cm['TN']} + {cm['FP']} + {cm['FN']})\n",
        "           = {cm['TP'] + cm['TN']} / {total}\n",
        "           = {(cm['TP'] + cm['TN']) / total:.4f}\n",
        "           = {(cm['TP'] + cm['TN']) / total * 100:.1f}%\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZE THE CONFUSION MATRIX\n",
        "# =============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Confusion Matrix as heatmap\n",
        "ax1 = axes[0]\n",
        "cm_matrix = np.array([[cm['TN'], cm['FP']], \n",
        "                       [cm['FN'], cm['TP']]])\n",
        "\n",
        "im = ax1.imshow(cm_matrix, cmap='Blues')\n",
        "ax1.set_xticks([0, 1])\n",
        "ax1.set_yticks([0, 1])\n",
        "ax1.set_xticklabels(['Horizontal (0)', 'Vertical (1)'])\n",
        "ax1.set_yticklabels(['Horizontal (0)', 'Vertical (1)'])\n",
        "ax1.set_xlabel('Predicted Label', fontsize=12)\n",
        "ax1.set_ylabel('Actual Label', fontsize=12)\n",
        "ax1.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Add text annotations\n",
        "labels = [['TN', 'FP'], ['FN', 'TP']]\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        text_color = 'white' if cm_matrix[i, j] > cm_matrix.max()/2 else 'black'\n",
        "        ax1.text(j, i, f'{labels[i][j]}\\n{cm_matrix[i, j]}', \n",
        "                ha='center', va='center', fontsize=14, fontweight='bold', color=text_color)\n",
        "\n",
        "plt.colorbar(im, ax=ax1)\n",
        "\n",
        "# Plot 2: Visual explanation\n",
        "ax2 = axes[1]\n",
        "ax2.axis('off')\n",
        "\n",
        "explanation_text = f\"\"\"\n",
        "READING THE CONFUSION MATRIX\n",
        "{'='*45}\n",
        "\n",
        "The DIAGONAL (top-left to bottom-right) shows \n",
        "CORRECT predictions:\n",
        "  • TN ({cm['TN']}): Horizontal predicted as Horizontal ✓\n",
        "  • TP ({cm['TP']}): Vertical predicted as Vertical ✓\n",
        "\n",
        "The OFF-DIAGONAL shows ERRORS:\n",
        "  • FP ({cm['FP']}): Horizontal wrongly called Vertical ✗\n",
        "  • FN ({cm['FN']}): Vertical wrongly called Horizontal ✗\n",
        "\n",
        "A PERFECT model has:\n",
        "  • All values on the diagonal\n",
        "  • Zeros everywhere else\n",
        "\"\"\"\n",
        "\n",
        "ax2.text(0.1, 0.5, explanation_text, fontsize=11, family='monospace',\n",
        "        verticalalignment='center', transform=ax2.transAxes,\n",
        "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6.4 Precision, Recall, and F1 Score\n",
        "\n",
        "The confusion matrix gives us four numbers. From these, we can calculate more specific metrics that answer different questions.\n",
        "\n",
        "### Precision: \"When I Say Positive, Am I Right?\"\n",
        "\n",
        "**Precision** answers: \"Of all the times I predicted 'positive' (vertical), how many were actually positive?\"\n",
        "\n",
        "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
        "\n",
        "**Breaking it down:**\n",
        "- **Numerator (TP):** Cases we correctly called positive\n",
        "- **Denominator (TP + FP):** ALL cases we called positive (right or wrong)\n",
        "\n",
        "**High precision means:** When we say \"vertical\", we're usually right. Few false alarms.\n",
        "\n",
        "**When to prioritize precision:**\n",
        "- Spam filters (don't delete legitimate emails!)\n",
        "- Recommender systems (don't recommend things users hate!)\n",
        "- Any case where false alarms are costly\n",
        "\n",
        "### Recall: \"Did I Catch All the Positives?\"\n",
        "\n",
        "**Recall** (also called **Sensitivity**) answers: \"Of all the actual positives, how many did I catch?\"\n",
        "\n",
        "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
        "\n",
        "**Breaking it down:**\n",
        "- **Numerator (TP):** Cases we correctly caught\n",
        "- **Denominator (TP + FN):** ALL actual positives (caught or missed)\n",
        "\n",
        "**High recall means:** We catch most of the actual vertical lines. Few misses.\n",
        "\n",
        "**When to prioritize recall:**\n",
        "- Disease detection (don't miss sick patients!)\n",
        "- Fraud detection (don't miss fraudulent transactions!)\n",
        "- Any case where missing positives is costly\n",
        "\n",
        "### The Precision-Recall Trade-off\n",
        "\n",
        "Here's the fundamental tension:\n",
        "\n",
        "| Strategy | Precision | Recall | Problem |\n",
        "|----------|-----------|--------|---------|\n",
        "| \"Only say vertical when 100% sure\" | HIGH (few false alarms) | LOW (miss many) | Miss too many positives |\n",
        "| \"Say vertical for anything remotely vertical\" | LOW (many false alarms) | HIGH (catch most) | Too many false alarms |\n",
        "\n",
        "**You often can't maximize both!** This is called the **precision-recall trade-off**.\n",
        "\n",
        "### Concrete Example: Airport Security\n",
        "\n",
        "Imagine a security scanner detecting threats:\n",
        "\n",
        "| Setting | Precision | Recall | Outcome |\n",
        "|---------|-----------|--------|---------|\n",
        "| **Super sensitive** | 10% | 99% | Catches ALL threats but 90% of \"threats\" are false alarms. Massive delays! |\n",
        "| **Super strict** | 95% | 20% | Few false alarms but misses 80% of real threats. Dangerous! |\n",
        "| **Balanced** | 70% | 70% | Some false alarms, catches most threats. Practical! |\n",
        "\n",
        "**Why the trade-off exists:**\n",
        "\n",
        "When we lower the threshold for saying \"positive\":\n",
        "- We catch MORE true positives (recall goes UP ↑)\n",
        "- But we also catch MORE false positives (precision goes DOWN ↓)\n",
        "\n",
        "When we raise the threshold:\n",
        "- We have FEWER false positives (precision goes UP ↑)\n",
        "- But we miss MORE true positives (recall goes DOWN ↓)\n",
        "\n",
        "**There's no free lunch!** The art is finding the right balance for your specific application.\n",
        "\n",
        "### F1 Score: Finding the Balance\n",
        "\n",
        "The **F1 Score** is the **harmonic mean** of precision and recall - a single number that balances both:\n",
        "\n",
        "$$F1 = 2 \\cdot \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
        "\n",
        "### What IS a Harmonic Mean and Why Use It?\n",
        "\n",
        "You might wonder: \"Why not just use a regular average (arithmetic mean)?\"\n",
        "\n",
        "**Three Types of Means:**\n",
        "\n",
        "| Mean Type | Formula | Example: (99%, 10%) |\n",
        "|-----------|---------|---------------------|\n",
        "| **Arithmetic** | (a + b) / 2 | (99 + 10) / 2 = **54.5%** |\n",
        "| **Geometric** | √(a × b) | √(99 × 10) = **31.5%** |\n",
        "| **Harmonic** | 2ab / (a + b) | 2×99×10 / (99+10) = **18.2%** |\n",
        "\n",
        "**Why harmonic mean is better for F1:**\n",
        "\n",
        "The harmonic mean is **punishing when values are imbalanced**. If you have 99% precision but only 10% recall:\n",
        "- Arithmetic mean says \"54.5% - not bad!\"\n",
        "- Harmonic mean says \"18.2% - this is terrible!\"\n",
        "\n",
        "**The harmonic mean forces BOTH values to be reasonably high to get a good score.**\n",
        "\n",
        "**Intuition:** Think about speed. If you drive 60 mph for half a trip and 20 mph for the other half, your average speed isn't 40 mph - it's closer to 30 mph (harmonic mean). The slow part dominates.\n",
        "\n",
        "**Why this matters for ML:**\n",
        "A model that predicts \"positive\" for everything gets 100% recall but ~0% precision. The harmonic mean correctly identifies this as a terrible model.\n",
        "\n",
        "| Precision | Recall | F1 Score | Verdict |\n",
        "|-----------|--------|----------|---------|\n",
        "| 90% | 90% | 90% | Great! Both balanced |\n",
        "| 99% | 10% | 18% | Terrible! Very unbalanced |\n",
        "| 50% | 50% | 50% | Mediocre |\n",
        "\n",
        "**F1 is high only when BOTH precision AND recall are reasonably high.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PRECISION, RECALL, F1: Calculation\n",
        "# =============================================================================\n",
        "\n",
        "def calculate_metrics(cm):\n",
        "    \"\"\"\n",
        "    Calculate precision, recall, F1 from confusion matrix.\n",
        "    \n",
        "    Parameters:\n",
        "        cm: dict with TP, TN, FP, FN\n",
        "    \n",
        "    Returns:\n",
        "        dict with precision, recall, f1, accuracy\n",
        "    \"\"\"\n",
        "    TP, TN, FP, FN = cm['TP'], cm['TN'], cm['FP'], cm['FN']\n",
        "    \n",
        "    # Precision: When we say positive, are we right?\n",
        "    # Note: We add a check to avoid division by zero\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    \n",
        "    # Recall: Did we catch all the positives?\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    \n",
        "    # F1: Harmonic mean of precision and recall\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    # Accuracy (for comparison)\n",
        "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "    \n",
        "    return {'precision': precision, 'recall': recall, 'f1': f1, 'accuracy': accuracy}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PRECISION, RECALL, AND F1 SCORE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "metrics = calculate_metrics(cm)\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"STEP-BY-STEP CALCULATION:\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "print(f\"\"\"\n",
        "From our confusion matrix:\n",
        "  TP = {cm['TP']} (correctly identified vertical lines)\n",
        "  TN = {cm['TN']} (correctly identified horizontal lines)\n",
        "  FP = {cm['FP']} (horizontal lines wrongly called vertical)\n",
        "  FN = {cm['FN']} (vertical lines wrongly called horizontal)\n",
        "\n",
        "PRECISION: \"When I say vertical, am I right?\"\n",
        "  Formula: Precision = TP / (TP + FP)\n",
        "  \n",
        "  Precision = {cm['TP']} / ({cm['TP']} + {cm['FP']})\n",
        "            = {cm['TP']} / {cm['TP'] + cm['FP']}\n",
        "            = {metrics['precision']:.4f}\n",
        "            = {metrics['precision']*100:.1f}%\n",
        "\n",
        "RECALL: \"Did I catch all the vertical lines?\"\n",
        "  Formula: Recall = TP / (TP + FN)\n",
        "  \n",
        "  Recall = {cm['TP']} / ({cm['TP']} + {cm['FN']})\n",
        "         = {cm['TP']} / {cm['TP'] + cm['FN']}\n",
        "         = {metrics['recall']:.4f}\n",
        "         = {metrics['recall']*100:.1f}%\n",
        "\n",
        "F1 SCORE: \"Balance of precision and recall\"\n",
        "  Formula: F1 = 2 × (Precision × Recall) / (Precision + Recall)\n",
        "  \n",
        "  F1 = 2 × ({metrics['precision']:.4f} × {metrics['recall']:.4f}) / ({metrics['precision']:.4f} + {metrics['recall']:.4f})\n",
        "     = 2 × {metrics['precision'] * metrics['recall']:.4f} / {metrics['precision'] + metrics['recall']:.4f}\n",
        "     = {metrics['f1']:.4f}\n",
        "     = {metrics['f1']*100:.1f}%\n",
        "\n",
        "ACCURACY (for comparison):\n",
        "  Formula: Accuracy = (TP + TN) / Total\n",
        "  \n",
        "  Accuracy = ({cm['TP']} + {cm['TN']}) / {cm['TP'] + cm['TN'] + cm['FP'] + cm['FN']}\n",
        "           = {cm['TP'] + cm['TN']} / {cm['TP'] + cm['TN'] + cm['FP'] + cm['FN']}\n",
        "           = {metrics['accuracy']:.4f}\n",
        "           = {metrics['accuracy']*100:.1f}%\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DEMONSTRATING THE PRECISION-RECALL TRADE-OFF\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"THE PRECISION-RECALL TRADE-OFF: A Visual Demonstration\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "To understand the trade-off, let's see what happens when we change\n",
        "our THRESHOLD for saying \"vertical\" (positive).\n",
        "\n",
        "Currently we use: threshold = 0.5\n",
        "  - If output >= 0.5 → predict \"vertical\"\n",
        "  - If output < 0.5 → predict \"horizontal\"\n",
        "\n",
        "But what if we change this threshold?\n",
        "\"\"\")\n",
        "\n",
        "# Try different thresholds\n",
        "thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "results = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    # Make predictions at this threshold\n",
        "    preds = np.array([1 if model.forward(x) >= threshold else 0 for x in X_test])\n",
        "    \n",
        "    # Calculate confusion matrix\n",
        "    TP = np.sum((preds == 1) & (y_test == 1))\n",
        "    TN = np.sum((preds == 0) & (y_test == 0))\n",
        "    FP = np.sum((preds == 1) & (y_test == 0))\n",
        "    FN = np.sum((preds == 0) & (y_test == 1))\n",
        "    \n",
        "    # Calculate metrics\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    results.append({\n",
        "        'threshold': threshold,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'TP': TP, 'FP': FP, 'FN': FN\n",
        "    })\n",
        "    \n",
        "    print(f\"Threshold = {threshold}:\")\n",
        "    print(f\"  TP={TP:2d}, FP={FP:2d}, FN={FN:2d}\")\n",
        "    print(f\"  Precision={precision:.1%}, Recall={recall:.1%}, F1={f1:.1%}\")\n",
        "    print()\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Precision vs Recall at different thresholds\n",
        "ax = axes[0]\n",
        "precisions = [r['precision'] for r in results]\n",
        "recalls = [r['recall'] for r in results]\n",
        "\n",
        "ax.plot(recalls, precisions, 'b-o', linewidth=2, markersize=10)\n",
        "for r in results:\n",
        "    ax.annotate(f\"  t={r['threshold']}\", \n",
        "               (r['recall'], r['precision']), fontsize=9)\n",
        "\n",
        "ax.set_xlabel('Recall', fontsize=12)\n",
        "ax.set_ylabel('Precision', fontsize=12)\n",
        "ax.set_title('Precision-Recall Trade-off\\n(Each point is a different threshold)', \n",
        "            fontsize=12, fontweight='bold')\n",
        "ax.set_xlim(-0.05, 1.05)\n",
        "ax.set_ylim(-0.05, 1.05)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Add ideal point\n",
        "ax.scatter([1], [1], color='gold', s=200, marker='*', zorder=5, label='Ideal (1,1)')\n",
        "ax.legend()\n",
        "\n",
        "# Plot 2: Bar chart showing trade-off\n",
        "ax = axes[1]\n",
        "x = np.arange(len(thresholds))\n",
        "width = 0.25\n",
        "\n",
        "bars1 = ax.bar(x - width, precisions, width, label='Precision', color='#e74c3c')\n",
        "bars2 = ax.bar(x, recalls, width, label='Recall', color='#27ae60')\n",
        "bars3 = ax.bar(x + width, [r['f1'] for r in results], width, label='F1', color='#9b59b6')\n",
        "\n",
        "ax.set_xlabel('Threshold', fontsize=12)\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_title('Metrics at Different Thresholds', fontsize=12, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([f'{t}' for t in thresholds])\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1.1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\"\"\n",
        "KEY INSIGHT:\n",
        "════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "• LOW threshold (0.1): \"Say vertical for almost everything!\"\n",
        "  → High recall (catch most verticals) but low precision (many false alarms)\n",
        "  \n",
        "• HIGH threshold (0.9): \"Only say vertical when VERY confident!\"\n",
        "  → High precision (rarely wrong when we say vertical) but low recall (miss many)\n",
        "  \n",
        "• MIDDLE threshold (0.5): Balanced trade-off\n",
        "\n",
        "Notice how the precision-recall curve shows the trade-off: as one goes up, \n",
        "the other tends to go down. F1 score helps us find a good balance!\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZE ALL METRICS\n",
        "# =============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Bar chart of all metrics\n",
        "ax1 = axes[0]\n",
        "metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "metric_values = [metrics['accuracy'], metrics['precision'], metrics['recall'], metrics['f1']]\n",
        "colors = ['#3498db', '#e74c3c', '#27ae60', '#9b59b6']\n",
        "\n",
        "bars = ax1.bar(metric_names, metric_values, color=colors, edgecolor='white', linewidth=2)\n",
        "ax1.set_ylim(0, 1.1)\n",
        "ax1.set_ylabel('Score', fontsize=12)\n",
        "ax1.set_title('Model Performance Metrics', fontsize=14, fontweight='bold')\n",
        "ax1.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5, label='Perfect score')\n",
        "\n",
        "# Add value labels\n",
        "for bar, val in zip(bars, metric_values):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
        "            f'{val:.1%}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Plot 2: Which metric to use guide\n",
        "ax2 = axes[1]\n",
        "ax2.axis('off')\n",
        "\n",
        "metrics_explanation = \"\"\"\n",
        "WHICH METRIC SHOULD YOU USE?\n",
        "═══════════════════════════════════════════════════\n",
        "\n",
        "ACCURACY\n",
        "  • Best when: Classes are balanced (50/50)\n",
        "  • Misleading when: Rare events (e.g., 1% fraud)\n",
        "  \n",
        "PRECISION\n",
        "  • Best when: False alarms are COSTLY\n",
        "  • Examples: \n",
        "    - Spam filter (don't delete real email!)\n",
        "    - Criminal conviction (don't jail innocent!)\n",
        "  \n",
        "RECALL\n",
        "  • Best when: Missing positives is COSTLY\n",
        "  • Examples: \n",
        "    - Disease detection (don't miss sick patients!)\n",
        "    - Fraud detection (don't miss fraud!)\n",
        "  \n",
        "F1 SCORE\n",
        "  • Best when: You need balance between P & R\n",
        "  • Most real-world applications use F1\n",
        "\n",
        "═══════════════════════════════════════════════════\n",
        "For our V/H classifier, all metrics are similar \n",
        "because our dataset is balanced and model works well!\n",
        "\"\"\"\n",
        "\n",
        "ax2.text(0.05, 0.5, metrics_explanation, fontsize=10, family='monospace',\n",
        "        verticalalignment='center', transform=ax2.transAxes,\n",
        "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6.5 The Committee Report: Saliency and Interpretability\n",
        "\n",
        "We know our model works well, but **WHY** does it work? What has it actually learned?\n",
        "\n",
        "### What IS Interpretability?\n",
        "\n",
        "**Interpretability** (also called **Explainability**) means understanding:\n",
        "1. What patterns did the model learn?\n",
        "2. Why does it make specific predictions?\n",
        "3. Is it using the \"right\" features?\n",
        "\n",
        "| Question | How to Answer |\n",
        "|----------|---------------|\n",
        "| What patterns did it learn? | Look at the weights |\n",
        "| Why did it predict \"vertical\"? | Look at which inputs contributed most |\n",
        "| Is it using the right features? | Visualize the saliency map |\n",
        "\n",
        "### What IS Saliency?\n",
        "\n",
        "The word **\"saliency\"** comes from Latin *salire* meaning \"to leap.\" In machine learning:\n",
        "\n",
        "**Saliency = Which parts of the input \"leap out\" as important to the model**\n",
        "\n",
        "For our Perceptron, saliency is beautifully simple:\n",
        "\n",
        "$$\\text{Saliency}_i = |w_i \\times x_i|$$\n",
        "\n",
        "Where:\n",
        "- $w_i$ = weight for input $i$\n",
        "- $x_i$ = value of input $i$\n",
        "- $|...|$ = absolute value (we care about magnitude, not sign)\n",
        "\n",
        "### Why Absolute Value?\n",
        "\n",
        "| Weight × Input | Meaning | Contribution |\n",
        "|----------------|---------|--------------|\n",
        "| +2.0 × 1.0 = +2.0 | Strongly SUPPORTS vertical | HIGH |\n",
        "| -2.0 × 1.0 = -2.0 | Strongly OPPOSES vertical | HIGH |\n",
        "| +0.1 × 1.0 = +0.1 | Weakly supports vertical | LOW |\n",
        "\n",
        "Both +2.0 and -2.0 are **strong contributions** - just in opposite directions. The absolute value captures the **strength of influence**.\n",
        "\n",
        "### Committee Analogy\n",
        "\n",
        "*\"We ask the committee: 'Show us your reasoning. Highlight the evidence that most influenced your decision.' They produce a report where the most influential pieces of evidence glow brightly. This is the saliency map - a visual explanation of the committee's thought process.\"*\n",
        "\n",
        "### Why Interpretability Matters\n",
        "\n",
        "| Reason | Example |\n",
        "|--------|---------|\n",
        "| **Trust** | Can we trust this medical diagnosis? |\n",
        "| **Debugging** | Why is the model getting this wrong? |\n",
        "| **Discovery** | What features actually matter? |\n",
        "| **Fairness** | Is it unfairly using race or gender? |\n",
        "| **Legal** | GDPR requires \"right to explanation\" |\n",
        "\n",
        "### The Math Behind Saliency\n",
        "\n",
        "For our Perceptron, let's trace WHY $|w_i \\times x_i|$ measures importance:\n",
        "\n",
        "**Step 1: The Neuron's Decision**\n",
        "$$z = w_1 x_1 + w_2 x_2 + ... + w_9 x_9 + b$$\n",
        "\n",
        "Each term $w_i x_i$ is that pixel's **contribution** to the final sum $z$.\n",
        "\n",
        "**Step 2: How Much Did Each Pixel Contribute?**\n",
        "\n",
        "| Pixel | Weight ($w_i$) | Input ($x_i$) | Contribution ($w_i \\times x_i$) |\n",
        "|-------|----------------|---------------|--------------------------------|\n",
        "| 0 | 0.5 | 0 | 0.5 × 0 = 0 (no contribution) |\n",
        "| 1 | 1.2 | 1 | 1.2 × 1 = 1.2 (strong positive) |\n",
        "| 4 | -0.8 | 1 | -0.8 × 1 = -0.8 (strong negative) |\n",
        "\n",
        "**Step 3: Why Absolute Value?**\n",
        "\n",
        "Both +1.2 and -0.8 are **strong influences** on the decision - they just push in opposite directions. The absolute value captures **strength of influence regardless of direction**.\n",
        "\n",
        "$$\\text{Saliency}_i = |w_i \\times x_i|$$\n",
        "\n",
        "**Interpretation:**\n",
        "- High saliency = This pixel strongly influenced the decision (positively OR negatively)\n",
        "- Low saliency = This pixel didn't matter much for this prediction\n",
        "\n",
        "### Looking at What Our Model Learned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SALIENCY: What Did the Model Learn?\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"THE COMMITTEE REPORT: What Did the Model Learn?\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# First, let's look at the learned weights\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"STEP 1: Examine the Learned Weights\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "weights_grid = model.weights.reshape(3, 3)\n",
        "print(\"\"\"\n",
        "Remember our pixel positions:\n",
        "\n",
        "    Position Index:     Image Layout:\n",
        "    [0] [1] [2]         [row 0]\n",
        "    [3] [4] [5]   →     [row 1]\n",
        "    [6] [7] [8]         [row 2]\n",
        "\n",
        "Our model's learned weights (as 3x3 grid):\n",
        "\"\"\")\n",
        "for i, row in enumerate(weights_grid):\n",
        "    print(f\"  Row {i}: [{row[0]:6.3f}, {row[1]:6.3f}, {row[2]:6.3f}]\")\n",
        "\n",
        "print(f\"\\n  Bias: {model.bias:.4f}\")\n",
        "\n",
        "# Interpret the weights\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"STEP 2: Interpret What the Weights Mean\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "HOW TO READ WEIGHTS:\n",
        "  • Positive weight → This pixel being bright INCREASES \"vertical\" confidence\n",
        "  • Negative weight → This pixel being bright DECREASES \"vertical\" confidence\n",
        "  • Near-zero weight → This pixel doesn't matter much\n",
        "\"\"\")\n",
        "\n",
        "# Find which positions have highest/lowest weights\n",
        "flat_weights = model.weights\n",
        "max_idx = np.argmax(flat_weights)\n",
        "min_idx = np.argmin(flat_weights)\n",
        "\n",
        "print(f\"\"\"\n",
        "KEY OBSERVATIONS:\n",
        "\n",
        "  Maximum weight: position {max_idx} (row {max_idx//3}, col {max_idx%3}) = {flat_weights[max_idx]:.3f}\n",
        "    → If this pixel is bright, model is MORE confident it's vertical\n",
        "    \n",
        "  Minimum weight: position {min_idx} (row {min_idx//3}, col {min_idx%3}) = {flat_weights[min_idx]:.3f}\n",
        "    → If this pixel is bright, model is LESS confident it's vertical\n",
        "    \n",
        "  Positions with HIGH positive weights: {np.where(flat_weights > 0.3)[0].tolist()}\n",
        "    → These pixels SUPPORT \"vertical\" classification\n",
        "    \n",
        "  Positions with HIGH negative weights: {np.where(flat_weights < -0.3)[0].tolist()}\n",
        "    → These pixels OPPOSE \"vertical\" classification\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZE: Weights and Saliency Maps - THE \"AHA!\" MOMENT\n",
        "# =============================================================================\n",
        "\n",
        "def compute_saliency(model, x):\n",
        "    \"\"\"\n",
        "    Compute saliency map for an input.\n",
        "    \n",
        "    Saliency = |weight × input|\n",
        "    \n",
        "    This tells us: \"How much did each input pixel \n",
        "    contribute to the final decision?\"\n",
        "    \n",
        "    Parameters:\n",
        "        model: Trained model with weights\n",
        "        x: Input image (flattened)\n",
        "    \n",
        "    Returns:\n",
        "        saliency: Array of contribution magnitudes\n",
        "    \"\"\"\n",
        "    x = np.array(x).flatten()\n",
        "    # Multiply each input by its weight, take absolute value\n",
        "    return np.abs(model.weights * x)\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "\n",
        "# =================\n",
        "# Top row: Vertical line analysis\n",
        "# =================\n",
        "\n",
        "# 1. Input image\n",
        "ax = axes[0, 0]\n",
        "ax.imshow(vertical_line, cmap='Blues', vmin=0, vmax=1)\n",
        "ax.set_title('INPUT:\\nVertical Line', fontsize=11, fontweight='bold')\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        ax.text(j, i, f'{vertical_line[i,j]:.0f}', ha='center', va='center', fontsize=12)\n",
        "ax.axis('off')\n",
        "\n",
        "# 2. Model weights\n",
        "ax = axes[0, 1]\n",
        "im = ax.imshow(weights_grid, cmap='RdBu', vmin=-2, vmax=2)\n",
        "ax.set_title('WEIGHTS:\\nLearned by Model', fontsize=11, fontweight='bold')\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        color = 'white' if abs(weights_grid[i,j]) > 1 else 'black'\n",
        "        ax.text(j, i, f'{weights_grid[i,j]:.2f}', ha='center', va='center', fontsize=9, color=color)\n",
        "ax.axis('off')\n",
        "\n",
        "# 3. Saliency map\n",
        "ax = axes[0, 2]\n",
        "saliency_v = compute_saliency(model, vertical_flat).reshape(3, 3)\n",
        "im = ax.imshow(saliency_v, cmap='hot', vmin=0)\n",
        "ax.set_title('SALIENCY MAP:\\n|Weight × Input|', fontsize=11, fontweight='bold')\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        color = 'white' if saliency_v[i,j] > saliency_v.max()/2 else 'black'\n",
        "        ax.text(j, i, f'{saliency_v[i,j]:.2f}', ha='center', va='center', fontsize=9, color=color)\n",
        "ax.axis('off')\n",
        "\n",
        "# 4. Prediction result\n",
        "ax = axes[0, 3]\n",
        "ax.axis('off')\n",
        "pred_v = model.forward(vertical_flat)\n",
        "result_text = f\"\"\"PREDICTION\n",
        "\n",
        "Raw output: {pred_v:.4f}\n",
        "Confidence: {pred_v*100:.1f}%\n",
        "\n",
        "Decision: {\"VERTICAL\" if pred_v >= 0.5 else \"HORIZONTAL\"}\n",
        "\n",
        "Correct! ✓\"\"\"\n",
        "ax.text(0.5, 0.5, result_text, fontsize=11, fontweight='bold',\n",
        "       ha='center', va='center', transform=ax.transAxes,\n",
        "       bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
        "\n",
        "# =================\n",
        "# Bottom row: Horizontal line analysis\n",
        "# =================\n",
        "\n",
        "# 1. Input image\n",
        "ax = axes[1, 0]\n",
        "ax.imshow(horizontal_line, cmap='Blues', vmin=0, vmax=1)\n",
        "ax.set_title('INPUT:\\nHorizontal Line', fontsize=11, fontweight='bold')\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        ax.text(j, i, f'{horizontal_line[i,j]:.0f}', ha='center', va='center', fontsize=12)\n",
        "ax.axis('off')\n",
        "\n",
        "# 2. Model weights (same)\n",
        "ax = axes[1, 1]\n",
        "im = ax.imshow(weights_grid, cmap='RdBu', vmin=-2, vmax=2)\n",
        "ax.set_title('WEIGHTS:\\n(Same model)', fontsize=11, fontweight='bold')\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        color = 'white' if abs(weights_grid[i,j]) > 1 else 'black'\n",
        "        ax.text(j, i, f'{weights_grid[i,j]:.2f}', ha='center', va='center', fontsize=9, color=color)\n",
        "ax.axis('off')\n",
        "\n",
        "# 3. Saliency map\n",
        "ax = axes[1, 2]\n",
        "saliency_h = compute_saliency(model, horizontal_flat).reshape(3, 3)\n",
        "im = ax.imshow(saliency_h, cmap='hot', vmin=0)\n",
        "ax.set_title('SALIENCY MAP:\\n|Weight × Input|', fontsize=11, fontweight='bold')\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        color = 'white' if saliency_h[i,j] > saliency_h.max()/2 else 'black'\n",
        "        ax.text(j, i, f'{saliency_h[i,j]:.2f}', ha='center', va='center', fontsize=9, color=color)\n",
        "ax.axis('off')\n",
        "\n",
        "# 4. Prediction result\n",
        "ax = axes[1, 3]\n",
        "ax.axis('off')\n",
        "pred_h = model.forward(horizontal_flat)\n",
        "result_text = f\"\"\"PREDICTION\n",
        "\n",
        "Raw output: {pred_h:.4f}\n",
        "Confidence: {(1-pred_h)*100:.1f}% horizontal\n",
        "\n",
        "Decision: {\"VERTICAL\" if pred_h >= 0.5 else \"HORIZONTAL\"}\n",
        "\n",
        "Correct! ✓\"\"\"\n",
        "ax.text(0.5, 0.5, result_text, fontsize=11, fontweight='bold',\n",
        "       ha='center', va='center', transform=ax.transAxes,\n",
        "       bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
        "\n",
        "plt.suptitle('THE COMMITTEE REPORT: How the Model Makes Decisions', fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# THE \"AHA!\" MOMENT: Understanding What the Model Learned\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"THE KEY INSIGHT: What Did the Model ACTUALLY Learn?\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "Looking at the visualizations above, we can see something beautiful:\n",
        "\n",
        "FOR VERTICAL LINES:\n",
        "  • The middle column (positions 1, 4, 7) has POSITIVE weights\n",
        "  • When bright pixels appear in the middle column, the model says \"VERTICAL!\"\n",
        "  • The saliency map lights up exactly where the vertical line is\n",
        "  \n",
        "FOR HORIZONTAL LINES:\n",
        "  • The middle row (positions 3, 4, 5) has NEGATIVE or low weights for the sides\n",
        "  • When bright pixels appear across a row, they don't activate the \"vertical\" detector\n",
        "  • The output is LOW, meaning \"not vertical\" = \"horizontal\"\n",
        "\n",
        "THE MODEL LEARNED THE RIGHT PATTERN!\n",
        "═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "Our model didn't just memorize examples. It learned a GENERAL RULE:\n",
        "\n",
        "  \"Vertical lines have bright pixels stacked in a column.\n",
        "   Horizontal lines have bright pixels spread across a row.\"\n",
        "\n",
        "This is exactly what we hoped it would learn!\n",
        "\n",
        "═══════════════════════════════════════════════════════════════════════\n",
        "\"\"\")\n",
        "\n",
        "# Show the pattern it learned\n",
        "print(\"\\nVisualized Pattern Recognition:\")\n",
        "print(\"-\"*50)\n",
        "print(\"\"\"\n",
        "  VERTICAL LINE:          MODEL LOOKS AT:\n",
        "  [ ] [●] [ ]             [ ] [HIGH] [ ]\n",
        "  [ ] [●] [ ]     →       [ ] [HIGH] [ ]\n",
        "  [ ] [●] [ ]             [ ] [HIGH] [ ]\n",
        "                          (Middle column weights are positive)\n",
        "  \n",
        "  HORIZONTAL LINE:        MODEL LOOKS AT:\n",
        "  [ ] [ ] [ ]             [ ] [ ] [ ]\n",
        "  [●] [●] [●]     →       [LOW] [LOW] [LOW]\n",
        "  [ ] [ ] [ ]             [ ] [ ] [ ]\n",
        "                          (Row weights don't support \"vertical\")\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6.6 Train/Test Split: Why We Need Separate Data\n",
        "\n",
        "Throughout this notebook, we've used separate **training** and **test** data. This is crucial for honest evaluation.\n",
        "\n",
        "### The Problem: Memorization vs Learning\n",
        "\n",
        "A model could achieve 100% accuracy on training data by simply **memorizing** every example - like a student who memorizes test answers instead of understanding concepts.\n",
        "\n",
        "But memorization isn't useful - we need the model to **generalize** to NEW data it has never seen.\n",
        "\n",
        "| Approach | Training Accuracy | Test Accuracy | What Happened? |\n",
        "|----------|------------------|---------------|----------------|\n",
        "| True learning | 95% | 93% | Learned the general pattern |\n",
        "| Memorization | 100% | 50% | Memorized training, fails on new |\n",
        "\n",
        "### What IS a Train/Test Split?\n",
        "\n",
        "We divide our data into two groups:\n",
        "\n",
        "```\n",
        "ALL DATA (150 samples)\n",
        "    │\n",
        "    ├── TRAINING SET (100 samples) ──→ Used to TRAIN the model\n",
        "    │                                  Model sees these during learning\n",
        "    │\n",
        "    └── TEST SET (50 samples) ───────→ Used to EVALUATE the model\n",
        "                                       Model NEVER sees these during training\n",
        "```\n",
        "\n",
        "### Why This Works\n",
        "\n",
        "| Data Set | Model Sees During Training? | Purpose |\n",
        "|----------|---------------------------|---------|\n",
        "| **Training** | YES | Learn patterns |\n",
        "| **Test** | NO | Evaluate generalization |\n",
        "\n",
        "The test set acts as a \"final exam\" - questions the model has never seen.\n",
        "\n",
        "### Committee Analogy\n",
        "\n",
        "*\"It's like preparing for an exam:*\n",
        "- *Training data = study materials (examples you practice with)*\n",
        "- *Test data = the actual exam (new questions you've never seen)*\n",
        "\n",
        "*If you just memorize your notes without understanding, you'll ace the practice problems but fail the exam. If you truly learned the concepts, you'll do well on both.\"*\n",
        "\n",
        "### The Golden Rule\n",
        "\n",
        "**NEVER use test data for training!**\n",
        "\n",
        "If the model sees test data during training, it can memorize those examples too, and our evaluation becomes meaningless.\n",
        "\n",
        "### Common Split Ratios\n",
        "\n",
        "| Split | Training | Test | When to Use |\n",
        "|-------|----------|------|-------------|\n",
        "| 80/20 | 80% | 20% | Large datasets (>10,000 samples) |\n",
        "| 70/30 | 70% | 30% | Medium datasets (1,000-10,000) |\n",
        "| 60/40 | 60% | 40% | Small datasets (<1,000) |\n",
        "\n",
        "More test data = more reliable evaluation, but less training data.\n",
        "\n",
        "### Understanding Overfitting Mathematically\n",
        "\n",
        "**What IS Overfitting?**\n",
        "\n",
        "Overfitting is when a model learns the **noise** in the training data, not just the **signal**.\n",
        "\n",
        "**Analogy:** Imagine studying for an exam by memorizing the exact wording of practice questions instead of understanding the concepts. You'd ace those exact questions but fail on new ones.\n",
        "\n",
        "**How Train/Test Split Reveals Overfitting:**\n",
        "\n",
        "| Scenario | Training Accuracy | Test Accuracy | What's Happening |\n",
        "|----------|------------------|---------------|------------------|\n",
        "| **Good learning** | 95% | 93% | Learned the pattern! |\n",
        "| **Mild overfitting** | 99% | 85% | Some memorization |\n",
        "| **Severe overfitting** | 100% | 50% | Memorized everything, learned nothing |\n",
        "\n",
        "**The Math:**\n",
        "- If a model memorizes all 100 training examples, it can get 100% training accuracy\n",
        "- But those memorized patterns don't apply to new data\n",
        "- Test accuracy reveals true generalization\n",
        "\n",
        "**The Gap:**\n",
        "$$\\text{Overfitting Gap} = \\text{Training Accuracy} - \\text{Test Accuracy}$$\n",
        "\n",
        "- Gap < 5%: Great! Model generalizes well\n",
        "- Gap 5-15%: Some overfitting, might need more data or simpler model\n",
        "- Gap > 15%: Serious overfitting, model is memorizing\n",
        "\n",
        "### Why These Specific Ratios?\n",
        "\n",
        "| More Training Data | More Test Data |\n",
        "|-------------------|----------------|\n",
        "| Model can learn more | More reliable evaluation |\n",
        "| Better final accuracy | Smaller margin of error |\n",
        "| Less reliable evaluation | Model might underfit |\n",
        "\n",
        "**The sweet spot:** Enough training data to learn well, enough test data to evaluate reliably. With 100 samples, 80/20 gives 80 for training (decent) and 20 for testing (acceptable). With 10,000 samples, even 90/10 gives 1,000 test samples (very reliable).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAIN/TEST SPLIT: Our Results\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TRAIN/TEST SPLIT: Checking for Generalization\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\"\"\n",
        "OUR DATA SPLIT:\n",
        "  • Training set: {len(X_train)} samples (used for learning)\n",
        "  • Test set: {len(X_test)} samples (used for evaluation only)\n",
        "  • Split ratio: {len(X_train)}/{len(X_train)+len(X_test)} = {len(X_train)/(len(X_train)+len(X_test))*100:.0f}% training\n",
        "  \n",
        "RESULTS:\n",
        "  • Training accuracy: {train_accuracy:.1%}\n",
        "  • Test accuracy: {test_accuracy:.1%}\n",
        "  • Difference: {abs(train_accuracy - test_accuracy):.1%}\n",
        "\"\"\")\n",
        "\n",
        "# Interpret the gap\n",
        "diff = train_accuracy - test_accuracy\n",
        "\n",
        "print(\"-\"*70)\n",
        "print(\"INTERPRETATION:\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "if diff < 0.05:\n",
        "    print(\"\"\"\n",
        "  ✓ EXCELLENT! Training and test accuracy are very similar.\n",
        "  \n",
        "  This suggests the model has LEARNED the general pattern,\n",
        "  not just memorized the training data.\n",
        "  \n",
        "  Our model generalizes well to new data!\n",
        "\"\"\")\n",
        "elif diff < 0.15:\n",
        "    print(f\"\"\"\n",
        "  ⚠ CAUTION: Training accuracy is {diff:.1%} higher than test accuracy.\n",
        "  \n",
        "  Some memorization may have occurred.\n",
        "  The model might be slightly \"overfitting\" to training data.\n",
        "\"\"\")\n",
        "else:\n",
        "    print(f\"\"\"\n",
        "  ⚠ WARNING: Training accuracy is {diff:.1%} higher than test accuracy!\n",
        "  \n",
        "  This suggests OVERFITTING - the model memorized training data\n",
        "  but doesn't generalize well to new data.\n",
        "  \n",
        "  Possible solutions:\n",
        "    - Get more training data\n",
        "    - Use regularization\n",
        "    - Simplify the model\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 6 Summary: What We've Learned\n",
        "\n",
        "### Key Concepts Mastered\n",
        "\n",
        "| Concept | Definition/Formula | Why It Matters |\n",
        "|---------|-------------------|----------------|\n",
        "| **Training vs Inference** | Learning mode vs using mode | Different behaviors, same weights |\n",
        "| **Accuracy** | (TP + TN) / Total | Simple overall view (but can mislead) |\n",
        "| **Confusion Matrix** | TP, TN, FP, FN breakdown | Shows WHAT mistakes are made |\n",
        "| **Precision** | TP / (TP + FP) | \"When I say yes, am I right?\" |\n",
        "| **Recall** | TP / (TP + FN) | \"Did I catch all the positives?\" |\n",
        "| **F1 Score** | 2 × (P × R) / (P + R) | Balance precision and recall |\n",
        "| **Saliency** | \\|weight × input\\| | What did the model look at? |\n",
        "| **Train/Test Split** | Separate data for evaluation | Detect memorization vs learning |\n",
        "\n",
        "### The Four Categories Explained\n",
        "\n",
        "| Category | Model Said | Truth Was | Meaning |\n",
        "|----------|-----------|-----------|---------|\n",
        "| **TP** (True Positive) | Vertical | Vertical | Correct detection |\n",
        "| **TN** (True Negative) | Horizontal | Horizontal | Correct rejection |\n",
        "| **FP** (False Positive) | Vertical | Horizontal | False alarm |\n",
        "| **FN** (False Negative) | Horizontal | Vertical | Missed detection |\n",
        "\n",
        "### Committee Analogy Progress\n",
        "\n",
        "| Part | What Happened |\n",
        "|------|--------------|\\n| Parts 1-3 | Committee member learned procedures |\n",
        "| Part 4 | First case - confused, random guessing |\n",
        "| Part 5 | Learned from feedback, became expert |\n",
        "| **Part 6** | **Performance review: verified expertise and understood reasoning** |\n",
        "| Part 7 | (Next) One expert isn't enough - building the full committee |\n",
        "\n",
        "### The Big Picture\n",
        "\n",
        "We now have a **complete, evaluated model** that:\n",
        "- Achieves high accuracy on both training and test data\n",
        "- Makes few mistakes (low FP and FN)\n",
        "- Has interpretable learned weights\n",
        "- Uses the RIGHT features (column patterns for vertical detection)\n",
        "- Generalizes well to new data\n",
        "\n",
        "---\n",
        "\n",
        "## Knowledge Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# KNOWLEDGE CHECK - Part 6\n",
        "# =============================================================================\n",
        "\n",
        "print(\"KNOWLEDGE CHECK - Part 6: Evaluation\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nAnswer these questions to test your understanding:\\n\")\n",
        "\n",
        "questions = [\n",
        "    {\n",
        "        \"q\": \"1. What's the difference between training and inference mode?\",\n",
        "        \"options\": [\n",
        "            \"A) Training is faster than inference\",\n",
        "            \"B) In training, weights update; in inference, weights are frozen\",\n",
        "            \"C) Inference uses more data than training\",\n",
        "            \"D) They're the same thing with different names\"\n",
        "        ],\n",
        "        \"answer\": \"B\",\n",
        "        \"explanation\": \"During training, the model learns and weights change after each example. During inference, weights are frozen and we just make predictions - no learning happens.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"2. A model predicts 'sick' for a healthy patient. What type of error is this?\",\n",
        "        \"options\": [\n",
        "            \"A) True Positive (TP)\",\n",
        "            \"B) True Negative (TN)\",\n",
        "            \"C) False Positive (FP)\",\n",
        "            \"D) False Negative (FN)\"\n",
        "        ],\n",
        "        \"answer\": \"C\",\n",
        "        \"explanation\": \"False Positive: We predicted Positive (sick), but we were False (wrong) - the patient was actually healthy. This is a 'false alarm'.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"3. You're building a disease detection system. Missing a sick patient is VERY bad.\\n   Which metric should you prioritize?\",\n",
        "        \"options\": [\n",
        "            \"A) Accuracy\",\n",
        "            \"B) Precision\",\n",
        "            \"C) Recall\",\n",
        "            \"D) F1 Score\"\n",
        "        ],\n",
        "        \"answer\": \"C\",\n",
        "        \"explanation\": \"Recall measures 'did we catch all the positives?' High recall means we catch most sick patients, even if we have some false alarms. When missing positives is costly, prioritize recall.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"4. Why do we use a separate test set?\",\n",
        "        \"options\": [\n",
        "            \"A) To have more data for training\",\n",
        "            \"B) To make training faster\",\n",
        "            \"C) To check if the model memorized vs truly learned\",\n",
        "            \"D) It's optional and not really needed\"\n",
        "        ],\n",
        "        \"answer\": \"C\",\n",
        "        \"explanation\": \"A model could memorize training data and fail on new data. The test set (unseen data) reveals if it truly learned the general pattern or just memorized examples.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"5. What does a saliency map show?\",\n",
        "        \"options\": [\n",
        "            \"A) The accuracy of the model over time\",\n",
        "            \"B) Which inputs the model focused on for its decision\",\n",
        "            \"C) The training loss curve\",\n",
        "            \"D) How fast the model runs\"\n",
        "        ],\n",
        "        \"answer\": \"B\",\n",
        "        \"explanation\": \"Saliency maps highlight which parts of the input were most important for the model's decision. It's a form of interpretability - understanding WHY the model made its prediction.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(q[\"q\"])\n",
        "    for opt in q[\"options\"]:\n",
        "        print(f\"   {opt}\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Scroll down for answers...\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ANSWERS - Knowledge Check Part 6\n",
        "# =============================================================================\n",
        "\n",
        "print(\"ANSWERS - Part 6 Knowledge Check\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, q in enumerate(questions, 1):\n",
        "    print(f\"\\n{i}. Answer: {q['answer']}\")\n",
        "    print(f\"   {q['explanation']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"How did you do?\")\n",
        "print(\"  5/5: Evaluation Master! Ready for Part 7!\")\n",
        "print(\"  4/5: Solid understanding - great job!\")\n",
        "print(\"  3/5: Review the sections you missed\")\n",
        "print(\"  <3:  Re-read Part 6 before continuing\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## What's Next?\n",
        "\n",
        "**Congratulations!** You've completed Part 6!\n",
        "\n",
        "Our single neuron is now a **verified expert** - we've evaluated its performance, understood its decision-making process, and confirmed it learned the RIGHT patterns.\n",
        "\n",
        "### But Here's the Thing...\n",
        "\n",
        "A single neuron (Perceptron) can only learn **linear patterns** - patterns that can be separated by a straight line. For more complex problems, one expert isn't enough.\n",
        "\n",
        "### The Limitation of Single Neurons\n",
        "\n",
        "Some problems are **not linearly separable**. The classic example is the **XOR problem**:\n",
        "\n",
        "| Input A | Input B | Output (XOR) |\n",
        "|---------|---------|--------------|\n",
        "| 0 | 0 | 0 |\n",
        "| 0 | 1 | 1 |\n",
        "| 1 | 0 | 1 |\n",
        "| 1 | 1 | 0 |\n",
        "\n",
        "No single neuron can learn this pattern! We need **multiple neurons working together**.\n",
        "\n",
        "### Coming Up in Part 7: Hidden Layers - The Full Committee\n",
        "\n",
        "In the next notebook, we'll explore:\n",
        "\n",
        "- **Why one neuron isn't enough** - The XOR problem demonstration\n",
        "- **Hidden layers** - Adding more neurons between input and output\n",
        "- **The full committee** - Multiple experts with different perspectives\n",
        "- **Universal approximation** - Why deep networks can learn (almost) anything\n",
        "\n",
        "---\n",
        "\n",
        "**Continue to Part 7:** `part_7_hidden_layers.ipynb`\n",
        "\n",
        "---\n",
        "\n",
        "*\"One expert is good. A committee of experts is powerful.\"*\n",
        "\n",
        "**The Brain's Decision Committee** - From Expert to Team\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
